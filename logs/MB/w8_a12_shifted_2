You are using fake SyncBatchNorm2d who is actually the official BatchNorm2d
==> Using Pytorch Dataset
QuantModel(
  (model): EfficientViTCls(
    (backbone): EfficientViTBackbone(
      (input_stem): OpSequential(
        (op_list): ModuleList(
          (0): ConvLayer(
            (conv): QuantModule(
              3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)
              (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
              (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (activation_function): Hardswish()
            )
            (norm): StraightThrough()
            (act): StraightThrough()
          )
          (1): QauntMBBlock(
            (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (activation_function): StraightThrough()
            (inv_res): ResidualBlock(
              (main): DSConv(
                (depth_conv): ConvLayer(
                  (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)
                  (norm): StraightThrough()
                  (act): Hardswish()
                )
                (point_conv): ConvLayer(
                  (conv): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))
                  (norm): StraightThrough()
                )
              )
              (shortcut): IdentityLayer()
            )
            (conv): Sequential(
              (0): QuantModule(
                16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16
                (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): Hardswish()
              )
              (1): QuantModule(
                16, 16, kernel_size=(1, 1), stride=(1, 1)
                (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
              )
            )
          )
        )
      )
      (stages): ModuleList(
        (0): OpSequential(
          (op_list): ModuleList(
            (0): QauntMBBlock(
              (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (activation_function): StraightThrough()
              (inv_res): ResidualBlock(
                (main): MBConv(
                  (inverted_conv): ConvLayer(
                    (conv): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
                    (norm): StraightThrough()
                    (act): Hardswish()
                  )
                  (depth_conv): ConvLayer(
                    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)
                    (norm): StraightThrough()
                    (act): Hardswish()
                  )
                  (point_conv): ConvLayer(
                    (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
                    (norm): StraightThrough()
                  )
                )
              )
              (conv): Sequential(
                (0): QuantModule(
                  16, 64, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): Hardswish()
                )
                (1): QuantModule_Shifted(
                  64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (input_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (output_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): Hardswish()
                )
                (2): QuantModule_Shifted(
                  64, 32, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (input_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (output_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
              )
            )
            (1): QauntMBBlock(
              (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (activation_function): StraightThrough()
              (inv_res): ResidualBlock(
                (main): MBConv(
                  (inverted_conv): ConvLayer(
                    (conv): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
                    (norm): StraightThrough()
                    (act): Hardswish()
                  )
                  (depth_conv): ConvLayer(
                    (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                    (norm): StraightThrough()
                    (act): Hardswish()
                  )
                  (point_conv): ConvLayer(
                    (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
                    (norm): StraightThrough()
                  )
                )
                (shortcut): IdentityLayer()
              )
              (conv): Sequential(
                (0): QuantModule(
                  32, 128, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): Hardswish()
                )
                (1): QuantModule_Shifted(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (input_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (output_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): Hardswish()
                )
                (2): QuantModule_Shifted(
                  128, 32, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (input_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (output_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
              )
            )
          )
        )
        (1): OpSequential(
          (op_list): ModuleList(
            (0): QauntMBBlock(
              (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (activation_function): StraightThrough()
              (inv_res): ResidualBlock(
                (main): MBConv(
                  (inverted_conv): ConvLayer(
                    (conv): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
                    (norm): StraightThrough()
                    (act): Hardswish()
                  )
                  (depth_conv): ConvLayer(
                    (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)
                    (norm): StraightThrough()
                    (act): Hardswish()
                  )
                  (point_conv): ConvLayer(
                    (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
                    (norm): StraightThrough()
                  )
                )
              )
              (conv): Sequential(
                (0): QuantModule(
                  32, 128, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): Hardswish()
                )
                (1): QuantModule_Shifted(
                  128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (input_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (output_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): Hardswish()
                )
                (2): QuantModule_Shifted(
                  128, 64, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (input_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (output_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
              )
            )
            (1): QauntMBBlock(
              (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (activation_function): StraightThrough()
              (inv_res): ResidualBlock(
                (main): MBConv(
                  (inverted_conv): ConvLayer(
                    (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                    (norm): StraightThrough()
                    (act): Hardswish()
                  )
                  (depth_conv): ConvLayer(
                    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
                    (norm): StraightThrough()
                    (act): Hardswish()
                  )
                  (point_conv): ConvLayer(
                    (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
                    (norm): StraightThrough()
                  )
                )
                (shortcut): IdentityLayer()
              )
              (conv): Sequential(
                (0): QuantModule(
                  64, 256, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): Hardswish()
                )
                (1): QuantModule_Shifted(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (input_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (output_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): Hardswish()
                )
                (2): QuantModule_Shifted(
                  256, 64, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (input_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (output_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
              )
            )
            (2): QauntMBBlock(
              (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (activation_function): StraightThrough()
              (inv_res): ResidualBlock(
                (main): MBConv(
                  (inverted_conv): ConvLayer(
                    (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                    (norm): StraightThrough()
                    (act): Hardswish()
                  )
                  (depth_conv): ConvLayer(
                    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
                    (norm): StraightThrough()
                    (act): Hardswish()
                  )
                  (point_conv): ConvLayer(
                    (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
                    (norm): StraightThrough()
                  )
                )
                (shortcut): IdentityLayer()
              )
              (conv): Sequential(
                (0): QuantModule(
                  64, 256, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): Hardswish()
                )
                (1): QuantModule_Shifted(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (input_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (output_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): Hardswish()
                )
                (2): QuantModule_Shifted(
                  256, 64, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (input_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (output_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
              )
            )
          )
        )
        (2): OpSequential(
          (op_list): ModuleList(
            (0): QauntMBBlock(
              (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (activation_function): StraightThrough()
              (inv_res): ResidualBlock(
                (main): MBConv(
                  (inverted_conv): ConvLayer(
                    (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                    (act): Hardswish()
                  )
                  (depth_conv): ConvLayer(
                    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)
                    (act): Hardswish()
                  )
                  (point_conv): ConvLayer(
                    (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
                    (norm): StraightThrough()
                  )
                )
              )
              (conv): Sequential(
                (0): QuantModule(
                  64, 256, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): Hardswish()
                )
                (1): QuantModule_Shifted(
                  256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (input_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (output_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): Hardswish()
                )
                (2): QuantModule_Shifted(
                  256, 128, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (input_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (output_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
              )
            )
            (1): EfficientViTBlock(
              (context_module): QauntMBBlock(
                (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
                (inv_res): ResidualBlock(
                  (main): LiteMSA(
                    (qkv): ConvLayer(
                      (conv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
                    )
                    (aggreg): ModuleList(
                      (0): Sequential(
                        (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)
                        (1): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), groups=24, bias=False)
                      )
                    )
                    (kernel_func): ReLU()
                    (proj): ConvLayer(
                      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
                      (norm): StraightThrough()
                    )
                  )
                  (shortcut): IdentityLayer()
                )
                (qkv): QuantModule(
                  128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
                (aggreg): ModuleList(
                  (0): Sequential(
                    (0): QuantModule(
                      384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False
                      (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                      (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                      (activation_function): StraightThrough()
                    )
                    (1): QuantModule(
                      384, 384, kernel_size=(1, 1), stride=(1, 1), groups=24, bias=False
                      (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                      (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                      (activation_function): StraightThrough()
                    )
                  )
                )
                (proj): QuantModule(
                  256, 128, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
                (kernel_func): ReLU()
              )
              (local_module): QauntMBBlock(
                (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
                (inv_res): ResidualBlock(
                  (main): MBConv(
                    (inverted_conv): ConvLayer(
                      (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                      (act): Hardswish()
                    )
                    (depth_conv): ConvLayer(
                      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                      (act): Hardswish()
                    )
                    (point_conv): ConvLayer(
                      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                      (norm): StraightThrough()
                    )
                  )
                  (shortcut): IdentityLayer()
                )
                (conv): Sequential(
                  (0): QuantModule(
                    128, 512, kernel_size=(1, 1), stride=(1, 1)
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): Hardswish()
                  )
                  (1): QuantModule_Shifted(
                    512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (input_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (output_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): Hardswish()
                  )
                  (2): QuantModule_Shifted(
                    512, 128, kernel_size=(1, 1), stride=(1, 1)
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (input_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (output_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): StraightThrough()
                  )
                )
              )
            )
            (2): EfficientViTBlock(
              (context_module): QauntMBBlock(
                (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
                (inv_res): ResidualBlock(
                  (main): LiteMSA(
                    (qkv): ConvLayer(
                      (conv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
                    )
                    (aggreg): ModuleList(
                      (0): Sequential(
                        (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)
                        (1): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), groups=24, bias=False)
                      )
                    )
                    (kernel_func): ReLU()
                    (proj): ConvLayer(
                      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
                      (norm): StraightThrough()
                    )
                  )
                  (shortcut): IdentityLayer()
                )
                (qkv): QuantModule(
                  128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
                (aggreg): ModuleList(
                  (0): Sequential(
                    (0): QuantModule(
                      384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False
                      (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                      (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                      (activation_function): StraightThrough()
                    )
                    (1): QuantModule(
                      384, 384, kernel_size=(1, 1), stride=(1, 1), groups=24, bias=False
                      (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                      (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                      (activation_function): StraightThrough()
                    )
                  )
                )
                (proj): QuantModule(
                  256, 128, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
                (kernel_func): ReLU()
              )
              (local_module): QauntMBBlock(
                (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
                (inv_res): ResidualBlock(
                  (main): MBConv(
                    (inverted_conv): ConvLayer(
                      (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                      (act): Hardswish()
                    )
                    (depth_conv): ConvLayer(
                      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                      (act): Hardswish()
                    )
                    (point_conv): ConvLayer(
                      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                      (norm): StraightThrough()
                    )
                  )
                  (shortcut): IdentityLayer()
                )
                (conv): Sequential(
                  (0): QuantModule(
                    128, 512, kernel_size=(1, 1), stride=(1, 1)
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): Hardswish()
                  )
                  (1): QuantModule_Shifted(
                    512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (input_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (output_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): Hardswish()
                  )
                  (2): QuantModule_Shifted(
                    512, 128, kernel_size=(1, 1), stride=(1, 1)
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (input_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (output_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): StraightThrough()
                  )
                )
              )
            )
            (3): EfficientViTBlock(
              (context_module): QauntMBBlock(
                (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
                (inv_res): ResidualBlock(
                  (main): LiteMSA(
                    (qkv): ConvLayer(
                      (conv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
                    )
                    (aggreg): ModuleList(
                      (0): Sequential(
                        (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)
                        (1): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), groups=24, bias=False)
                      )
                    )
                    (kernel_func): ReLU()
                    (proj): ConvLayer(
                      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
                      (norm): StraightThrough()
                    )
                  )
                  (shortcut): IdentityLayer()
                )
                (qkv): QuantModule(
                  128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
                (aggreg): ModuleList(
                  (0): Sequential(
                    (0): QuantModule(
                      384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False
                      (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                      (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                      (activation_function): StraightThrough()
                    )
                    (1): QuantModule(
                      384, 384, kernel_size=(1, 1), stride=(1, 1), groups=24, bias=False
                      (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                      (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                      (activation_function): StraightThrough()
                    )
                  )
                )
                (proj): QuantModule(
                  256, 128, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
                (kernel_func): ReLU()
              )
              (local_module): QauntMBBlock(
                (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
                (inv_res): ResidualBlock(
                  (main): MBConv(
                    (inverted_conv): ConvLayer(
                      (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                      (act): Hardswish()
                    )
                    (depth_conv): ConvLayer(
                      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                      (act): Hardswish()
                    )
                    (point_conv): ConvLayer(
                      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                      (norm): StraightThrough()
                    )
                  )
                  (shortcut): IdentityLayer()
                )
                (conv): Sequential(
                  (0): QuantModule(
                    128, 512, kernel_size=(1, 1), stride=(1, 1)
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): Hardswish()
                  )
                  (1): QuantModule_Shifted(
                    512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (input_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (output_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): Hardswish()
                  )
                  (2): QuantModule_Shifted(
                    512, 128, kernel_size=(1, 1), stride=(1, 1)
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (input_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (output_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): StraightThrough()
                  )
                )
              )
            )
          )
        )
        (3): OpSequential(
          (op_list): ModuleList(
            (0): QauntMBBlock(
              (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (activation_function): StraightThrough()
              (inv_res): ResidualBlock(
                (main): MBConv(
                  (inverted_conv): ConvLayer(
                    (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                    (act): Hardswish()
                  )
                  (depth_conv): ConvLayer(
                    (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)
                    (act): Hardswish()
                  )
                  (point_conv): ConvLayer(
                    (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
                    (norm): StraightThrough()
                  )
                )
              )
              (conv): Sequential(
                (0): QuantModule(
                  128, 512, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): Hardswish()
                )
                (1): QuantModule_Shifted(
                  512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (input_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (output_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): Hardswish()
                )
                (2): QuantModule_Shifted(
                  512, 256, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (input_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (output_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
              )
            )
            (1): EfficientViTBlock(
              (context_module): QauntMBBlock(
                (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
                (inv_res): ResidualBlock(
                  (main): LiteMSA(
                    (qkv): ConvLayer(
                      (conv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
                    )
                    (aggreg): ModuleList(
                      (0): Sequential(
                        (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)
                        (1): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False)
                      )
                    )
                    (kernel_func): ReLU()
                    (proj): ConvLayer(
                      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
                      (norm): StraightThrough()
                    )
                  )
                  (shortcut): IdentityLayer()
                )
                (qkv): QuantModule(
                  256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
                (aggreg): ModuleList(
                  (0): Sequential(
                    (0): QuantModule(
                      768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False
                      (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                      (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                      (activation_function): StraightThrough()
                    )
                    (1): QuantModule(
                      768, 768, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False
                      (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                      (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                      (activation_function): StraightThrough()
                    )
                  )
                )
                (proj): QuantModule(
                  512, 256, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
                (kernel_func): ReLU()
              )
              (local_module): QauntMBBlock(
                (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
                (inv_res): ResidualBlock(
                  (main): MBConv(
                    (inverted_conv): ConvLayer(
                      (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
                      (act): Hardswish()
                    )
                    (depth_conv): ConvLayer(
                      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
                      (act): Hardswish()
                    )
                    (point_conv): ConvLayer(
                      (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
                      (norm): StraightThrough()
                    )
                  )
                  (shortcut): IdentityLayer()
                )
                (conv): Sequential(
                  (0): QuantModule(
                    256, 1024, kernel_size=(1, 1), stride=(1, 1)
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): Hardswish()
                  )
                  (1): QuantModule_Shifted(
                    1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (input_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (output_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): Hardswish()
                  )
                  (2): QuantModule_Shifted(
                    1024, 256, kernel_size=(1, 1), stride=(1, 1)
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (input_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (output_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): StraightThrough()
                  )
                )
              )
            )
            (2): EfficientViTBlock(
              (context_module): QauntMBBlock(
                (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
                (inv_res): ResidualBlock(
                  (main): LiteMSA(
                    (qkv): ConvLayer(
                      (conv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
                    )
                    (aggreg): ModuleList(
                      (0): Sequential(
                        (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)
                        (1): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False)
                      )
                    )
                    (kernel_func): ReLU()
                    (proj): ConvLayer(
                      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
                      (norm): StraightThrough()
                    )
                  )
                  (shortcut): IdentityLayer()
                )
                (qkv): QuantModule(
                  256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
                (aggreg): ModuleList(
                  (0): Sequential(
                    (0): QuantModule(
                      768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False
                      (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                      (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                      (activation_function): StraightThrough()
                    )
                    (1): QuantModule(
                      768, 768, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False
                      (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                      (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                      (activation_function): StraightThrough()
                    )
                  )
                )
                (proj): QuantModule(
                  512, 256, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
                (kernel_func): ReLU()
              )
              (local_module): QauntMBBlock(
                (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
                (inv_res): ResidualBlock(
                  (main): MBConv(
                    (inverted_conv): ConvLayer(
                      (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
                      (act): Hardswish()
                    )
                    (depth_conv): ConvLayer(
                      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
                      (act): Hardswish()
                    )
                    (point_conv): ConvLayer(
                      (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
                      (norm): StraightThrough()
                    )
                  )
                  (shortcut): IdentityLayer()
                )
                (conv): Sequential(
                  (0): QuantModule(
                    256, 1024, kernel_size=(1, 1), stride=(1, 1)
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): Hardswish()
                  )
                  (1): QuantModule_Shifted(
                    1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (input_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (output_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): Hardswish()
                  )
                  (2): QuantModule_Shifted(
                    1024, 256, kernel_size=(1, 1), stride=(1, 1)
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (input_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (output_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): StraightThrough()
                  )
                )
              )
            )
            (3): EfficientViTBlock(
              (context_module): QauntMBBlock(
                (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
                (inv_res): ResidualBlock(
                  (main): LiteMSA(
                    (qkv): ConvLayer(
                      (conv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
                    )
                    (aggreg): ModuleList(
                      (0): Sequential(
                        (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)
                        (1): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False)
                      )
                    )
                    (kernel_func): ReLU()
                    (proj): ConvLayer(
                      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
                      (norm): StraightThrough()
                    )
                  )
                  (shortcut): IdentityLayer()
                )
                (qkv): QuantModule(
                  256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
                (aggreg): ModuleList(
                  (0): Sequential(
                    (0): QuantModule(
                      768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False
                      (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                      (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                      (activation_function): StraightThrough()
                    )
                    (1): QuantModule(
                      768, 768, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False
                      (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                      (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                      (activation_function): StraightThrough()
                    )
                  )
                )
                (proj): QuantModule(
                  512, 256, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
                (kernel_func): ReLU()
              )
              (local_module): QauntMBBlock(
                (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
                (inv_res): ResidualBlock(
                  (main): MBConv(
                    (inverted_conv): ConvLayer(
                      (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
                      (act): Hardswish()
                    )
                    (depth_conv): ConvLayer(
                      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
                      (act): Hardswish()
                    )
                    (point_conv): ConvLayer(
                      (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
                      (norm): StraightThrough()
                    )
                  )
                  (shortcut): IdentityLayer()
                )
                (conv): Sequential(
                  (0): QuantModule(
                    256, 1024, kernel_size=(1, 1), stride=(1, 1)
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): Hardswish()
                  )
                  (1): QuantModule_Shifted(
                    1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (input_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (output_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): Hardswish()
                  )
                  (2): QuantModule_Shifted(
                    1024, 256, kernel_size=(1, 1), stride=(1, 1)
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (input_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (output_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): StraightThrough()
                  )
                )
              )
            )
            (4): EfficientViTBlock(
              (context_module): QauntMBBlock(
                (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
                (inv_res): ResidualBlock(
                  (main): LiteMSA(
                    (qkv): ConvLayer(
                      (conv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
                    )
                    (aggreg): ModuleList(
                      (0): Sequential(
                        (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)
                        (1): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False)
                      )
                    )
                    (kernel_func): ReLU()
                    (proj): ConvLayer(
                      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
                      (norm): StraightThrough()
                    )
                  )
                  (shortcut): IdentityLayer()
                )
                (qkv): QuantModule(
                  256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
                (aggreg): ModuleList(
                  (0): Sequential(
                    (0): QuantModule(
                      768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False
                      (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                      (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                      (activation_function): StraightThrough()
                    )
                    (1): QuantModule(
                      768, 768, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False
                      (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                      (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                      (activation_function): StraightThrough()
                    )
                  )
                )
                (proj): QuantModule(
                  512, 256, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
                (kernel_func): ReLU()
              )
              (local_module): QauntMBBlock(
                (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
                (inv_res): ResidualBlock(
                  (main): MBConv(
                    (inverted_conv): ConvLayer(
                      (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
                      (act): Hardswish()
                    )
                    (depth_conv): ConvLayer(
                      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
                      (act): Hardswish()
                    )
                    (point_conv): ConvLayer(
                      (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
                      (norm): StraightThrough()
                    )
                  )
                  (shortcut): IdentityLayer()
                )
                (conv): Sequential(
                  (0): QuantModule(
                    256, 1024, kernel_size=(1, 1), stride=(1, 1)
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): Hardswish()
                  )
                  (1): QuantModule_Shifted(
                    1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (input_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (output_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): Hardswish()
                  )
                  (2): QuantModule_Shifted(
                    1024, 256, kernel_size=(1, 1), stride=(1, 1)
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (input_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (output_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): StraightThrough()
                  )
                )
              )
            )
          )
        )
      )
    )
    (head): ClsHead(
      (op_list): ModuleList(
        (0): ConvLayer(
          (conv): QuantModule(
            256, 1536, kernel_size=(1, 1), stride=(1, 1)
            (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
            (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (activation_function): Hardswish()
          )
          (norm): StraightThrough()
          (act): StraightThrough()
        )
        (1): AdaptiveAvgPool2d(output_size=1)
        (2): LinearLayer(
          (linear): QuantModule(
            in_features=1536, out_features=1600, bias=False
            (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
            (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (activation_function): StraightThrough()
          )
          (norm): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)
          (act): Hardswish()
        )
        (3): LinearLayer(
          (linear): QuantModule(
            in_features=1600, out_features=1000, bias=True
            (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
            (act_quantizer): UniformAffineQuantizer(bit=12, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (activation_function): StraightThrough()
          )
        )
      )
    )
  )
)
Test: [  0/782]	Time  1.543 ( 1.543)	Acc@1  90.62 ( 90.62)	Acc@5  96.88 ( 96.88)
Test: [200/782]	Time  0.374 ( 0.442)	Acc@1  87.50 ( 84.06)	Acc@5  95.31 ( 96.44)
Test: [400/782]	Time  0.037 ( 0.302)	Acc@1  90.62 ( 81.87)	Acc@5 100.00 ( 95.59)
Test: [600/782]	Time  0.041 ( 0.254)	Acc@1  81.25 ( 80.15)	Acc@5  96.88 ( 94.62)
 * Acc@1 79.300 Acc@5 94.322
Quantized accuracy before brecq: 79.29999542236328
Reconstruction for layer conv
Init alpha to be FP32
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3500
Total loss:	203.092 (rec:0.000, round:203.092)	b=20.00	count=4000
Total loss:	62.563 (rec:0.000, round:62.563)	b=19.44	count=4500
Total loss:	51.255 (rec:0.000, round:51.255)	b=18.88	count=5000
Total loss:	42.414 (rec:0.000, round:42.414)	b=18.31	count=5500
Total loss:	36.464 (rec:0.000, round:36.464)	b=17.75	count=6000
Total loss:	31.478 (rec:0.000, round:31.478)	b=17.19	count=6500
Total loss:	24.578 (rec:0.000, round:24.578)	b=16.62	count=7000
Total loss:	18.004 (rec:0.000, round:18.004)	b=16.06	count=7500
Total loss:	13.998 (rec:0.000, round:13.998)	b=15.50	count=8000
Total loss:	13.000 (rec:0.000, round:13.000)	b=14.94	count=8500
Total loss:	11.051 (rec:0.000, round:11.051)	b=14.38	count=9000
Total loss:	8.458 (rec:0.000, round:8.458)	b=13.81	count=9500
Total loss:	7.473 (rec:0.000, round:7.473)	b=13.25	count=10000
Total loss:	5.500 (rec:0.000, round:5.500)	b=12.69	count=10500
Total loss:	5.000 (rec:0.000, round:5.000)	b=12.12	count=11000
Total loss:	3.604 (rec:0.000, round:3.603)	b=11.56	count=11500
Total loss:	3.000 (rec:0.000, round:3.000)	b=11.00	count=12000
Total loss:	3.000 (rec:0.000, round:3.000)	b=10.44	count=12500
Total loss:	3.000 (rec:0.000, round:3.000)	b=9.88	count=13000
Total loss:	3.000 (rec:0.000, round:3.000)	b=9.31	count=13500
Total loss:	3.000 (rec:0.000, round:3.000)	b=8.75	count=14000
Total loss:	2.962 (rec:0.000, round:2.962)	b=8.19	count=14500
Total loss:	2.500 (rec:0.000, round:2.500)	b=7.62	count=15000
Total loss:	2.500 (rec:0.000, round:2.500)	b=7.06	count=15500
Total loss:	2.500 (rec:0.000, round:2.500)	b=6.50	count=16000
Total loss:	2.217 (rec:0.000, round:2.217)	b=5.94	count=16500
Total loss:	1.609 (rec:0.000, round:1.608)	b=5.38	count=17000
Total loss:	1.000 (rec:0.000, round:1.000)	b=4.81	count=17500
Total loss:	0.971 (rec:0.000, round:0.971)	b=4.25	count=18000
Total loss:	0.254 (rec:0.000, round:0.254)	b=3.69	count=18500
Total loss:	0.000 (rec:0.000, round:0.000)	b=3.12	count=19000
Total loss:	0.000 (rec:0.000, round:0.000)	b=2.56	count=19500
Total loss:	0.000 (rec:0.000, round:0.000)	b=2.00	count=20000
Reconstruction for block 1
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3500
Total loss:	173.501 (rec:0.000, round:173.501)	b=20.00	count=4000
Total loss:	68.099 (rec:0.000, round:68.099)	b=19.44	count=4500
Total loss:	55.981 (rec:0.000, round:55.980)	b=18.88	count=5000
Total loss:	48.939 (rec:0.000, round:48.938)	b=18.31	count=5500
Total loss:	46.000 (rec:0.000, round:46.000)	b=17.75	count=6000
Total loss:	43.740 (rec:0.000, round:43.739)	b=17.19	count=6500
Total loss:	42.491 (rec:0.000, round:42.490)	b=16.62	count=7000
Total loss:	39.712 (rec:0.000, round:39.711)	b=16.06	count=7500
Total loss:	38.411 (rec:0.000, round:38.410)	b=15.50	count=8000
Total loss:	34.761 (rec:0.001, round:34.761)	b=14.94	count=8500
Total loss:	29.905 (rec:0.001, round:29.905)	b=14.38	count=9000
Total loss:	23.974 (rec:0.001, round:23.974)	b=13.81	count=9500
Total loss:	20.514 (rec:0.001, round:20.513)	b=13.25	count=10000
Total loss:	16.911 (rec:0.001, round:16.910)	b=12.69	count=10500
Total loss:	14.264 (rec:0.001, round:14.263)	b=12.12	count=11000
Total loss:	10.910 (rec:0.001, round:10.909)	b=11.56	count=11500
Total loss:	8.720 (rec:0.001, round:8.719)	b=11.00	count=12000
Total loss:	6.698 (rec:0.001, round:6.697)	b=10.44	count=12500
Total loss:	4.517 (rec:0.001, round:4.515)	b=9.88	count=13000
Total loss:	3.001 (rec:0.001, round:3.000)	b=9.31	count=13500
Total loss:	2.628 (rec:0.001, round:2.627)	b=8.75	count=14000
Total loss:	1.995 (rec:0.001, round:1.993)	b=8.19	count=14500
Total loss:	1.001 (rec:0.001, round:1.000)	b=7.62	count=15000
Total loss:	0.869 (rec:0.001, round:0.868)	b=7.06	count=15500
Total loss:	0.501 (rec:0.001, round:0.500)	b=6.50	count=16000
Total loss:	0.501 (rec:0.001, round:0.500)	b=5.94	count=16500
Total loss:	0.501 (rec:0.001, round:0.500)	b=5.38	count=17000
Total loss:	0.501 (rec:0.001, round:0.500)	b=4.81	count=17500
Total loss:	0.062 (rec:0.001, round:0.060)	b=4.25	count=18000
Total loss:	0.001 (rec:0.001, round:0.000)	b=3.69	count=18500
Total loss:	0.001 (rec:0.001, round:0.000)	b=3.12	count=19000
Total loss:	0.002 (rec:0.002, round:0.000)	b=2.56	count=19500
Total loss:	0.002 (rec:0.002, round:0.000)	b=2.00	count=20000
Reconstruction for block 0
Init alpha to be FP32
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	429.961 (rec:0.001, round:429.960)	b=20.00	count=4000
Total loss:	161.276 (rec:0.001, round:161.275)	b=19.44	count=4500
Total loss:	131.057 (rec:0.001, round:131.056)	b=18.88	count=5000
Total loss:	113.636 (rec:0.001, round:113.634)	b=18.31	count=5500
Total loss:	106.088 (rec:0.001, round:106.087)	b=17.75	count=6000
Total loss:	97.470 (rec:0.001, round:97.468)	b=17.19	count=6500
Total loss:	89.794 (rec:0.001, round:89.793)	b=16.62	count=7000
Total loss:	86.132 (rec:0.001, round:86.130)	b=16.06	count=7500
Total loss:	79.932 (rec:0.001, round:79.930)	b=15.50	count=8000
Total loss:	76.692 (rec:0.001, round:76.691)	b=14.94	count=8500
Total loss:	71.484 (rec:0.001, round:71.483)	b=14.38	count=9000
Total loss:	59.783 (rec:0.002, round:59.781)	b=13.81	count=9500
Total loss:	52.040 (rec:0.001, round:52.039)	b=13.25	count=10000
Total loss:	48.905 (rec:0.002, round:48.903)	b=12.69	count=10500
Total loss:	42.880 (rec:0.002, round:42.878)	b=12.12	count=11000
Total loss:	37.073 (rec:0.002, round:37.071)	b=11.56	count=11500
Total loss:	28.819 (rec:0.002, round:28.817)	b=11.00	count=12000
Total loss:	21.627 (rec:0.002, round:21.625)	b=10.44	count=12500
Total loss:	16.979 (rec:0.002, round:16.977)	b=9.88	count=13000
Total loss:	11.367 (rec:0.002, round:11.365)	b=9.31	count=13500
Total loss:	8.102 (rec:0.002, round:8.100)	b=8.75	count=14000
Total loss:	7.174 (rec:0.003, round:7.172)	b=8.19	count=14500
Total loss:	6.015 (rec:0.003, round:6.012)	b=7.62	count=15000
Total loss:	5.503 (rec:0.003, round:5.500)	b=7.06	count=15500
Total loss:	3.370 (rec:0.003, round:3.366)	b=6.50	count=16000
Total loss:	0.526 (rec:0.003, round:0.523)	b=5.94	count=16500
Total loss:	0.401 (rec:0.004, round:0.398)	b=5.38	count=17000
Total loss:	0.004 (rec:0.004, round:0.000)	b=4.81	count=17500
Total loss:	0.004 (rec:0.004, round:0.000)	b=4.25	count=18000
Total loss:	0.004 (rec:0.004, round:0.000)	b=3.69	count=18500
Total loss:	0.004 (rec:0.004, round:0.000)	b=3.12	count=19000
Total loss:	0.004 (rec:0.004, round:0.000)	b=2.56	count=19500
Total loss:	0.004 (rec:0.004, round:0.000)	b=2.00	count=20000
Reconstruction for block 1
Init alpha to be FP32
Total loss:	0.014 (rec:0.014, round:0.000)	b=0.00	count=500
Total loss:	0.015 (rec:0.015, round:0.000)	b=0.00	count=1000
Total loss:	0.015 (rec:0.015, round:0.000)	b=0.00	count=1500
Total loss:	0.013 (rec:0.013, round:0.000)	b=0.00	count=2000
Total loss:	0.011 (rec:0.011, round:0.000)	b=0.00	count=2500
Total loss:	0.012 (rec:0.012, round:0.000)	b=0.00	count=3000
Total loss:	0.012 (rec:0.012, round:0.000)	b=0.00	count=3500
Total loss:	1540.183 (rec:0.014, round:1540.169)	b=20.00	count=4000
Total loss:	559.404 (rec:0.016, round:559.388)	b=19.44	count=4500
Total loss:	452.941 (rec:0.012, round:452.928)	b=18.88	count=5000
Total loss:	391.946 (rec:0.016, round:391.931)	b=18.31	count=5500
Total loss:	345.387 (rec:0.017, round:345.370)	b=17.75	count=6000
Total loss:	312.932 (rec:0.016, round:312.916)	b=17.19	count=6500
Total loss:	286.705 (rec:0.013, round:286.692)	b=16.62	count=7000
Total loss:	262.532 (rec:0.012, round:262.520)	b=16.06	count=7500
Total loss:	243.454 (rec:0.013, round:243.440)	b=15.50	count=8000
Total loss:	221.070 (rec:0.013, round:221.058)	b=14.94	count=8500
Total loss:	197.525 (rec:0.012, round:197.513)	b=14.38	count=9000
Total loss:	175.588 (rec:0.014, round:175.575)	b=13.81	count=9500
Total loss:	162.416 (rec:0.014, round:162.402)	b=13.25	count=10000
Total loss:	142.635 (rec:0.013, round:142.623)	b=12.69	count=10500
Total loss:	124.160 (rec:0.014, round:124.146)	b=12.12	count=11000
Total loss:	102.061 (rec:0.014, round:102.047)	b=11.56	count=11500
Total loss:	85.386 (rec:0.013, round:85.373)	b=11.00	count=12000
Total loss:	70.322 (rec:0.013, round:70.309)	b=10.44	count=12500
Total loss:	56.596 (rec:0.012, round:56.584)	b=9.88	count=13000
Total loss:	45.926 (rec:0.014, round:45.912)	b=9.31	count=13500
Total loss:	37.975 (rec:0.012, round:37.962)	b=8.75	count=14000
Total loss:	28.003 (rec:0.014, round:27.990)	b=8.19	count=14500
Total loss:	19.730 (rec:0.015, round:19.716)	b=7.62	count=15000
Total loss:	14.467 (rec:0.015, round:14.451)	b=7.06	count=15500
Total loss:	9.008 (rec:0.014, round:8.994)	b=6.50	count=16000
Total loss:	4.425 (rec:0.015, round:4.410)	b=5.94	count=16500
Total loss:	0.690 (rec:0.013, round:0.677)	b=5.38	count=17000
Total loss:	0.364 (rec:0.014, round:0.350)	b=4.81	count=17500
Total loss:	0.013 (rec:0.013, round:0.000)	b=4.25	count=18000
Total loss:	0.015 (rec:0.015, round:0.000)	b=3.69	count=18500
Total loss:	0.013 (rec:0.013, round:0.000)	b=3.12	count=19000
Total loss:	0.014 (rec:0.014, round:0.000)	b=2.56	count=19500
Total loss:	0.015 (rec:0.015, round:0.000)	b=2.00	count=20000
Reconstruction for block 0
Init alpha to be FP32
Total loss:	0.008 (rec:0.008, round:0.000)	b=0.00	count=500
Total loss:	0.007 (rec:0.007, round:0.000)	b=0.00	count=1000
Total loss:	0.008 (rec:0.008, round:0.000)	b=0.00	count=1500
Total loss:	0.007 (rec:0.007, round:0.000)	b=0.00	count=2000
Total loss:	0.007 (rec:0.007, round:0.000)	b=0.00	count=2500
Total loss:	0.007 (rec:0.007, round:0.000)	b=0.00	count=3000
Total loss:	0.008 (rec:0.008, round:0.000)	b=0.00	count=3500
Total loss:	1729.502 (rec:0.007, round:1729.495)	b=20.00	count=4000
Total loss:	705.542 (rec:0.008, round:705.534)	b=19.44	count=4500
Total loss:	603.966 (rec:0.007, round:603.959)	b=18.88	count=5000
Total loss:	546.172 (rec:0.007, round:546.165)	b=18.31	count=5500
Total loss:	499.890 (rec:0.007, round:499.882)	b=17.75	count=6000
Total loss:	454.274 (rec:0.008, round:454.266)	b=17.19	count=6500
Total loss:	416.188 (rec:0.007, round:416.181)	b=16.62	count=7000
Total loss:	386.299 (rec:0.008, round:386.292)	b=16.06	count=7500
Total loss:	350.193 (rec:0.008, round:350.185)	b=15.50	count=8000
Total loss:	322.817 (rec:0.008, round:322.810)	b=14.94	count=8500
Total loss:	288.826 (rec:0.008, round:288.819)	b=14.38	count=9000
Total loss:	260.752 (rec:0.008, round:260.744)	b=13.81	count=9500
Total loss:	232.304 (rec:0.008, round:232.297)	b=13.25	count=10000
Total loss:	199.846 (rec:0.008, round:199.838)	b=12.69	count=10500
Total loss:	174.300 (rec:0.008, round:174.293)	b=12.12	count=11000
Total loss:	151.104 (rec:0.009, round:151.095)	b=11.56	count=11500
Total loss:	120.513 (rec:0.008, round:120.504)	b=11.00	count=12000
Total loss:	91.538 (rec:0.008, round:91.529)	b=10.44	count=12500
Total loss:	68.979 (rec:0.009, round:68.969)	b=9.88	count=13000
Total loss:	56.217 (rec:0.009, round:56.209)	b=9.31	count=13500
Total loss:	46.903 (rec:0.008, round:46.895)	b=8.75	count=14000
Total loss:	37.359 (rec:0.010, round:37.349)	b=8.19	count=14500
Total loss:	29.178 (rec:0.009, round:29.169)	b=7.62	count=15000
Total loss:	20.486 (rec:0.009, round:20.477)	b=7.06	count=15500
Total loss:	13.450 (rec:0.009, round:13.441)	b=6.50	count=16000
Total loss:	9.114 (rec:0.009, round:9.105)	b=5.94	count=16500
Total loss:	5.647 (rec:0.009, round:5.638)	b=5.38	count=17000
Total loss:	2.497 (rec:0.009, round:2.488)	b=4.81	count=17500
Total loss:	0.934 (rec:0.009, round:0.925)	b=4.25	count=18000
Total loss:	0.048 (rec:0.009, round:0.039)	b=3.69	count=18500
Total loss:	0.009 (rec:0.009, round:0.000)	b=3.12	count=19000
Total loss:	0.010 (rec:0.010, round:0.000)	b=2.56	count=19500
Total loss:	0.010 (rec:0.010, round:0.000)	b=2.00	count=20000
Reconstruction for block 1
Init alpha to be FP32
Total loss:	0.014 (rec:0.014, round:0.000)	b=0.00	count=500
Total loss:	0.015 (rec:0.015, round:0.000)	b=0.00	count=1000
Total loss:	0.014 (rec:0.014, round:0.000)	b=0.00	count=1500
Total loss:	0.015 (rec:0.015, round:0.000)	b=0.00	count=2000
Total loss:	0.015 (rec:0.015, round:0.000)	b=0.00	count=2500
Total loss:	0.014 (rec:0.014, round:0.000)	b=0.00	count=3000
Total loss:	0.015 (rec:0.015, round:0.000)	b=0.00	count=3500
Total loss:	7019.932 (rec:0.015, round:7019.917)	b=20.00	count=4000
Total loss:	2626.035 (rec:0.014, round:2626.021)	b=19.44	count=4500
Total loss:	2212.416 (rec:0.014, round:2212.402)	b=18.88	count=5000
Total loss:	1954.616 (rec:0.016, round:1954.600)	b=18.31	count=5500
Total loss:	1729.997 (rec:0.015, round:1729.981)	b=17.75	count=6000
Total loss:	1546.182 (rec:0.014, round:1546.169)	b=17.19	count=6500
Total loss:	1404.149 (rec:0.015, round:1404.134)	b=16.62	count=7000
Total loss:	1279.345 (rec:0.014, round:1279.331)	b=16.06	count=7500
Total loss:	1154.098 (rec:0.016, round:1154.082)	b=15.50	count=8000
Total loss:	1061.183 (rec:0.014, round:1061.169)	b=14.94	count=8500
Total loss:	955.818 (rec:0.013, round:955.805)	b=14.38	count=9000
Total loss:	859.746 (rec:0.014, round:859.732)	b=13.81	count=9500
Total loss:	761.414 (rec:0.015, round:761.399)	b=13.25	count=10000
Total loss:	670.469 (rec:0.015, round:670.454)	b=12.69	count=10500
Total loss:	576.374 (rec:0.016, round:576.359)	b=12.12	count=11000
Total loss:	498.435 (rec:0.014, round:498.421)	b=11.56	count=11500
Total loss:	427.089 (rec:0.016, round:427.073)	b=11.00	count=12000
Total loss:	348.331 (rec:0.014, round:348.317)	b=10.44	count=12500
Total loss:	289.041 (rec:0.016, round:289.024)	b=9.88	count=13000
Total loss:	224.015 (rec:0.016, round:223.999)	b=9.31	count=13500
Total loss:	169.420 (rec:0.015, round:169.405)	b=8.75	count=14000
Total loss:	124.479 (rec:0.014, round:124.465)	b=8.19	count=14500
Total loss:	84.936 (rec:0.017, round:84.919)	b=7.62	count=15000
Total loss:	52.447 (rec:0.017, round:52.430)	b=7.06	count=15500
Total loss:	30.249 (rec:0.016, round:30.233)	b=6.50	count=16000
Total loss:	18.933 (rec:0.015, round:18.918)	b=5.94	count=16500
Total loss:	12.526 (rec:0.015, round:12.510)	b=5.38	count=17000
Total loss:	6.938 (rec:0.016, round:6.922)	b=4.81	count=17500
Total loss:	2.963 (rec:0.014, round:2.949)	b=4.25	count=18000
Total loss:	0.356 (rec:0.016, round:0.341)	b=3.69	count=18500
Total loss:	0.015 (rec:0.015, round:0.000)	b=3.12	count=19000
Total loss:	0.016 (rec:0.016, round:0.000)	b=2.56	count=19500
Total loss:	0.016 (rec:0.016, round:0.000)	b=2.00	count=20000
Reconstruction for block 2
Init alpha to be FP32
Total loss:	0.022 (rec:0.022, round:0.000)	b=0.00	count=500
Total loss:	0.024 (rec:0.024, round:0.000)	b=0.00	count=1000
Total loss:	0.020 (rec:0.020, round:0.000)	b=0.00	count=1500
Total loss:	0.021 (rec:0.021, round:0.000)	b=0.00	count=2000
Total loss:	0.021 (rec:0.021, round:0.000)	b=0.00	count=2500
Total loss:	0.021 (rec:0.021, round:0.000)	b=0.00	count=3000
Total loss:	0.023 (rec:0.023, round:0.000)	b=0.00	count=3500
Total loss:	7176.494 (rec:0.020, round:7176.474)	b=20.00	count=4000
Total loss:	2800.773 (rec:0.022, round:2800.751)	b=19.44	count=4500
Total loss:	2448.229 (rec:0.022, round:2448.208)	b=18.88	count=5000
Total loss:	2226.023 (rec:0.021, round:2226.002)	b=18.31	count=5500
Total loss:	2043.880 (rec:0.022, round:2043.858)	b=17.75	count=6000
Total loss:	1910.177 (rec:0.022, round:1910.155)	b=17.19	count=6500
Total loss:	1773.166 (rec:0.022, round:1773.144)	b=16.62	count=7000
Total loss:	1619.186 (rec:0.023, round:1619.163)	b=16.06	count=7500
Total loss:	1499.956 (rec:0.022, round:1499.934)	b=15.50	count=8000
Total loss:	1390.812 (rec:0.021, round:1390.791)	b=14.94	count=8500
Total loss:	1271.005 (rec:0.022, round:1270.983)	b=14.38	count=9000
Total loss:	1157.741 (rec:0.023, round:1157.718)	b=13.81	count=9500
Total loss:	1041.852 (rec:0.021, round:1041.830)	b=13.25	count=10000
Total loss:	932.742 (rec:0.024, round:932.718)	b=12.69	count=10500
Total loss:	820.177 (rec:0.021, round:820.155)	b=12.12	count=11000
Total loss:	705.785 (rec:0.022, round:705.763)	b=11.56	count=11500
Total loss:	590.013 (rec:0.024, round:589.989)	b=11.00	count=12000
Total loss:	484.187 (rec:0.022, round:484.165)	b=10.44	count=12500
Total loss:	388.120 (rec:0.026, round:388.094)	b=9.88	count=13000
Total loss:	303.680 (rec:0.025, round:303.655)	b=9.31	count=13500
Total loss:	222.839 (rec:0.024, round:222.815)	b=8.75	count=14000
Total loss:	167.734 (rec:0.021, round:167.712)	b=8.19	count=14500
Total loss:	115.139 (rec:0.023, round:115.116)	b=7.62	count=15000
Total loss:	72.639 (rec:0.023, round:72.616)	b=7.06	count=15500
Total loss:	40.939 (rec:0.024, round:40.916)	b=6.50	count=16000
Total loss:	21.649 (rec:0.023, round:21.626)	b=5.94	count=16500
Total loss:	8.650 (rec:0.023, round:8.627)	b=5.38	count=17000
Total loss:	1.813 (rec:0.022, round:1.791)	b=4.81	count=17500
Total loss:	0.030 (rec:0.023, round:0.008)	b=4.25	count=18000
Total loss:	0.023 (rec:0.023, round:0.000)	b=3.69	count=18500
Total loss:	0.024 (rec:0.024, round:0.000)	b=3.12	count=19000
Total loss:	0.022 (rec:0.022, round:0.000)	b=2.56	count=19500
Total loss:	0.023 (rec:0.023, round:0.000)	b=2.00	count=20000
Reconstruction for block 0
Init alpha to be FP32
Total loss:	0.016 (rec:0.016, round:0.000)	b=0.00	count=500
Total loss:	0.014 (rec:0.014, round:0.000)	b=0.00	count=1000
Total loss:	0.015 (rec:0.015, round:0.000)	b=0.00	count=1500
Total loss:	0.013 (rec:0.013, round:0.000)	b=0.00	count=2000
Total loss:	0.014 (rec:0.014, round:0.000)	b=0.00	count=2500
Total loss:	0.013 (rec:0.013, round:0.000)	b=0.00	count=3000
Total loss:	0.014 (rec:0.014, round:0.000)	b=0.00	count=3500
Total loss:	7352.548 (rec:0.014, round:7352.534)	b=20.00	count=4000
Total loss:	2949.989 (rec:0.015, round:2949.973)	b=19.44	count=4500
Total loss:	2540.677 (rec:0.015, round:2540.662)	b=18.88	count=5000
Total loss:	2268.108 (rec:0.016, round:2268.092)	b=18.31	count=5500
Total loss:	2061.070 (rec:0.017, round:2061.053)	b=17.75	count=6000
Total loss:	1867.764 (rec:0.016, round:1867.749)	b=17.19	count=6500
Total loss:	1703.798 (rec:0.017, round:1703.782)	b=16.62	count=7000
Total loss:	1555.442 (rec:0.017, round:1555.425)	b=16.06	count=7500
Total loss:	1428.130 (rec:0.016, round:1428.114)	b=15.50	count=8000
Total loss:	1295.475 (rec:0.017, round:1295.458)	b=14.94	count=8500
Total loss:	1179.600 (rec:0.015, round:1179.585)	b=14.38	count=9000
Total loss:	1052.617 (rec:0.017, round:1052.600)	b=13.81	count=9500
Total loss:	924.330 (rec:0.016, round:924.313)	b=13.25	count=10000
Total loss:	812.743 (rec:0.017, round:812.727)	b=12.69	count=10500
Total loss:	699.785 (rec:0.017, round:699.768)	b=12.12	count=11000
Total loss:	595.257 (rec:0.016, round:595.240)	b=11.56	count=11500
Total loss:	486.939 (rec:0.019, round:486.920)	b=11.00	count=12000
Total loss:	395.205 (rec:0.018, round:395.187)	b=10.44	count=12500
Total loss:	302.573 (rec:0.017, round:302.557)	b=9.88	count=13000
Total loss:	222.557 (rec:0.017, round:222.540)	b=9.31	count=13500
Total loss:	170.054 (rec:0.018, round:170.036)	b=8.75	count=14000
Total loss:	128.962 (rec:0.018, round:128.944)	b=8.19	count=14500
Total loss:	92.555 (rec:0.019, round:92.536)	b=7.62	count=15000
Total loss:	64.014 (rec:0.018, round:63.996)	b=7.06	count=15500
Total loss:	39.383 (rec:0.018, round:39.365)	b=6.50	count=16000
Total loss:	21.800 (rec:0.020, round:21.780)	b=5.94	count=16500
Total loss:	10.334 (rec:0.019, round:10.316)	b=5.38	count=17000
Total loss:	4.331 (rec:0.019, round:4.311)	b=4.81	count=17500
Total loss:	1.021 (rec:0.018, round:1.003)	b=4.25	count=18000
Total loss:	0.140 (rec:0.019, round:0.120)	b=3.69	count=18500
Total loss:	0.019 (rec:0.019, round:0.000)	b=3.12	count=19000
Total loss:	0.017 (rec:0.017, round:0.000)	b=2.56	count=19500
Total loss:	0.020 (rec:0.020, round:0.000)	b=2.00	count=20000
Reconstruction for block context_module
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.018 (rec:0.018, round:0.000)	b=0.00	count=500
Total loss:	0.018 (rec:0.018, round:0.000)	b=0.00	count=1000
Total loss:	0.018 (rec:0.018, round:0.000)	b=0.00	count=1500
Total loss:	0.017 (rec:0.017, round:0.000)	b=0.00	count=2000
Total loss:	0.017 (rec:0.017, round:0.000)	b=0.00	count=2500
Total loss:	0.018 (rec:0.018, round:0.000)	b=0.00	count=3000
Total loss:	0.018 (rec:0.018, round:0.000)	b=0.00	count=3500
Total loss:	45216.379 (rec:0.018, round:45216.359)	b=20.00	count=4000
Total loss:	15640.537 (rec:0.019, round:15640.519)	b=19.44	count=4500
Total loss:	13957.992 (rec:0.017, round:13957.976)	b=18.88	count=5000
Total loss:	12911.264 (rec:0.017, round:12911.246)	b=18.31	count=5500
Total loss:	11964.363 (rec:0.018, round:11964.346)	b=17.75	count=6000
Total loss:	11085.355 (rec:0.017, round:11085.338)	b=17.19	count=6500
Total loss:	10236.438 (rec:0.018, round:10236.420)	b=16.62	count=7000
Total loss:	9406.209 (rec:0.016, round:9406.192)	b=16.06	count=7500
Total loss:	8578.273 (rec:0.017, round:8578.257)	b=15.50	count=8000
Total loss:	7808.928 (rec:0.017, round:7808.911)	b=14.94	count=8500
Total loss:	7097.219 (rec:0.018, round:7097.201)	b=14.38	count=9000
Total loss:	6326.919 (rec:0.018, round:6326.901)	b=13.81	count=9500
Total loss:	5567.157 (rec:0.017, round:5567.141)	b=13.25	count=10000
Total loss:	4875.340 (rec:0.017, round:4875.322)	b=12.69	count=10500
Total loss:	4181.734 (rec:0.017, round:4181.717)	b=12.12	count=11000
Total loss:	3496.114 (rec:0.018, round:3496.096)	b=11.56	count=11500
Total loss:	2878.360 (rec:0.018, round:2878.343)	b=11.00	count=12000
Total loss:	2269.749 (rec:0.018, round:2269.730)	b=10.44	count=12500
Total loss:	1679.115 (rec:0.019, round:1679.096)	b=9.88	count=13000
Total loss:	1174.053 (rec:0.019, round:1174.034)	b=9.31	count=13500
Total loss:	818.220 (rec:0.017, round:818.202)	b=8.75	count=14000
Total loss:	551.786 (rec:0.019, round:551.767)	b=8.19	count=14500
Total loss:	367.023 (rec:0.019, round:367.004)	b=7.62	count=15000
Total loss:	246.626 (rec:0.018, round:246.608)	b=7.06	count=15500
Total loss:	152.267 (rec:0.020, round:152.247)	b=6.50	count=16000
Total loss:	89.826 (rec:0.018, round:89.808)	b=5.94	count=16500
Total loss:	49.104 (rec:0.019, round:49.086)	b=5.38	count=17000
Total loss:	24.775 (rec:0.018, round:24.756)	b=4.81	count=17500
Total loss:	7.981 (rec:0.020, round:7.960)	b=4.25	count=18000
Total loss:	1.367 (rec:0.018, round:1.348)	b=3.69	count=18500
Total loss:	0.123 (rec:0.018, round:0.105)	b=3.12	count=19000
Total loss:	0.019 (rec:0.019, round:0.000)	b=2.56	count=19500
Total loss:	0.018 (rec:0.018, round:0.000)	b=2.00	count=20000
Reconstruction for block local_module
Init alpha to be FP32
Total loss:	0.025 (rec:0.025, round:0.000)	b=0.00	count=500
Total loss:	0.024 (rec:0.024, round:0.000)	b=0.00	count=1000
Total loss:	0.026 (rec:0.026, round:0.000)	b=0.00	count=1500
Total loss:	0.026 (rec:0.026, round:0.000)	b=0.00	count=2000
Total loss:	0.024 (rec:0.024, round:0.000)	b=0.00	count=2500
Total loss:	0.026 (rec:0.026, round:0.000)	b=0.00	count=3000
Total loss:	0.025 (rec:0.025, round:0.000)	b=0.00	count=3500
Total loss:	28676.678 (rec:0.025, round:28676.652)	b=20.00	count=4000
Total loss:	10488.743 (rec:0.026, round:10488.718)	b=19.44	count=4500
Total loss:	9303.469 (rec:0.025, round:9303.443)	b=18.88	count=5000
Total loss:	8453.633 (rec:0.028, round:8453.605)	b=18.31	count=5500
Total loss:	7725.120 (rec:0.024, round:7725.096)	b=17.75	count=6000
Total loss:	7069.440 (rec:0.026, round:7069.415)	b=17.19	count=6500
Total loss:	6461.137 (rec:0.027, round:6461.110)	b=16.62	count=7000
Total loss:	5902.104 (rec:0.026, round:5902.077)	b=16.06	count=7500
Total loss:	5362.016 (rec:0.025, round:5361.991)	b=15.50	count=8000
Total loss:	4871.292 (rec:0.025, round:4871.267)	b=14.94	count=8500
Total loss:	4379.333 (rec:0.026, round:4379.308)	b=14.38	count=9000
Total loss:	3925.672 (rec:0.025, round:3925.647)	b=13.81	count=9500
Total loss:	3482.323 (rec:0.026, round:3482.297)	b=13.25	count=10000
Total loss:	3067.136 (rec:0.024, round:3067.111)	b=12.69	count=10500
Total loss:	2674.541 (rec:0.026, round:2674.515)	b=12.12	count=11000
Total loss:	2301.922 (rec:0.027, round:2301.895)	b=11.56	count=11500
Total loss:	1950.686 (rec:0.026, round:1950.659)	b=11.00	count=12000
Total loss:	1621.525 (rec:0.024, round:1621.501)	b=10.44	count=12500
Total loss:	1322.583 (rec:0.027, round:1322.556)	b=9.88	count=13000
Total loss:	1030.543 (rec:0.027, round:1030.516)	b=9.31	count=13500
Total loss:	792.775 (rec:0.026, round:792.749)	b=8.75	count=14000
Total loss:	581.005 (rec:0.026, round:580.978)	b=8.19	count=14500
Total loss:	399.141 (rec:0.027, round:399.114)	b=7.62	count=15000
Total loss:	253.660 (rec:0.026, round:253.634)	b=7.06	count=15500
Total loss:	143.132 (rec:0.027, round:143.105)	b=6.50	count=16000
Total loss:	65.602 (rec:0.025, round:65.576)	b=5.94	count=16500
Total loss:	28.188 (rec:0.027, round:28.161)	b=5.38	count=17000
Total loss:	10.228 (rec:0.026, round:10.202)	b=4.81	count=17500
Total loss:	0.835 (rec:0.025, round:0.810)	b=4.25	count=18000
Total loss:	0.027 (rec:0.027, round:0.000)	b=3.69	count=18500
Total loss:	0.025 (rec:0.025, round:0.000)	b=3.12	count=19000
Total loss:	0.026 (rec:0.026, round:0.000)	b=2.56	count=19500
Total loss:	0.027 (rec:0.027, round:0.000)	b=2.00	count=20000
Reconstruction for block context_module
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.027 (rec:0.027, round:0.000)	b=0.00	count=500
Total loss:	0.028 (rec:0.028, round:0.000)	b=0.00	count=1000
Total loss:	0.028 (rec:0.028, round:0.000)	b=0.00	count=1500
Total loss:	0.027 (rec:0.027, round:0.000)	b=0.00	count=2000
Total loss:	0.028 (rec:0.028, round:0.000)	b=0.00	count=2500
Total loss:	0.027 (rec:0.027, round:0.000)	b=0.00	count=3000
Total loss:	0.029 (rec:0.029, round:0.000)	b=0.00	count=3500
Total loss:	45339.215 (rec:0.029, round:45339.188)	b=20.00	count=4000
Total loss:	16749.715 (rec:0.028, round:16749.688)	b=19.44	count=4500
Total loss:	15031.769 (rec:0.027, round:15031.742)	b=18.88	count=5000
Total loss:	13835.520 (rec:0.028, round:13835.492)	b=18.31	count=5500
Total loss:	12840.188 (rec:0.028, round:12840.159)	b=17.75	count=6000
Total loss:	11934.120 (rec:0.028, round:11934.092)	b=17.19	count=6500
Total loss:	11063.877 (rec:0.030, round:11063.848)	b=16.62	count=7000
Total loss:	10188.582 (rec:0.027, round:10188.555)	b=16.06	count=7500
Total loss:	9341.719 (rec:0.027, round:9341.691)	b=15.50	count=8000
Total loss:	8544.824 (rec:0.029, round:8544.795)	b=14.94	count=8500
Total loss:	7762.026 (rec:0.028, round:7761.998)	b=14.38	count=9000
Total loss:	6996.925 (rec:0.028, round:6996.896)	b=13.81	count=9500
Total loss:	6224.801 (rec:0.027, round:6224.774)	b=13.25	count=10000
Total loss:	5497.292 (rec:0.029, round:5497.262)	b=12.69	count=10500
Total loss:	4806.634 (rec:0.028, round:4806.606)	b=12.12	count=11000
Total loss:	4101.514 (rec:0.029, round:4101.484)	b=11.56	count=11500
Total loss:	3457.609 (rec:0.029, round:3457.581)	b=11.00	count=12000
Total loss:	2804.164 (rec:0.027, round:2804.137)	b=10.44	count=12500
Total loss:	2206.944 (rec:0.030, round:2206.914)	b=9.88	count=13000
Total loss:	1646.957 (rec:0.029, round:1646.928)	b=9.31	count=13500
Total loss:	1170.566 (rec:0.028, round:1170.538)	b=8.75	count=14000
Total loss:	727.703 (rec:0.029, round:727.674)	b=8.19	count=14500
Total loss:	388.210 (rec:0.028, round:388.182)	b=7.62	count=15000
Total loss:	170.142 (rec:0.028, round:170.114)	b=7.06	count=15500
Total loss:	86.479 (rec:0.032, round:86.448)	b=6.50	count=16000
Total loss:	46.472 (rec:0.030, round:46.443)	b=5.94	count=16500
Total loss:	22.275 (rec:0.028, round:22.247)	b=5.38	count=17000
Total loss:	13.622 (rec:0.030, round:13.592)	b=4.81	count=17500
Total loss:	4.548 (rec:0.030, round:4.519)	b=4.25	count=18000
Total loss:	0.947 (rec:0.029, round:0.918)	b=3.69	count=18500
Total loss:	0.286 (rec:0.030, round:0.256)	b=3.12	count=19000
Total loss:	0.031 (rec:0.031, round:0.000)	b=2.56	count=19500
Total loss:	0.030 (rec:0.030, round:0.000)	b=2.00	count=20000
Reconstruction for block local_module
Init alpha to be FP32
Total loss:	0.037 (rec:0.037, round:0.000)	b=0.00	count=500
Total loss:	0.036 (rec:0.036, round:0.000)	b=0.00	count=1000
Total loss:	0.035 (rec:0.035, round:0.000)	b=0.00	count=1500
Total loss:	0.036 (rec:0.036, round:0.000)	b=0.00	count=2000
Total loss:	0.034 (rec:0.034, round:0.000)	b=0.00	count=2500
Total loss:	0.037 (rec:0.037, round:0.000)	b=0.00	count=3000
Total loss:	0.037 (rec:0.037, round:0.000)	b=0.00	count=3500
Total loss:	29614.033 (rec:0.037, round:29613.996)	b=20.00	count=4000
Total loss:	11308.747 (rec:0.036, round:11308.711)	b=19.44	count=4500
Total loss:	10100.159 (rec:0.037, round:10100.123)	b=18.88	count=5000
Total loss:	9322.559 (rec:0.035, round:9322.523)	b=18.31	count=5500
Total loss:	8639.899 (rec:0.038, round:8639.861)	b=17.75	count=6000
Total loss:	8023.185 (rec:0.038, round:8023.146)	b=17.19	count=6500
Total loss:	7462.450 (rec:0.035, round:7462.414)	b=16.62	count=7000
Total loss:	6931.476 (rec:0.038, round:6931.438)	b=16.06	count=7500
Total loss:	6418.198 (rec:0.036, round:6418.162)	b=15.50	count=8000
Total loss:	5939.035 (rec:0.038, round:5938.997)	b=14.94	count=8500
Total loss:	5459.329 (rec:0.036, round:5459.292)	b=14.38	count=9000
Total loss:	4997.198 (rec:0.036, round:4997.162)	b=13.81	count=9500
Total loss:	4527.297 (rec:0.039, round:4527.258)	b=13.25	count=10000
Total loss:	4048.955 (rec:0.033, round:4048.922)	b=12.69	count=10500
Total loss:	3583.144 (rec:0.036, round:3583.108)	b=12.12	count=11000
Total loss:	3144.925 (rec:0.039, round:3144.887)	b=11.56	count=11500
Total loss:	2694.114 (rec:0.038, round:2694.076)	b=11.00	count=12000
Total loss:	2284.035 (rec:0.036, round:2283.999)	b=10.44	count=12500
Total loss:	1895.099 (rec:0.037, round:1895.062)	b=9.88	count=13000
Total loss:	1518.460 (rec:0.036, round:1518.424)	b=9.31	count=13500
Total loss:	1175.978 (rec:0.036, round:1175.942)	b=8.75	count=14000
Total loss:	853.625 (rec:0.037, round:853.588)	b=8.19	count=14500
Total loss:	576.464 (rec:0.036, round:576.428)	b=7.62	count=15000
Total loss:	363.370 (rec:0.034, round:363.336)	b=7.06	count=15500
Total loss:	207.595 (rec:0.036, round:207.559)	b=6.50	count=16000
Total loss:	99.391 (rec:0.037, round:99.354)	b=5.94	count=16500
Total loss:	39.177 (rec:0.036, round:39.141)	b=5.38	count=17000
Total loss:	8.811 (rec:0.040, round:8.771)	b=4.81	count=17500
Total loss:	0.558 (rec:0.035, round:0.523)	b=4.25	count=18000
Total loss:	0.036 (rec:0.036, round:0.000)	b=3.69	count=18500
Total loss:	0.039 (rec:0.039, round:0.000)	b=3.12	count=19000
Total loss:	0.037 (rec:0.037, round:0.000)	b=2.56	count=19500
Total loss:	0.039 (rec:0.039, round:0.000)	b=2.00	count=20000
Reconstruction for block context_module
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.037 (rec:0.037, round:0.000)	b=0.00	count=500
Total loss:	0.039 (rec:0.039, round:0.000)	b=0.00	count=1000
Total loss:	0.036 (rec:0.036, round:0.000)	b=0.00	count=1500
Total loss:	0.040 (rec:0.040, round:0.000)	b=0.00	count=2000
Total loss:	0.037 (rec:0.037, round:0.000)	b=0.00	count=2500
Total loss:	0.039 (rec:0.039, round:0.000)	b=0.00	count=3000
Total loss:	0.034 (rec:0.034, round:0.000)	b=0.00	count=3500
Total loss:	45467.516 (rec:0.043, round:45467.473)	b=20.00	count=4000
Total loss:	16970.893 (rec:0.037, round:16970.855)	b=19.44	count=4500
Total loss:	15249.396 (rec:0.035, round:15249.361)	b=18.88	count=5000
Total loss:	14131.578 (rec:0.040, round:14131.538)	b=18.31	count=5500
Total loss:	13175.797 (rec:0.041, round:13175.756)	b=17.75	count=6000
Total loss:	12261.470 (rec:0.038, round:12261.432)	b=17.19	count=6500
Total loss:	11406.671 (rec:0.039, round:11406.632)	b=16.62	count=7000
Total loss:	10580.166 (rec:0.037, round:10580.129)	b=16.06	count=7500
Total loss:	9756.800 (rec:0.038, round:9756.762)	b=15.50	count=8000
Total loss:	8928.540 (rec:0.038, round:8928.502)	b=14.94	count=8500
Total loss:	8160.787 (rec:0.039, round:8160.748)	b=14.38	count=9000
Total loss:	7406.122 (rec:0.036, round:7406.085)	b=13.81	count=9500
Total loss:	6653.176 (rec:0.039, round:6653.138)	b=13.25	count=10000
Total loss:	5900.875 (rec:0.040, round:5900.834)	b=12.69	count=10500
Total loss:	5165.668 (rec:0.039, round:5165.629)	b=12.12	count=11000
Total loss:	4463.420 (rec:0.036, round:4463.384)	b=11.56	count=11500
Total loss:	3752.538 (rec:0.037, round:3752.501)	b=11.00	count=12000
Total loss:	3093.036 (rec:0.038, round:3092.998)	b=10.44	count=12500
Total loss:	2458.083 (rec:0.037, round:2458.046)	b=9.88	count=13000
Total loss:	1823.909 (rec:0.035, round:1823.874)	b=9.31	count=13500
Total loss:	1283.865 (rec:0.038, round:1283.827)	b=8.75	count=14000
Total loss:	852.687 (rec:0.037, round:852.650)	b=8.19	count=14500
Total loss:	506.338 (rec:0.039, round:506.300)	b=7.62	count=15000
Total loss:	281.294 (rec:0.039, round:281.255)	b=7.06	count=15500
Total loss:	139.933 (rec:0.038, round:139.896)	b=6.50	count=16000
Total loss:	48.232 (rec:0.037, round:48.195)	b=5.94	count=16500
Total loss:	17.795 (rec:0.038, round:17.757)	b=5.38	count=17000
Total loss:	5.308 (rec:0.040, round:5.268)	b=4.81	count=17500
Total loss:	1.031 (rec:0.039, round:0.992)	b=4.25	count=18000
Total loss:	0.252 (rec:0.039, round:0.213)	b=3.69	count=18500
Total loss:	0.038 (rec:0.038, round:0.000)	b=3.12	count=19000
Total loss:	0.039 (rec:0.039, round:0.000)	b=2.56	count=19500
Total loss:	0.034 (rec:0.034, round:0.000)	b=2.00	count=20000
Reconstruction for block local_module
Init alpha to be FP32
Total loss:	0.048 (rec:0.048, round:0.000)	b=0.00	count=500
Total loss:	0.044 (rec:0.044, round:0.000)	b=0.00	count=1000
Total loss:	0.042 (rec:0.042, round:0.000)	b=0.00	count=1500
Total loss:	0.049 (rec:0.049, round:0.000)	b=0.00	count=2000
Total loss:	0.048 (rec:0.048, round:0.000)	b=0.00	count=2500
Total loss:	0.042 (rec:0.042, round:0.000)	b=0.00	count=3000
Total loss:	0.044 (rec:0.044, round:0.000)	b=0.00	count=3500
Total loss:	29759.439 (rec:0.056, round:29759.383)	b=20.00	count=4000
Total loss:	11533.915 (rec:0.054, round:11533.860)	b=19.44	count=4500
Total loss:	10409.628 (rec:0.047, round:10409.580)	b=18.88	count=5000
Total loss:	9663.958 (rec:0.045, round:9663.913)	b=18.31	count=5500
Total loss:	9041.777 (rec:0.041, round:9041.736)	b=17.75	count=6000
Total loss:	8456.376 (rec:0.046, round:8456.330)	b=17.19	count=6500
Total loss:	7907.707 (rec:0.047, round:7907.659)	b=16.62	count=7000
Total loss:	7373.611 (rec:0.045, round:7373.566)	b=16.06	count=7500
Total loss:	6857.019 (rec:0.049, round:6856.970)	b=15.50	count=8000
Total loss:	6361.174 (rec:0.047, round:6361.127)	b=14.94	count=8500
Total loss:	5865.725 (rec:0.047, round:5865.678)	b=14.38	count=9000
Total loss:	5359.335 (rec:0.045, round:5359.290)	b=13.81	count=9500
Total loss:	4886.668 (rec:0.048, round:4886.620)	b=13.25	count=10000
Total loss:	4405.987 (rec:0.045, round:4405.941)	b=12.69	count=10500
Total loss:	3925.133 (rec:0.053, round:3925.080)	b=12.12	count=11000
Total loss:	3456.735 (rec:0.045, round:3456.690)	b=11.56	count=11500
Total loss:	3010.937 (rec:0.047, round:3010.890)	b=11.00	count=12000
Total loss:	2577.230 (rec:0.049, round:2577.181)	b=10.44	count=12500
Total loss:	2142.538 (rec:0.049, round:2142.489)	b=9.88	count=13000
Total loss:	1731.937 (rec:0.044, round:1731.893)	b=9.31	count=13500
Total loss:	1358.314 (rec:0.045, round:1358.269)	b=8.75	count=14000
Total loss:	1009.432 (rec:0.047, round:1009.385)	b=8.19	count=14500
Total loss:	710.541 (rec:0.045, round:710.496)	b=7.62	count=15000
Total loss:	459.786 (rec:0.047, round:459.739)	b=7.06	count=15500
Total loss:	271.028 (rec:0.049, round:270.979)	b=6.50	count=16000
Total loss:	130.543 (rec:0.046, round:130.496)	b=5.94	count=16500
Total loss:	58.198 (rec:0.046, round:58.151)	b=5.38	count=17000
Total loss:	19.944 (rec:0.045, round:19.899)	b=4.81	count=17500
Total loss:	2.867 (rec:0.050, round:2.817)	b=4.25	count=18000
Total loss:	0.046 (rec:0.046, round:0.000)	b=3.69	count=18500
Total loss:	0.047 (rec:0.047, round:0.000)	b=3.12	count=19000
Total loss:	0.046 (rec:0.046, round:0.000)	b=2.56	count=19500
Total loss:	0.045 (rec:0.045, round:0.000)	b=2.00	count=20000
Reconstruction for block 0
Init alpha to be FP32
Total loss:	0.025 (rec:0.025, round:0.000)	b=0.00	count=500
Total loss:	0.025 (rec:0.025, round:0.000)	b=0.00	count=1000
Total loss:	0.025 (rec:0.025, round:0.000)	b=0.00	count=1500
Total loss:	0.027 (rec:0.027, round:0.000)	b=0.00	count=2000
Total loss:	0.025 (rec:0.025, round:0.000)	b=0.00	count=2500
Total loss:	0.026 (rec:0.026, round:0.000)	b=0.00	count=3000
Total loss:	0.025 (rec:0.025, round:0.000)	b=0.00	count=3500
Total loss:	30176.965 (rec:0.027, round:30176.938)	b=20.00	count=4000
Total loss:	12100.058 (rec:0.026, round:12100.031)	b=19.44	count=4500
Total loss:	10870.935 (rec:0.026, round:10870.908)	b=18.88	count=5000
Total loss:	10060.525 (rec:0.027, round:10060.498)	b=18.31	count=5500
Total loss:	9390.642 (rec:0.029, round:9390.613)	b=17.75	count=6000
Total loss:	8793.114 (rec:0.026, round:8793.088)	b=17.19	count=6500
Total loss:	8234.140 (rec:0.028, round:8234.111)	b=16.62	count=7000
Total loss:	7712.268 (rec:0.027, round:7712.241)	b=16.06	count=7500
Total loss:	7201.674 (rec:0.028, round:7201.646)	b=15.50	count=8000
Total loss:	6724.055 (rec:0.027, round:6724.028)	b=14.94	count=8500
Total loss:	6236.211 (rec:0.029, round:6236.182)	b=14.38	count=9000
Total loss:	5783.442 (rec:0.030, round:5783.412)	b=13.81	count=9500
Total loss:	5283.275 (rec:0.028, round:5283.247)	b=13.25	count=10000
Total loss:	4821.435 (rec:0.028, round:4821.406)	b=12.69	count=10500
Total loss:	4310.900 (rec:0.029, round:4310.871)	b=12.12	count=11000
Total loss:	3806.440 (rec:0.028, round:3806.412)	b=11.56	count=11500
Total loss:	3340.528 (rec:0.029, round:3340.498)	b=11.00	count=12000
Total loss:	2852.693 (rec:0.029, round:2852.665)	b=10.44	count=12500
Total loss:	2397.586 (rec:0.031, round:2397.555)	b=9.88	count=13000
Total loss:	1954.891 (rec:0.031, round:1954.859)	b=9.31	count=13500
Total loss:	1548.678 (rec:0.030, round:1548.648)	b=8.75	count=14000
Total loss:	1159.094 (rec:0.031, round:1159.063)	b=8.19	count=14500
Total loss:	813.921 (rec:0.030, round:813.891)	b=7.62	count=15000
Total loss:	533.051 (rec:0.031, round:533.021)	b=7.06	count=15500
Total loss:	314.539 (rec:0.031, round:314.508)	b=6.50	count=16000
Total loss:	151.425 (rec:0.030, round:151.395)	b=5.94	count=16500
Total loss:	66.789 (rec:0.032, round:66.757)	b=5.38	count=17000
Total loss:	23.309 (rec:0.032, round:23.277)	b=4.81	count=17500
Total loss:	5.004 (rec:0.030, round:4.974)	b=4.25	count=18000
Total loss:	0.363 (rec:0.031, round:0.332)	b=3.69	count=18500
Total loss:	0.032 (rec:0.032, round:0.000)	b=3.12	count=19000
Total loss:	0.030 (rec:0.030, round:0.000)	b=2.56	count=19500
Total loss:	0.031 (rec:0.031, round:0.000)	b=2.00	count=20000
Reconstruction for block context_module
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.032 (rec:0.032, round:0.000)	b=0.00	count=500
Total loss:	0.032 (rec:0.032, round:0.000)	b=0.00	count=1000
Total loss:	0.031 (rec:0.031, round:0.000)	b=0.00	count=1500
Total loss:	0.033 (rec:0.033, round:0.000)	b=0.00	count=2000
Total loss:	0.033 (rec:0.033, round:0.000)	b=0.00	count=2500
Total loss:	0.034 (rec:0.034, round:0.000)	b=0.00	count=3000
Total loss:	0.032 (rec:0.032, round:0.000)	b=0.00	count=3500
Total loss:	167378.969 (rec:0.033, round:167378.938)	b=20.00	count=4000
Total loss:	58877.984 (rec:0.032, round:58877.953)	b=19.44	count=4500
Total loss:	53292.273 (rec:0.033, round:53292.242)	b=18.88	count=5000
Total loss:	49604.633 (rec:0.031, round:49604.602)	b=18.31	count=5500
Total loss:	46326.961 (rec:0.032, round:46326.930)	b=17.75	count=6000
Total loss:	43301.707 (rec:0.033, round:43301.676)	b=17.19	count=6500
Total loss:	40315.312 (rec:0.031, round:40315.281)	b=16.62	count=7000
Total loss:	37428.832 (rec:0.031, round:37428.801)	b=16.06	count=7500
Total loss:	34609.555 (rec:0.032, round:34609.523)	b=15.50	count=8000
Total loss:	31806.924 (rec:0.034, round:31806.891)	b=14.94	count=8500
Total loss:	29108.820 (rec:0.033, round:29108.787)	b=14.38	count=9000
Total loss:	26358.576 (rec:0.033, round:26358.543)	b=13.81	count=9500
Total loss:	23684.990 (rec:0.034, round:23684.957)	b=13.25	count=10000
Total loss:	21079.264 (rec:0.033, round:21079.230)	b=12.69	count=10500
Total loss:	18501.207 (rec:0.033, round:18501.174)	b=12.12	count=11000
Total loss:	16071.953 (rec:0.032, round:16071.922)	b=11.56	count=11500
Total loss:	13714.045 (rec:0.033, round:13714.012)	b=11.00	count=12000
Total loss:	11433.469 (rec:0.032, round:11433.438)	b=10.44	count=12500
Total loss:	9328.835 (rec:0.035, round:9328.800)	b=9.88	count=13000
Total loss:	7206.600 (rec:0.033, round:7206.567)	b=9.31	count=13500
Total loss:	5289.400 (rec:0.033, round:5289.367)	b=8.75	count=14000
Total loss:	3530.579 (rec:0.033, round:3530.545)	b=8.19	count=14500
Total loss:	1980.301 (rec:0.032, round:1980.269)	b=7.62	count=15000
Total loss:	951.548 (rec:0.031, round:951.516)	b=7.06	count=15500
Total loss:	393.647 (rec:0.031, round:393.616)	b=6.50	count=16000
Total loss:	156.423 (rec:0.033, round:156.391)	b=5.94	count=16500
Total loss:	59.671 (rec:0.033, round:59.639)	b=5.38	count=17000
Total loss:	17.163 (rec:0.034, round:17.130)	b=4.81	count=17500
Total loss:	2.989 (rec:0.033, round:2.956)	b=4.25	count=18000
Total loss:	0.529 (rec:0.033, round:0.496)	b=3.69	count=18500
Total loss:	0.031 (rec:0.031, round:0.000)	b=3.12	count=19000
Total loss:	0.035 (rec:0.035, round:0.000)	b=2.56	count=19500
Total loss:	0.032 (rec:0.032, round:0.000)	b=2.00	count=20000
Reconstruction for block local_module
Init alpha to be FP32
Total loss:	0.040 (rec:0.040, round:0.000)	b=0.00	count=500
Total loss:	0.041 (rec:0.041, round:0.000)	b=0.00	count=1000
Total loss:	0.038 (rec:0.038, round:0.000)	b=0.00	count=1500
Total loss:	0.039 (rec:0.039, round:0.000)	b=0.00	count=2000
Total loss:	0.042 (rec:0.042, round:0.000)	b=0.00	count=2500
Total loss:	0.041 (rec:0.041, round:0.000)	b=0.00	count=3000
Total loss:	0.038 (rec:0.038, round:0.000)	b=0.00	count=3500
Total loss:	120157.047 (rec:0.038, round:120157.008)	b=20.00	count=4000
Total loss:	44233.953 (rec:0.039, round:44233.914)	b=19.44	count=4500
Total loss:	40129.934 (rec:0.043, round:40129.891)	b=18.88	count=5000
Total loss:	37276.512 (rec:0.039, round:37276.473)	b=18.31	count=5500
Total loss:	34750.863 (rec:0.039, round:34750.824)	b=17.75	count=6000
Total loss:	32375.412 (rec:0.038, round:32375.375)	b=17.19	count=6500
Total loss:	30072.027 (rec:0.038, round:30071.990)	b=16.62	count=7000
Total loss:	27904.070 (rec:0.040, round:27904.029)	b=16.06	count=7500
Total loss:	25790.467 (rec:0.041, round:25790.426)	b=15.50	count=8000
Total loss:	23688.219 (rec:0.040, round:23688.180)	b=14.94	count=8500
Total loss:	21625.994 (rec:0.041, round:21625.953)	b=14.38	count=9000
Total loss:	19650.102 (rec:0.039, round:19650.062)	b=13.81	count=9500
Total loss:	17659.732 (rec:0.041, round:17659.691)	b=13.25	count=10000
Total loss:	15859.135 (rec:0.041, round:15859.094)	b=12.69	count=10500
Total loss:	14046.728 (rec:0.038, round:14046.689)	b=12.12	count=11000
Total loss:	12398.622 (rec:0.039, round:12398.583)	b=11.56	count=11500
Total loss:	10761.988 (rec:0.041, round:10761.947)	b=11.00	count=12000
Total loss:	9186.535 (rec:0.038, round:9186.497)	b=10.44	count=12500
Total loss:	7698.086 (rec:0.040, round:7698.046)	b=9.88	count=13000
Total loss:	6296.520 (rec:0.043, round:6296.477)	b=9.31	count=13500
Total loss:	4988.497 (rec:0.041, round:4988.455)	b=8.75	count=14000
Total loss:	3776.628 (rec:0.041, round:3776.587)	b=8.19	count=14500
Total loss:	2721.433 (rec:0.041, round:2721.393)	b=7.62	count=15000
Total loss:	1807.502 (rec:0.040, round:1807.462)	b=7.06	count=15500
Total loss:	1069.937 (rec:0.038, round:1069.899)	b=6.50	count=16000
Total loss:	554.434 (rec:0.042, round:554.392)	b=5.94	count=16500
Total loss:	221.787 (rec:0.042, round:221.745)	b=5.38	count=17000
Total loss:	59.484 (rec:0.041, round:59.443)	b=4.81	count=17500
Total loss:	7.904 (rec:0.039, round:7.865)	b=4.25	count=18000
Total loss:	0.140 (rec:0.040, round:0.099)	b=3.69	count=18500
Total loss:	0.040 (rec:0.040, round:0.000)	b=3.12	count=19000
Total loss:	0.041 (rec:0.041, round:0.000)	b=2.56	count=19500
Total loss:	0.045 (rec:0.045, round:0.000)	b=2.00	count=20000
Reconstruction for block context_module
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.042 (rec:0.042, round:0.000)	b=0.00	count=500
Total loss:	0.042 (rec:0.042, round:0.000)	b=0.00	count=1000
Total loss:	0.040 (rec:0.040, round:0.000)	b=0.00	count=1500
Total loss:	0.042 (rec:0.042, round:0.000)	b=0.00	count=2000
Total loss:	0.042 (rec:0.042, round:0.000)	b=0.00	count=2500
Total loss:	0.042 (rec:0.042, round:0.000)	b=0.00	count=3000
Total loss:	0.040 (rec:0.040, round:0.000)	b=0.00	count=3500
Total loss:	167189.875 (rec:0.042, round:167189.828)	b=20.00	count=4000
Total loss:	60188.805 (rec:0.044, round:60188.762)	b=19.44	count=4500
Total loss:	54620.855 (rec:0.045, round:54620.812)	b=18.88	count=5000
Total loss:	50718.055 (rec:0.042, round:50718.012)	b=18.31	count=5500
Total loss:	47236.109 (rec:0.046, round:47236.062)	b=17.75	count=6000
Total loss:	43928.145 (rec:0.044, round:43928.102)	b=17.19	count=6500
Total loss:	40848.453 (rec:0.043, round:40848.410)	b=16.62	count=7000
Total loss:	37836.855 (rec:0.041, round:37836.812)	b=16.06	count=7500
Total loss:	34884.895 (rec:0.043, round:34884.852)	b=15.50	count=8000
Total loss:	32012.439 (rec:0.042, round:32012.398)	b=14.94	count=8500
Total loss:	29222.854 (rec:0.041, round:29222.812)	b=14.38	count=9000
Total loss:	26480.947 (rec:0.041, round:26480.906)	b=13.81	count=9500
Total loss:	23784.564 (rec:0.044, round:23784.521)	b=13.25	count=10000
Total loss:	21124.535 (rec:0.043, round:21124.492)	b=12.69	count=10500
Total loss:	18581.639 (rec:0.039, round:18581.600)	b=12.12	count=11000
Total loss:	16147.312 (rec:0.040, round:16147.272)	b=11.56	count=11500
Total loss:	13796.156 (rec:0.042, round:13796.114)	b=11.00	count=12000
Total loss:	11666.176 (rec:0.043, round:11666.133)	b=10.44	count=12500
Total loss:	9581.347 (rec:0.041, round:9581.306)	b=9.88	count=13000
Total loss:	7625.055 (rec:0.043, round:7625.012)	b=9.31	count=13500
Total loss:	5816.374 (rec:0.043, round:5816.331)	b=8.75	count=14000
Total loss:	4238.784 (rec:0.041, round:4238.743)	b=8.19	count=14500
Total loss:	2788.294 (rec:0.043, round:2788.251)	b=7.62	count=15000
Total loss:	1667.322 (rec:0.041, round:1667.281)	b=7.06	count=15500
Total loss:	865.632 (rec:0.041, round:865.591)	b=6.50	count=16000
Total loss:	378.136 (rec:0.040, round:378.096)	b=5.94	count=16500
Total loss:	134.657 (rec:0.044, round:134.613)	b=5.38	count=17000
Total loss:	35.045 (rec:0.042, round:35.004)	b=4.81	count=17500
Total loss:	5.960 (rec:0.042, round:5.917)	b=4.25	count=18000
Total loss:	0.991 (rec:0.041, round:0.950)	b=3.69	count=18500
Total loss:	0.043 (rec:0.040, round:0.003)	b=3.12	count=19000
Total loss:	0.045 (rec:0.045, round:0.000)	b=2.56	count=19500
Total loss:	0.044 (rec:0.044, round:0.000)	b=2.00	count=20000
Reconstruction for block local_module
Init alpha to be FP32
Total loss:	0.050 (rec:0.050, round:0.000)	b=0.00	count=500
Total loss:	0.049 (rec:0.049, round:0.000)	b=0.00	count=1000
Total loss:	0.049 (rec:0.049, round:0.000)	b=0.00	count=1500
Total loss:	0.055 (rec:0.055, round:0.000)	b=0.00	count=2000
Total loss:	0.048 (rec:0.048, round:0.000)	b=0.00	count=2500
Total loss:	0.052 (rec:0.052, round:0.000)	b=0.00	count=3000
Total loss:	0.051 (rec:0.051, round:0.000)	b=0.00	count=3500
Total loss:	120620.664 (rec:0.050, round:120620.617)	b=20.00	count=4000
Total loss:	45263.926 (rec:0.050, round:45263.875)	b=19.44	count=4500
Total loss:	41210.770 (rec:0.050, round:41210.719)	b=18.88	count=5000
Total loss:	38432.137 (rec:0.051, round:38432.086)	b=18.31	count=5500
Total loss:	35957.812 (rec:0.048, round:35957.766)	b=17.75	count=6000
Total loss:	33709.895 (rec:0.051, round:33709.844)	b=17.19	count=6500
Total loss:	31548.553 (rec:0.047, round:31548.506)	b=16.62	count=7000
Total loss:	29452.961 (rec:0.049, round:29452.912)	b=16.06	count=7500
Total loss:	27358.777 (rec:0.050, round:27358.727)	b=15.50	count=8000
Total loss:	25322.609 (rec:0.050, round:25322.559)	b=14.94	count=8500
Total loss:	23332.164 (rec:0.050, round:23332.113)	b=14.38	count=9000
Total loss:	21373.803 (rec:0.055, round:21373.748)	b=13.81	count=9500
Total loss:	19449.715 (rec:0.050, round:19449.664)	b=13.25	count=10000
Total loss:	17532.889 (rec:0.046, round:17532.842)	b=12.69	count=10500
Total loss:	15723.310 (rec:0.047, round:15723.263)	b=12.12	count=11000
Total loss:	13927.208 (rec:0.050, round:13927.158)	b=11.56	count=11500
Total loss:	12225.684 (rec:0.051, round:12225.633)	b=11.00	count=12000
Total loss:	10586.911 (rec:0.054, round:10586.856)	b=10.44	count=12500
Total loss:	8996.533 (rec:0.049, round:8996.483)	b=9.88	count=13000
Total loss:	7448.849 (rec:0.053, round:7448.796)	b=9.31	count=13500
Total loss:	5973.705 (rec:0.052, round:5973.653)	b=8.75	count=14000
Total loss:	4596.857 (rec:0.049, round:4596.808)	b=8.19	count=14500
Total loss:	3380.752 (rec:0.049, round:3380.703)	b=7.62	count=15000
Total loss:	2313.270 (rec:0.049, round:2313.221)	b=7.06	count=15500
Total loss:	1444.253 (rec:0.052, round:1444.201)	b=6.50	count=16000
Total loss:	766.119 (rec:0.047, round:766.072)	b=5.94	count=16500
Total loss:	311.625 (rec:0.051, round:311.574)	b=5.38	count=17000
Total loss:	88.974 (rec:0.053, round:88.920)	b=4.81	count=17500
Total loss:	6.368 (rec:0.055, round:6.313)	b=4.25	count=18000
Total loss:	0.054 (rec:0.054, round:0.000)	b=3.69	count=18500
Total loss:	0.051 (rec:0.051, round:0.000)	b=3.12	count=19000
Total loss:	0.050 (rec:0.050, round:0.000)	b=2.56	count=19500
Total loss:	0.048 (rec:0.048, round:0.000)	b=2.00	count=20000
Reconstruction for block context_module
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.053 (rec:0.053, round:0.000)	b=0.00	count=500
Total loss:	0.052 (rec:0.052, round:0.000)	b=0.00	count=1000
Total loss:	0.056 (rec:0.056, round:0.000)	b=0.00	count=1500
Total loss:	0.050 (rec:0.050, round:0.000)	b=0.00	count=2000
Total loss:	0.050 (rec:0.050, round:0.000)	b=0.00	count=2500
Total loss:	0.049 (rec:0.049, round:0.000)	b=0.00	count=3000
Total loss:	0.050 (rec:0.050, round:0.000)	b=0.00	count=3500
Total loss:	166659.750 (rec:0.050, round:166659.703)	b=20.00	count=4000
Total loss:	60415.758 (rec:0.054, round:60415.703)	b=19.44	count=4500
Total loss:	54750.699 (rec:0.050, round:54750.648)	b=18.88	count=5000
Total loss:	50869.043 (rec:0.051, round:50868.992)	b=18.31	count=5500
Total loss:	47495.453 (rec:0.054, round:47495.398)	b=17.75	count=6000
Total loss:	44209.457 (rec:0.051, round:44209.406)	b=17.19	count=6500
Total loss:	41082.176 (rec:0.051, round:41082.125)	b=16.62	count=7000
Total loss:	38108.082 (rec:0.052, round:38108.031)	b=16.06	count=7500
Total loss:	35117.355 (rec:0.053, round:35117.301)	b=15.50	count=8000
Total loss:	32212.266 (rec:0.051, round:32212.215)	b=14.94	count=8500
Total loss:	29305.906 (rec:0.052, round:29305.854)	b=14.38	count=9000
Total loss:	26522.570 (rec:0.050, round:26522.521)	b=13.81	count=9500
Total loss:	23795.291 (rec:0.049, round:23795.242)	b=13.25	count=10000
Total loss:	21150.914 (rec:0.052, round:21150.861)	b=12.69	count=10500
Total loss:	18637.959 (rec:0.054, round:18637.906)	b=12.12	count=11000
Total loss:	16251.046 (rec:0.054, round:16250.992)	b=11.56	count=11500
Total loss:	13957.779 (rec:0.051, round:13957.728)	b=11.00	count=12000
Total loss:	11808.713 (rec:0.055, round:11808.658)	b=10.44	count=12500
Total loss:	9747.763 (rec:0.051, round:9747.711)	b=9.88	count=13000
Total loss:	7874.900 (rec:0.051, round:7874.849)	b=9.31	count=13500
Total loss:	6126.011 (rec:0.052, round:6125.959)	b=8.75	count=14000
Total loss:	4528.075 (rec:0.052, round:4528.023)	b=8.19	count=14500
Total loss:	3109.162 (rec:0.052, round:3109.110)	b=7.62	count=15000
Total loss:	1911.517 (rec:0.053, round:1911.464)	b=7.06	count=15500
Total loss:	1043.248 (rec:0.053, round:1043.195)	b=6.50	count=16000
Total loss:	485.907 (rec:0.052, round:485.855)	b=5.94	count=16500
Total loss:	180.182 (rec:0.051, round:180.131)	b=5.38	count=17000
Total loss:	44.061 (rec:0.051, round:44.010)	b=4.81	count=17500
Total loss:	4.578 (rec:0.054, round:4.524)	b=4.25	count=18000
Total loss:	0.670 (rec:0.050, round:0.619)	b=3.69	count=18500
Total loss:	0.120 (rec:0.052, round:0.069)	b=3.12	count=19000
Total loss:	0.052 (rec:0.052, round:0.000)	b=2.56	count=19500
Total loss:	0.051 (rec:0.051, round:0.000)	b=2.00	count=20000
Reconstruction for block local_module
Init alpha to be FP32
Total loss:	0.058 (rec:0.058, round:0.000)	b=0.00	count=500
Total loss:	0.060 (rec:0.060, round:0.000)	b=0.00	count=1000
Total loss:	0.054 (rec:0.054, round:0.000)	b=0.00	count=1500
Total loss:	0.059 (rec:0.059, round:0.000)	b=0.00	count=2000
Total loss:	0.059 (rec:0.059, round:0.000)	b=0.00	count=2500
Total loss:	0.062 (rec:0.062, round:0.000)	b=0.00	count=3000
Total loss:	0.058 (rec:0.058, round:0.000)	b=0.00	count=3500
Total loss:	120860.258 (rec:0.055, round:120860.203)	b=20.00	count=4000
Total loss:	45426.105 (rec:0.057, round:45426.047)	b=19.44	count=4500
Total loss:	41280.387 (rec:0.058, round:41280.328)	b=18.88	count=5000
Total loss:	38473.188 (rec:0.057, round:38473.129)	b=18.31	count=5500
Total loss:	35992.641 (rec:0.055, round:35992.586)	b=17.75	count=6000
Total loss:	33670.949 (rec:0.058, round:33670.891)	b=17.19	count=6500
Total loss:	31478.779 (rec:0.060, round:31478.719)	b=16.62	count=7000
Total loss:	29300.115 (rec:0.061, round:29300.055)	b=16.06	count=7500
Total loss:	27163.256 (rec:0.055, round:27163.201)	b=15.50	count=8000
Total loss:	25052.760 (rec:0.058, round:25052.701)	b=14.94	count=8500
Total loss:	22980.260 (rec:0.057, round:22980.203)	b=14.38	count=9000
Total loss:	20903.270 (rec:0.056, round:20903.213)	b=13.81	count=9500
Total loss:	18939.115 (rec:0.061, round:18939.055)	b=13.25	count=10000
Total loss:	17080.111 (rec:0.060, round:17080.051)	b=12.69	count=10500
Total loss:	15226.118 (rec:0.061, round:15226.058)	b=12.12	count=11000
Total loss:	13442.578 (rec:0.061, round:13442.518)	b=11.56	count=11500
Total loss:	11709.821 (rec:0.061, round:11709.761)	b=11.00	count=12000
Total loss:	10115.158 (rec:0.056, round:10115.102)	b=10.44	count=12500
Total loss:	8520.524 (rec:0.056, round:8520.469)	b=9.88	count=13000
Total loss:	7030.761 (rec:0.058, round:7030.703)	b=9.31	count=13500
Total loss:	5608.875 (rec:0.057, round:5608.818)	b=8.75	count=14000
Total loss:	4344.078 (rec:0.058, round:4344.020)	b=8.19	count=14500
Total loss:	3188.325 (rec:0.059, round:3188.266)	b=7.62	count=15000
Total loss:	2181.766 (rec:0.059, round:2181.706)	b=7.06	count=15500
Total loss:	1333.712 (rec:0.059, round:1333.653)	b=6.50	count=16000
Total loss:	695.895 (rec:0.061, round:695.834)	b=5.94	count=16500
Total loss:	289.716 (rec:0.058, round:289.658)	b=5.38	count=17000
Total loss:	79.289 (rec:0.057, round:79.231)	b=4.81	count=17500
Total loss:	8.881 (rec:0.059, round:8.822)	b=4.25	count=18000
Total loss:	0.433 (rec:0.056, round:0.376)	b=3.69	count=18500
Total loss:	0.060 (rec:0.060, round:0.000)	b=3.12	count=19000
Total loss:	0.059 (rec:0.059, round:0.000)	b=2.56	count=19500
Total loss:	0.060 (rec:0.060, round:0.000)	b=2.00	count=20000
Reconstruction for block context_module
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.063 (rec:0.063, round:0.000)	b=0.00	count=500
Total loss:	0.066 (rec:0.066, round:0.000)	b=0.00	count=1000
Total loss:	0.058 (rec:0.058, round:0.000)	b=0.00	count=1500
Total loss:	0.058 (rec:0.058, round:0.000)	b=0.00	count=2000
Total loss:	0.059 (rec:0.059, round:0.000)	b=0.00	count=2500
Total loss:	0.058 (rec:0.058, round:0.000)	b=0.00	count=3000
Total loss:	0.061 (rec:0.061, round:0.000)	b=0.00	count=3500
Total loss:	166931.000 (rec:0.060, round:166930.938)	b=20.00	count=4000
Total loss:	60499.805 (rec:0.060, round:60499.746)	b=19.44	count=4500
Total loss:	54845.719 (rec:0.059, round:54845.660)	b=18.88	count=5000
Total loss:	51046.504 (rec:0.059, round:51046.445)	b=18.31	count=5500
Total loss:	47630.020 (rec:0.060, round:47629.961)	b=17.75	count=6000
Total loss:	44394.039 (rec:0.060, round:44393.980)	b=17.19	count=6500
Total loss:	41266.652 (rec:0.061, round:41266.590)	b=16.62	count=7000
Total loss:	38236.492 (rec:0.060, round:38236.434)	b=16.06	count=7500
Total loss:	35198.926 (rec:0.059, round:35198.867)	b=15.50	count=8000
Total loss:	32219.453 (rec:0.060, round:32219.393)	b=14.94	count=8500
Total loss:	29360.600 (rec:0.061, round:29360.539)	b=14.38	count=9000
Total loss:	26496.869 (rec:0.058, round:26496.811)	b=13.81	count=9500
Total loss:	23784.523 (rec:0.062, round:23784.461)	b=13.25	count=10000
Total loss:	21158.928 (rec:0.060, round:21158.867)	b=12.69	count=10500
Total loss:	18679.934 (rec:0.059, round:18679.875)	b=12.12	count=11000
Total loss:	16273.062 (rec:0.064, round:16272.998)	b=11.56	count=11500
Total loss:	14028.075 (rec:0.061, round:14028.014)	b=11.00	count=12000
Total loss:	11797.503 (rec:0.059, round:11797.443)	b=10.44	count=12500
Total loss:	9785.791 (rec:0.062, round:9785.729)	b=9.88	count=13000
Total loss:	7844.138 (rec:0.059, round:7844.078)	b=9.31	count=13500
Total loss:	6063.071 (rec:0.060, round:6063.012)	b=8.75	count=14000
Total loss:	4375.965 (rec:0.060, round:4375.905)	b=8.19	count=14500
Total loss:	2896.881 (rec:0.057, round:2896.823)	b=7.62	count=15000
Total loss:	1661.091 (rec:0.058, round:1661.033)	b=7.06	count=15500
Total loss:	855.630 (rec:0.060, round:855.570)	b=6.50	count=16000
Total loss:	366.871 (rec:0.060, round:366.811)	b=5.94	count=16500
Total loss:	141.923 (rec:0.060, round:141.863)	b=5.38	count=17000
Total loss:	40.709 (rec:0.059, round:40.650)	b=4.81	count=17500
Total loss:	5.555 (rec:0.056, round:5.499)	b=4.25	count=18000
Total loss:	0.364 (rec:0.102, round:0.262)	b=3.69	count=18500
Total loss:	0.059 (rec:0.059, round:0.000)	b=3.12	count=19000
Total loss:	0.062 (rec:0.062, round:0.000)	b=2.56	count=19500
Total loss:	0.060 (rec:0.060, round:0.000)	b=2.00	count=20000
Reconstruction for block local_module
Init alpha to be FP32
Total loss:	0.071 (rec:0.071, round:0.000)	b=0.00	count=500
Total loss:	0.064 (rec:0.064, round:0.000)	b=0.00	count=1000
Total loss:	0.064 (rec:0.064, round:0.000)	b=0.00	count=1500
Total loss:	0.067 (rec:0.067, round:0.000)	b=0.00	count=2000
Total loss:	0.064 (rec:0.064, round:0.000)	b=0.00	count=2500
Total loss:	0.068 (rec:0.068, round:0.000)	b=0.00	count=3000
Total loss:	0.065 (rec:0.065, round:0.000)	b=0.00	count=3500
Total loss:	120917.539 (rec:0.067, round:120917.469)	b=20.00	count=4000
Total loss:	45955.797 (rec:0.067, round:45955.730)	b=19.44	count=4500
Total loss:	41846.418 (rec:0.067, round:41846.352)	b=18.88	count=5000
Total loss:	39042.043 (rec:0.065, round:39041.977)	b=18.31	count=5500
Total loss:	36539.922 (rec:0.068, round:36539.852)	b=17.75	count=6000
Total loss:	34209.137 (rec:0.065, round:34209.070)	b=17.19	count=6500
Total loss:	31942.539 (rec:0.066, round:31942.473)	b=16.62	count=7000
Total loss:	29733.697 (rec:0.065, round:29733.633)	b=16.06	count=7500
Total loss:	27554.805 (rec:0.062, round:27554.742)	b=15.50	count=8000
Total loss:	25421.672 (rec:0.064, round:25421.607)	b=14.94	count=8500
Total loss:	23324.318 (rec:0.064, round:23324.254)	b=14.38	count=9000
Total loss:	21272.965 (rec:0.071, round:21272.895)	b=13.81	count=9500
Total loss:	19298.594 (rec:0.071, round:19298.523)	b=13.25	count=10000
Total loss:	17325.150 (rec:0.068, round:17325.082)	b=12.69	count=10500
Total loss:	15469.952 (rec:0.069, round:15469.883)	b=12.12	count=11000
Total loss:	13684.519 (rec:0.070, round:13684.448)	b=11.56	count=11500
Total loss:	11936.325 (rec:0.070, round:11936.256)	b=11.00	count=12000
Total loss:	10242.043 (rec:0.070, round:10241.973)	b=10.44	count=12500
Total loss:	8605.746 (rec:0.066, round:8605.680)	b=9.88	count=13000
Total loss:	7063.834 (rec:0.069, round:7063.765)	b=9.31	count=13500
Total loss:	5615.228 (rec:0.069, round:5615.159)	b=8.75	count=14000
Total loss:	4323.828 (rec:0.071, round:4323.757)	b=8.19	count=14500
Total loss:	3116.591 (rec:0.068, round:3116.523)	b=7.62	count=15000
Total loss:	2133.521 (rec:0.067, round:2133.455)	b=7.06	count=15500
Total loss:	1299.554 (rec:0.067, round:1299.487)	b=6.50	count=16000
Total loss:	688.385 (rec:0.067, round:688.318)	b=5.94	count=16500
Total loss:	296.304 (rec:0.067, round:296.237)	b=5.38	count=17000
Total loss:	87.017 (rec:0.068, round:86.949)	b=4.81	count=17500
Total loss:	17.457 (rec:0.061, round:17.396)	b=4.25	count=18000
Total loss:	0.829 (rec:0.065, round:0.765)	b=3.69	count=18500
Total loss:	0.068 (rec:0.068, round:0.000)	b=3.12	count=19000
Total loss:	0.074 (rec:0.074, round:0.000)	b=2.56	count=19500
Total loss:	0.070 (rec:0.070, round:0.000)	b=2.00	count=20000
Reconstruction for layer conv
Init alpha to be FP32
Total loss:	0.027 (rec:0.027, round:0.000)	b=0.00	count=500
Total loss:	0.026 (rec:0.026, round:0.000)	b=0.00	count=1000
Total loss:	0.027 (rec:0.027, round:0.000)	b=0.00	count=1500
Total loss:	0.030 (rec:0.030, round:0.000)	b=0.00	count=2000
Total loss:	0.031 (rec:0.031, round:0.000)	b=0.00	count=2500
Total loss:	0.028 (rec:0.028, round:0.000)	b=0.00	count=3000
Total loss:	0.029 (rec:0.029, round:0.000)	b=0.00	count=3500
Total loss:	181955.406 (rec:0.027, round:181955.375)	b=20.00	count=4000
Total loss:	70263.422 (rec:0.030, round:70263.391)	b=19.44	count=4500
Total loss:	63832.020 (rec:0.035, round:63831.984)	b=18.88	count=5000
Total loss:	59659.012 (rec:0.032, round:59658.980)	b=18.31	count=5500
Total loss:	56090.852 (rec:0.032, round:56090.820)	b=17.75	count=6000
Total loss:	52859.566 (rec:0.034, round:52859.531)	b=17.19	count=6500
Total loss:	49815.051 (rec:0.031, round:49815.020)	b=16.62	count=7000
Total loss:	46826.137 (rec:0.033, round:46826.105)	b=16.06	count=7500
Total loss:	43905.367 (rec:0.032, round:43905.336)	b=15.50	count=8000
Total loss:	41029.488 (rec:0.033, round:41029.457)	b=14.94	count=8500
Total loss:	38242.309 (rec:0.030, round:38242.277)	b=14.38	count=9000
Total loss:	35410.855 (rec:0.034, round:35410.820)	b=13.81	count=9500
Total loss:	32627.193 (rec:0.034, round:32627.160)	b=13.25	count=10000
Total loss:	29742.807 (rec:0.033, round:29742.773)	b=12.69	count=10500
Total loss:	26875.566 (rec:0.032, round:26875.535)	b=12.12	count=11000
Total loss:	24105.156 (rec:0.033, round:24105.123)	b=11.56	count=11500
Total loss:	21340.840 (rec:0.035, round:21340.805)	b=11.00	count=12000
Total loss:	18572.717 (rec:0.033, round:18572.684)	b=10.44	count=12500
Total loss:	15851.993 (rec:0.036, round:15851.957)	b=9.88	count=13000
Total loss:	13222.659 (rec:0.035, round:13222.624)	b=9.31	count=13500
Total loss:	10665.771 (rec:0.034, round:10665.736)	b=8.75	count=14000
Total loss:	8319.369 (rec:0.035, round:8319.334)	b=8.19	count=14500
Total loss:	6135.177 (rec:0.037, round:6135.140)	b=7.62	count=15000
Total loss:	4154.217 (rec:0.040, round:4154.177)	b=7.06	count=15500
Total loss:	2598.207 (rec:0.036, round:2598.171)	b=6.50	count=16000
Total loss:	1386.718 (rec:0.040, round:1386.678)	b=5.94	count=16500
Total loss:	595.337 (rec:0.041, round:595.296)	b=5.38	count=17000
Total loss:	172.009 (rec:0.038, round:171.971)	b=4.81	count=17500
Total loss:	23.382 (rec:0.041, round:23.341)	b=4.25	count=18000
Total loss:	1.357 (rec:0.039, round:1.318)	b=3.69	count=18500
Total loss:	0.094 (rec:0.040, round:0.054)	b=3.12	count=19000
Total loss:	0.039 (rec:0.039, round:0.000)	b=2.56	count=19500
Total loss:	0.039 (rec:0.039, round:0.000)	b=2.00	count=20000
Reconstruction for layer linear
Init alpha to be FP32
Total loss:	0.003 (rec:0.003, round:0.000)	b=0.00	count=500
Total loss:	0.003 (rec:0.003, round:0.000)	b=0.00	count=1000
Total loss:	0.003 (rec:0.003, round:0.000)	b=0.00	count=1500
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=2000
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=2500
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=3000
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=3500
Total loss:	1154383.000 (rec:0.001, round:1154383.000)	b=20.00	count=4000
Total loss:	390759.125 (rec:0.004, round:390759.125)	b=19.44	count=4500
Total loss:	352427.562 (rec:0.004, round:352427.562)	b=18.88	count=5000
Total loss:	325679.125 (rec:0.004, round:325679.125)	b=18.31	count=5500
Total loss:	302238.188 (rec:0.004, round:302238.188)	b=17.75	count=6000
Total loss:	280191.312 (rec:0.005, round:280191.312)	b=17.19	count=6500
Total loss:	259156.562 (rec:0.004, round:259156.562)	b=16.62	count=7000
Total loss:	238641.531 (rec:0.004, round:238641.531)	b=16.06	count=7500
Total loss:	218447.422 (rec:0.006, round:218447.422)	b=15.50	count=8000
Total loss:	198722.344 (rec:0.005, round:198722.344)	b=14.94	count=8500
Total loss:	179353.219 (rec:0.004, round:179353.219)	b=14.38	count=9000
Total loss:	160841.406 (rec:0.004, round:160841.406)	b=13.81	count=9500
Total loss:	142740.266 (rec:0.004, round:142740.266)	b=13.25	count=10000
Total loss:	125529.273 (rec:0.004, round:125529.266)	b=12.69	count=10500
Total loss:	108996.250 (rec:0.005, round:108996.242)	b=12.12	count=11000
Total loss:	93642.352 (rec:0.005, round:93642.344)	b=11.56	count=11500
Total loss:	79078.359 (rec:0.006, round:79078.352)	b=11.00	count=12000
Total loss:	65335.777 (rec:0.005, round:65335.773)	b=10.44	count=12500
Total loss:	52789.719 (rec:0.005, round:52789.715)	b=9.88	count=13000
Total loss:	41597.453 (rec:0.006, round:41597.445)	b=9.31	count=13500
Total loss:	31606.785 (rec:0.006, round:31606.779)	b=8.75	count=14000
Total loss:	23010.861 (rec:0.006, round:23010.855)	b=8.19	count=14500
Total loss:	15810.321 (rec:0.005, round:15810.316)	b=7.62	count=15000
Total loss:	10099.710 (rec:0.007, round:10099.703)	b=7.06	count=15500
Total loss:	5782.176 (rec:0.006, round:5782.171)	b=6.50	count=16000
Total loss:	2859.427 (rec:0.007, round:2859.420)	b=5.94	count=16500
Total loss:	1122.940 (rec:0.008, round:1122.932)	b=5.38	count=17000
Total loss:	297.345 (rec:0.006, round:297.338)	b=4.81	count=17500
Total loss:	28.951 (rec:0.009, round:28.941)	b=4.25	count=18000
Total loss:	0.868 (rec:0.008, round:0.860)	b=3.69	count=18500
Total loss:	0.166 (rec:0.007, round:0.159)	b=3.12	count=19000
Total loss:	0.006 (rec:0.006, round:0.000)	b=2.56	count=19500
Total loss:	0.005 (rec:0.005, round:0.000)	b=2.00	count=20000
Reconstruction for layer linear
Init alpha to be FP32
Total loss:	0.067 (rec:0.067, round:0.000)	b=0.00	count=500
Total loss:	0.077 (rec:0.077, round:0.000)	b=0.00	count=1000
Total loss:	0.054 (rec:0.054, round:0.000)	b=0.00	count=1500
Total loss:	0.060 (rec:0.060, round:0.000)	b=0.00	count=2000
Total loss:	0.043 (rec:0.043, round:0.000)	b=0.00	count=2500
Total loss:	0.041 (rec:0.041, round:0.000)	b=0.00	count=3000
Total loss:	0.035 (rec:0.035, round:0.000)	b=0.00	count=3500
Total loss:	738532.562 (rec:0.033, round:738532.500)	b=20.00	count=4000
Total loss:	298134.375 (rec:0.053, round:298134.312)	b=19.44	count=4500
Total loss:	272037.594 (rec:0.052, round:272037.531)	b=18.88	count=5000
Total loss:	253573.266 (rec:0.051, round:253573.219)	b=18.31	count=5500
Total loss:	237651.828 (rec:0.048, round:237651.781)	b=17.75	count=6000
Total loss:	222700.828 (rec:0.048, round:222700.781)	b=17.19	count=6500
Total loss:	208242.578 (rec:0.049, round:208242.531)	b=16.62	count=7000
Total loss:	194244.125 (rec:0.046, round:194244.078)	b=16.06	count=7500
Total loss:	180636.250 (rec:0.050, round:180636.203)	b=15.50	count=8000
Total loss:	167074.953 (rec:0.047, round:167074.906)	b=14.94	count=8500
Total loss:	153699.422 (rec:0.047, round:153699.375)	b=14.38	count=9000
Total loss:	140584.234 (rec:0.047, round:140584.188)	b=13.81	count=9500
Total loss:	127578.516 (rec:0.052, round:127578.461)	b=13.25	count=10000
Total loss:	114870.109 (rec:0.052, round:114870.055)	b=12.69	count=10500
Total loss:	102647.156 (rec:0.052, round:102647.102)	b=12.12	count=11000
Total loss:	90814.789 (rec:0.054, round:90814.734)	b=11.56	count=11500
Total loss:	79139.375 (rec:0.057, round:79139.320)	b=11.00	count=12000
Total loss:	67963.742 (rec:0.057, round:67963.688)	b=10.44	count=12500
Total loss:	57224.867 (rec:0.062, round:57224.805)	b=9.88	count=13000
Total loss:	47098.152 (rec:0.064, round:47098.086)	b=9.31	count=13500
Total loss:	37521.148 (rec:0.064, round:37521.086)	b=8.75	count=14000
Total loss:	28469.893 (rec:0.068, round:28469.824)	b=8.19	count=14500
Total loss:	20140.131 (rec:0.073, round:20140.057)	b=7.62	count=15000
Total loss:	12826.238 (rec:0.078, round:12826.160)	b=7.06	count=15500
Total loss:	6940.915 (rec:0.077, round:6940.838)	b=6.50	count=16000
Total loss:	3078.269 (rec:0.082, round:3078.187)	b=5.94	count=16500
Total loss:	1107.833 (rec:0.082, round:1107.751)	b=5.38	count=17000
Total loss:	314.999 (rec:0.082, round:314.918)	b=4.81	count=17500
Total loss:	62.409 (rec:0.082, round:62.327)	b=4.25	count=18000
Total loss:	4.239 (rec:0.081, round:4.158)	b=3.69	count=18500
Total loss:	0.257 (rec:0.085, round:0.172)	b=3.12	count=19000
Total loss:	0.078 (rec:0.078, round:0.000)	b=2.56	count=19500
Total loss:	0.080 (rec:0.080, round:0.000)	b=2.00	count=20000
Test: [  0/782]	Time 58.356 (58.356)	Acc@1  90.62 ( 90.62)	Acc@5  96.88 ( 96.88)
Test: [200/782]	Time  0.052 ( 2.577)	Acc@1  87.50 ( 84.12)	Acc@5  95.31 ( 96.44)
Test: [400/782]	Time  0.039 ( 2.094)	Acc@1  92.19 ( 81.88)	Acc@5 100.00 ( 95.56)
Test: [600/782]	Time  1.925 ( 2.003)	Acc@1  81.25 ( 80.20)	Acc@5  95.31 ( 94.62)
 * Acc@1 79.354 Acc@5 94.330
Weight quantization accuracy: 79.35399627685547
Reconstruction for layer conv
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=5000
Reconstruction for block 1
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=5000
Reconstruction for block 0
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=5000
Reconstruction for block 1
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=5000
Reconstruction for block 0
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=5000
Reconstruction for block 1
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=5000
Reconstruction for block 2
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=5000
Reconstruction for block 0
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=5000
Reconstruction for block context_module
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=5000
Reconstruction for block local_module
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=5000
Reconstruction for block context_module
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=5000
Reconstruction for block local_module
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=5000
Reconstruction for block context_module
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=5000
Reconstruction for block local_module
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=5000
Reconstruction for block 0
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=5000
Reconstruction for block context_module
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=5000
Reconstruction for block local_module
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=5000
Reconstruction for block context_module
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.004 (rec:0.004, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=5000
Reconstruction for block local_module
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=5000
Reconstruction for block context_module
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=3500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=5000
Reconstruction for block local_module
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=5000
Reconstruction for block context_module
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=5000
Reconstruction for block local_module
Total loss:	0.003 (rec:0.003, round:0.000)	b=0.00	count=500
Total loss:	0.003 (rec:0.003, round:0.000)	b=0.00	count=1000
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=1500
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=2000
Total loss:	0.003 (rec:0.003, round:0.000)	b=0.00	count=2500
Total loss:	0.003 (rec:0.003, round:0.000)	b=0.00	count=3000
Total loss:	0.003 (rec:0.003, round:0.000)	b=0.00	count=3500
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=4000
Total loss:	0.003 (rec:0.003, round:0.000)	b=0.00	count=4500
Total loss:	0.003 (rec:0.003, round:0.000)	b=0.00	count=5000
Reconstruction for layer conv
Total loss:	0.009 (rec:0.009, round:0.000)	b=0.00	count=500
Total loss:	0.009 (rec:0.009, round:0.000)	b=0.00	count=1000
Total loss:	0.008 (rec:0.008, round:0.000)	b=0.00	count=1500
Total loss:	0.008 (rec:0.008, round:0.000)	b=0.00	count=2000
Total loss:	0.008 (rec:0.008, round:0.000)	b=0.00	count=2500
Total loss:	0.009 (rec:0.009, round:0.000)	b=0.00	count=3000
Total loss:	0.008 (rec:0.008, round:0.000)	b=0.00	count=3500
Total loss:	0.008 (rec:0.008, round:0.000)	b=0.00	count=4000
Total loss:	0.009 (rec:0.009, round:0.000)	b=0.00	count=4500
Total loss:	0.008 (rec:0.008, round:0.000)	b=0.00	count=5000
Reconstruction for layer linear
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=2000
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4000
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=4500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=5000
Reconstruction for layer linear
Total loss:	0.025 (rec:0.025, round:0.000)	b=0.00	count=500
Total loss:	0.027 (rec:0.027, round:0.000)	b=0.00	count=1000
Total loss:	0.023 (rec:0.023, round:0.000)	b=0.00	count=1500
Total loss:	0.024 (rec:0.024, round:0.000)	b=0.00	count=2000
Total loss:	0.023 (rec:0.023, round:0.000)	b=0.00	count=2500
Total loss:	0.025 (rec:0.025, round:0.000)	b=0.00	count=3000
Total loss:	0.025 (rec:0.025, round:0.000)	b=0.00	count=3500
Total loss:	0.029 (rec:0.029, round:0.000)	b=0.00	count=4000
Total loss:	0.026 (rec:0.026, round:0.000)	b=0.00	count=4500
Total loss:	0.023 (rec:0.023, round:0.000)	b=0.00	count=5000
Test: [  0/782]	Time  7.934 ( 7.934)	Acc@1  90.62 ( 90.62)	Acc@5  96.88 ( 96.88)
Test: [200/782]	Time  0.048 ( 1.216)	Acc@1  87.50 ( 84.13)	Acc@5  95.31 ( 96.43)
Test: [400/782]	Time  0.043 ( 1.255)	Acc@1  92.19 ( 81.89)	Acc@5 100.00 ( 95.55)
Test: [600/782]	Time  0.884 ( 1.362)	Acc@1  81.25 ( 80.18)	Acc@5  95.31 ( 94.62)
 * Acc@1 79.334 Acc@5 94.330
Full quantization (W8A12) accuracy: 79.33399963378906
