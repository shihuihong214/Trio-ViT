You are using fake SyncBatchNorm2d who is actually the official BatchNorm2d
==> Using Pytorch Dataset
QuantModel(
  (model): EfficientViTCls(
    (backbone): EfficientViTBackbone(
      (input_stem): OpSequential(
        (op_list): ModuleList(
          (0): ConvLayer(
            (conv): QuantModule(
              3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)
              (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
              (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (activation_function): Hardswish()
            )
            (norm): StraightThrough()
            (act): StraightThrough()
          )
          (1): QauntMBBlock(
            (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (activation_function): StraightThrough()
            (inv_res): ResidualBlock(
              (main): DSConv(
                (depth_conv): ConvLayer(
                  (conv): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16)
                  (norm): StraightThrough()
                  (act): Hardswish()
                )
                (point_conv): ConvLayer(
                  (conv): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1))
                  (norm): StraightThrough()
                )
              )
              (shortcut): IdentityLayer()
            )
            (conv): Sequential(
              (0): QuantModule(
                16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16
                (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): Hardswish()
              )
              (1): QuantModule(
                16, 16, kernel_size=(1, 1), stride=(1, 1)
                (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
              )
            )
          )
        )
      )
      (stages): ModuleList(
        (0): OpSequential(
          (op_list): ModuleList(
            (0): QauntMBBlock(
              (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (activation_function): StraightThrough()
              (inv_res): ResidualBlock(
                (main): MBConv(
                  (inverted_conv): ConvLayer(
                    (conv): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))
                    (norm): StraightThrough()
                    (act): Hardswish()
                  )
                  (depth_conv): ConvLayer(
                    (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64)
                    (norm): StraightThrough()
                    (act): Hardswish()
                  )
                  (point_conv): ConvLayer(
                    (conv): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))
                    (norm): StraightThrough()
                  )
                )
              )
              (conv): Sequential(
                (0): QuantModule(
                  16, 64, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): Hardswish()
                )
                (1): QuantModule_Shifted(
                  64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (input_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (output_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): Hardswish()
                )
                (2): QuantModule_Shifted(
                  64, 32, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (input_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (output_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
              )
            )
            (1): QauntMBBlock(
              (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (activation_function): StraightThrough()
              (inv_res): ResidualBlock(
                (main): MBConv(
                  (inverted_conv): ConvLayer(
                    (conv): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
                    (norm): StraightThrough()
                    (act): Hardswish()
                  )
                  (depth_conv): ConvLayer(
                    (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
                    (norm): StraightThrough()
                    (act): Hardswish()
                  )
                  (point_conv): ConvLayer(
                    (conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
                    (norm): StraightThrough()
                  )
                )
                (shortcut): IdentityLayer()
              )
              (conv): Sequential(
                (0): QuantModule(
                  32, 128, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): Hardswish()
                )
                (1): QuantModule_Shifted(
                  128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (input_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (output_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): Hardswish()
                )
                (2): QuantModule_Shifted(
                  128, 32, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (input_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (output_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
              )
            )
          )
        )
        (1): OpSequential(
          (op_list): ModuleList(
            (0): QauntMBBlock(
              (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (activation_function): StraightThrough()
              (inv_res): ResidualBlock(
                (main): MBConv(
                  (inverted_conv): ConvLayer(
                    (conv): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))
                    (norm): StraightThrough()
                    (act): Hardswish()
                  )
                  (depth_conv): ConvLayer(
                    (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)
                    (norm): StraightThrough()
                    (act): Hardswish()
                  )
                  (point_conv): ConvLayer(
                    (conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
                    (norm): StraightThrough()
                  )
                )
              )
              (conv): Sequential(
                (0): QuantModule(
                  32, 128, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): Hardswish()
                )
                (1): QuantModule_Shifted(
                  128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (input_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (output_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): Hardswish()
                )
                (2): QuantModule_Shifted(
                  128, 64, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (input_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (output_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
              )
            )
            (1): QauntMBBlock(
              (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (activation_function): StraightThrough()
              (inv_res): ResidualBlock(
                (main): MBConv(
                  (inverted_conv): ConvLayer(
                    (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                    (norm): StraightThrough()
                    (act): Hardswish()
                  )
                  (depth_conv): ConvLayer(
                    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
                    (norm): StraightThrough()
                    (act): Hardswish()
                  )
                  (point_conv): ConvLayer(
                    (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
                    (norm): StraightThrough()
                  )
                )
                (shortcut): IdentityLayer()
              )
              (conv): Sequential(
                (0): QuantModule(
                  64, 256, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): Hardswish()
                )
                (1): QuantModule_Shifted(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (input_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (output_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): Hardswish()
                )
                (2): QuantModule_Shifted(
                  256, 64, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (input_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (output_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
              )
            )
            (2): QauntMBBlock(
              (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (activation_function): StraightThrough()
              (inv_res): ResidualBlock(
                (main): MBConv(
                  (inverted_conv): ConvLayer(
                    (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                    (norm): StraightThrough()
                    (act): Hardswish()
                  )
                  (depth_conv): ConvLayer(
                    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)
                    (norm): StraightThrough()
                    (act): Hardswish()
                  )
                  (point_conv): ConvLayer(
                    (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
                    (norm): StraightThrough()
                  )
                )
                (shortcut): IdentityLayer()
              )
              (conv): Sequential(
                (0): QuantModule(
                  64, 256, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): Hardswish()
                )
                (1): QuantModule_Shifted(
                  256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (input_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (output_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): Hardswish()
                )
                (2): QuantModule_Shifted(
                  256, 64, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (input_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (output_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
              )
            )
          )
        )
        (2): OpSequential(
          (op_list): ModuleList(
            (0): QauntMBBlock(
              (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (activation_function): StraightThrough()
              (inv_res): ResidualBlock(
                (main): MBConv(
                  (inverted_conv): ConvLayer(
                    (conv): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
                    (act): Hardswish()
                  )
                  (depth_conv): ConvLayer(
                    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)
                    (act): Hardswish()
                  )
                  (point_conv): ConvLayer(
                    (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
                    (norm): StraightThrough()
                  )
                )
              )
              (conv): Sequential(
                (0): QuantModule(
                  64, 256, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): Hardswish()
                )
                (1): QuantModule_Shifted(
                  256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (input_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (output_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): Hardswish()
                )
                (2): QuantModule_Shifted(
                  256, 128, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (input_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (output_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
              )
            )
            (1): EfficientViTBlock(
              (context_module): QauntMBBlock(
                (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
                (inv_res): ResidualBlock(
                  (main): LiteMSA(
                    (qkv): ConvLayer(
                      (conv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
                    )
                    (aggreg): ModuleList(
                      (0): Sequential(
                        (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)
                        (1): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), groups=24, bias=False)
                      )
                    )
                    (kernel_func): ReLU()
                    (proj): ConvLayer(
                      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
                      (norm): StraightThrough()
                    )
                  )
                  (shortcut): IdentityLayer()
                )
                (qkv): QuantModule(
                  128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
                (aggreg): ModuleList(
                  (0): Sequential(
                    (0): QuantModule(
                      384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False
                      (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                      (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                      (activation_function): StraightThrough()
                    )
                    (1): QuantModule(
                      384, 384, kernel_size=(1, 1), stride=(1, 1), groups=24, bias=False
                      (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                      (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                      (activation_function): StraightThrough()
                    )
                  )
                )
                (proj): QuantModule(
                  256, 128, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
                (kernel_func): ReLU()
                (k_v_quant): Sequential(
                  (0): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (1): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                )
                (q_kv_quant): Sequential(
                  (0): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (1): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                )
              )
              (local_module): QauntMBBlock(
                (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
                (inv_res): ResidualBlock(
                  (main): MBConv(
                    (inverted_conv): ConvLayer(
                      (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                      (act): Hardswish()
                    )
                    (depth_conv): ConvLayer(
                      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                      (act): Hardswish()
                    )
                    (point_conv): ConvLayer(
                      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                      (norm): StraightThrough()
                    )
                  )
                  (shortcut): IdentityLayer()
                )
                (conv): Sequential(
                  (0): QuantModule(
                    128, 512, kernel_size=(1, 1), stride=(1, 1)
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): Hardswish()
                  )
                  (1): QuantModule_Shifted(
                    512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (input_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (output_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): Hardswish()
                  )
                  (2): QuantModule_Shifted(
                    512, 128, kernel_size=(1, 1), stride=(1, 1)
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (input_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (output_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): StraightThrough()
                  )
                )
              )
            )
            (2): EfficientViTBlock(
              (context_module): QauntMBBlock(
                (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
                (inv_res): ResidualBlock(
                  (main): LiteMSA(
                    (qkv): ConvLayer(
                      (conv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
                    )
                    (aggreg): ModuleList(
                      (0): Sequential(
                        (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)
                        (1): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), groups=24, bias=False)
                      )
                    )
                    (kernel_func): ReLU()
                    (proj): ConvLayer(
                      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
                      (norm): StraightThrough()
                    )
                  )
                  (shortcut): IdentityLayer()
                )
                (qkv): QuantModule(
                  128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
                (aggreg): ModuleList(
                  (0): Sequential(
                    (0): QuantModule(
                      384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False
                      (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                      (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                      (activation_function): StraightThrough()
                    )
                    (1): QuantModule(
                      384, 384, kernel_size=(1, 1), stride=(1, 1), groups=24, bias=False
                      (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                      (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                      (activation_function): StraightThrough()
                    )
                  )
                )
                (proj): QuantModule(
                  256, 128, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
                (kernel_func): ReLU()
                (k_v_quant): Sequential(
                  (0): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (1): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                )
                (q_kv_quant): Sequential(
                  (0): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (1): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                )
              )
              (local_module): QauntMBBlock(
                (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
                (inv_res): ResidualBlock(
                  (main): MBConv(
                    (inverted_conv): ConvLayer(
                      (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                      (act): Hardswish()
                    )
                    (depth_conv): ConvLayer(
                      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                      (act): Hardswish()
                    )
                    (point_conv): ConvLayer(
                      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                      (norm): StraightThrough()
                    )
                  )
                  (shortcut): IdentityLayer()
                )
                (conv): Sequential(
                  (0): QuantModule(
                    128, 512, kernel_size=(1, 1), stride=(1, 1)
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): Hardswish()
                  )
                  (1): QuantModule_Shifted(
                    512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (input_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (output_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): Hardswish()
                  )
                  (2): QuantModule_Shifted(
                    512, 128, kernel_size=(1, 1), stride=(1, 1)
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (input_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (output_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): StraightThrough()
                  )
                )
              )
            )
            (3): EfficientViTBlock(
              (context_module): QauntMBBlock(
                (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
                (inv_res): ResidualBlock(
                  (main): LiteMSA(
                    (qkv): ConvLayer(
                      (conv): Conv2d(128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
                    )
                    (aggreg): ModuleList(
                      (0): Sequential(
                        (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)
                        (1): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1), groups=24, bias=False)
                      )
                    )
                    (kernel_func): ReLU()
                    (proj): ConvLayer(
                      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))
                      (norm): StraightThrough()
                    )
                  )
                  (shortcut): IdentityLayer()
                )
                (qkv): QuantModule(
                  128, 384, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
                (aggreg): ModuleList(
                  (0): Sequential(
                    (0): QuantModule(
                      384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False
                      (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                      (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                      (activation_function): StraightThrough()
                    )
                    (1): QuantModule(
                      384, 384, kernel_size=(1, 1), stride=(1, 1), groups=24, bias=False
                      (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                      (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                      (activation_function): StraightThrough()
                    )
                  )
                )
                (proj): QuantModule(
                  256, 128, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
                (kernel_func): ReLU()
                (k_v_quant): Sequential(
                  (0): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (1): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                )
                (q_kv_quant): Sequential(
                  (0): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (1): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                )
              )
              (local_module): QauntMBBlock(
                (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
                (inv_res): ResidualBlock(
                  (main): MBConv(
                    (inverted_conv): ConvLayer(
                      (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                      (act): Hardswish()
                    )
                    (depth_conv): ConvLayer(
                      (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
                      (act): Hardswish()
                    )
                    (point_conv): ConvLayer(
                      (conv): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
                      (norm): StraightThrough()
                    )
                  )
                  (shortcut): IdentityLayer()
                )
                (conv): Sequential(
                  (0): QuantModule(
                    128, 512, kernel_size=(1, 1), stride=(1, 1)
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): Hardswish()
                  )
                  (1): QuantModule_Shifted(
                    512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (input_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (output_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): Hardswish()
                  )
                  (2): QuantModule_Shifted(
                    512, 128, kernel_size=(1, 1), stride=(1, 1)
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (input_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (output_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): StraightThrough()
                  )
                )
              )
            )
          )
        )
        (3): OpSequential(
          (op_list): ModuleList(
            (0): QauntMBBlock(
              (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
              (activation_function): StraightThrough()
              (inv_res): ResidualBlock(
                (main): MBConv(
                  (inverted_conv): ConvLayer(
                    (conv): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
                    (act): Hardswish()
                  )
                  (depth_conv): ConvLayer(
                    (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)
                    (act): Hardswish()
                  )
                  (point_conv): ConvLayer(
                    (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
                    (norm): StraightThrough()
                  )
                )
              )
              (conv): Sequential(
                (0): QuantModule(
                  128, 512, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): Hardswish()
                )
                (1): QuantModule_Shifted(
                  512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (input_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (output_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): Hardswish()
                )
                (2): QuantModule_Shifted(
                  512, 256, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (input_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (output_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
              )
            )
            (1): EfficientViTBlock(
              (context_module): QauntMBBlock(
                (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
                (inv_res): ResidualBlock(
                  (main): LiteMSA(
                    (qkv): ConvLayer(
                      (conv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
                    )
                    (aggreg): ModuleList(
                      (0): Sequential(
                        (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)
                        (1): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False)
                      )
                    )
                    (kernel_func): ReLU()
                    (proj): ConvLayer(
                      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
                      (norm): StraightThrough()
                    )
                  )
                  (shortcut): IdentityLayer()
                )
                (qkv): QuantModule(
                  256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
                (aggreg): ModuleList(
                  (0): Sequential(
                    (0): QuantModule(
                      768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False
                      (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                      (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                      (activation_function): StraightThrough()
                    )
                    (1): QuantModule(
                      768, 768, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False
                      (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                      (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                      (activation_function): StraightThrough()
                    )
                  )
                )
                (proj): QuantModule(
                  512, 256, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
                (kernel_func): ReLU()
                (k_v_quant): Sequential(
                  (0): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (1): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                )
                (q_kv_quant): Sequential(
                  (0): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (1): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                )
              )
              (local_module): QauntMBBlock(
                (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
                (inv_res): ResidualBlock(
                  (main): MBConv(
                    (inverted_conv): ConvLayer(
                      (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
                      (act): Hardswish()
                    )
                    (depth_conv): ConvLayer(
                      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
                      (act): Hardswish()
                    )
                    (point_conv): ConvLayer(
                      (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
                      (norm): StraightThrough()
                    )
                  )
                  (shortcut): IdentityLayer()
                )
                (conv): Sequential(
                  (0): QuantModule(
                    256, 1024, kernel_size=(1, 1), stride=(1, 1)
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): Hardswish()
                  )
                  (1): QuantModule_Shifted(
                    1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (input_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (output_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): Hardswish()
                  )
                  (2): QuantModule_Shifted(
                    1024, 256, kernel_size=(1, 1), stride=(1, 1)
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (input_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (output_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): StraightThrough()
                  )
                )
              )
            )
            (2): EfficientViTBlock(
              (context_module): QauntMBBlock(
                (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
                (inv_res): ResidualBlock(
                  (main): LiteMSA(
                    (qkv): ConvLayer(
                      (conv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
                    )
                    (aggreg): ModuleList(
                      (0): Sequential(
                        (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)
                        (1): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False)
                      )
                    )
                    (kernel_func): ReLU()
                    (proj): ConvLayer(
                      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
                      (norm): StraightThrough()
                    )
                  )
                  (shortcut): IdentityLayer()
                )
                (qkv): QuantModule(
                  256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
                (aggreg): ModuleList(
                  (0): Sequential(
                    (0): QuantModule(
                      768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False
                      (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                      (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                      (activation_function): StraightThrough()
                    )
                    (1): QuantModule(
                      768, 768, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False
                      (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                      (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                      (activation_function): StraightThrough()
                    )
                  )
                )
                (proj): QuantModule(
                  512, 256, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
                (kernel_func): ReLU()
                (k_v_quant): Sequential(
                  (0): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (1): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                )
                (q_kv_quant): Sequential(
                  (0): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (1): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                )
              )
              (local_module): QauntMBBlock(
                (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
                (inv_res): ResidualBlock(
                  (main): MBConv(
                    (inverted_conv): ConvLayer(
                      (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
                      (act): Hardswish()
                    )
                    (depth_conv): ConvLayer(
                      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
                      (act): Hardswish()
                    )
                    (point_conv): ConvLayer(
                      (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
                      (norm): StraightThrough()
                    )
                  )
                  (shortcut): IdentityLayer()
                )
                (conv): Sequential(
                  (0): QuantModule(
                    256, 1024, kernel_size=(1, 1), stride=(1, 1)
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): Hardswish()
                  )
                  (1): QuantModule_Shifted(
                    1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (input_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (output_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): Hardswish()
                  )
                  (2): QuantModule_Shifted(
                    1024, 256, kernel_size=(1, 1), stride=(1, 1)
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (input_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (output_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): StraightThrough()
                  )
                )
              )
            )
            (3): EfficientViTBlock(
              (context_module): QauntMBBlock(
                (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
                (inv_res): ResidualBlock(
                  (main): LiteMSA(
                    (qkv): ConvLayer(
                      (conv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
                    )
                    (aggreg): ModuleList(
                      (0): Sequential(
                        (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)
                        (1): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False)
                      )
                    )
                    (kernel_func): ReLU()
                    (proj): ConvLayer(
                      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
                      (norm): StraightThrough()
                    )
                  )
                  (shortcut): IdentityLayer()
                )
                (qkv): QuantModule(
                  256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
                (aggreg): ModuleList(
                  (0): Sequential(
                    (0): QuantModule(
                      768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False
                      (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                      (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                      (activation_function): StraightThrough()
                    )
                    (1): QuantModule(
                      768, 768, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False
                      (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                      (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                      (activation_function): StraightThrough()
                    )
                  )
                )
                (proj): QuantModule(
                  512, 256, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
                (kernel_func): ReLU()
                (k_v_quant): Sequential(
                  (0): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (1): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                )
                (q_kv_quant): Sequential(
                  (0): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (1): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                )
              )
              (local_module): QauntMBBlock(
                (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
                (inv_res): ResidualBlock(
                  (main): MBConv(
                    (inverted_conv): ConvLayer(
                      (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
                      (act): Hardswish()
                    )
                    (depth_conv): ConvLayer(
                      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
                      (act): Hardswish()
                    )
                    (point_conv): ConvLayer(
                      (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
                      (norm): StraightThrough()
                    )
                  )
                  (shortcut): IdentityLayer()
                )
                (conv): Sequential(
                  (0): QuantModule(
                    256, 1024, kernel_size=(1, 1), stride=(1, 1)
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): Hardswish()
                  )
                  (1): QuantModule_Shifted(
                    1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (input_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (output_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): Hardswish()
                  )
                  (2): QuantModule_Shifted(
                    1024, 256, kernel_size=(1, 1), stride=(1, 1)
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (input_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (output_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): StraightThrough()
                  )
                )
              )
            )
            (4): EfficientViTBlock(
              (context_module): QauntMBBlock(
                (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
                (inv_res): ResidualBlock(
                  (main): LiteMSA(
                    (qkv): ConvLayer(
                      (conv): Conv2d(256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
                    )
                    (aggreg): ModuleList(
                      (0): Sequential(
                        (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)
                        (1): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False)
                      )
                    )
                    (kernel_func): ReLU()
                    (proj): ConvLayer(
                      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))
                      (norm): StraightThrough()
                    )
                  )
                  (shortcut): IdentityLayer()
                )
                (qkv): QuantModule(
                  256, 768, kernel_size=(1, 1), stride=(1, 1), bias=False
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
                (aggreg): ModuleList(
                  (0): Sequential(
                    (0): QuantModule(
                      768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False
                      (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                      (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                      (activation_function): StraightThrough()
                    )
                    (1): QuantModule(
                      768, 768, kernel_size=(1, 1), stride=(1, 1), groups=48, bias=False
                      (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                      (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                      (activation_function): StraightThrough()
                    )
                  )
                )
                (proj): QuantModule(
                  512, 256, kernel_size=(1, 1), stride=(1, 1)
                  (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                  (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (activation_function): StraightThrough()
                )
                (kernel_func): ReLU()
                (k_v_quant): Sequential(
                  (0): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (1): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                )
                (q_kv_quant): Sequential(
                  (0): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                  (1): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                )
              )
              (local_module): QauntMBBlock(
                (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                (activation_function): StraightThrough()
                (inv_res): ResidualBlock(
                  (main): MBConv(
                    (inverted_conv): ConvLayer(
                      (conv): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
                      (act): Hardswish()
                    )
                    (depth_conv): ConvLayer(
                      (conv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)
                      (act): Hardswish()
                    )
                    (point_conv): ConvLayer(
                      (conv): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
                      (norm): StraightThrough()
                    )
                  )
                  (shortcut): IdentityLayer()
                )
                (conv): Sequential(
                  (0): QuantModule(
                    256, 1024, kernel_size=(1, 1), stride=(1, 1)
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): Hardswish()
                  )
                  (1): QuantModule_Shifted(
                    1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (input_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (output_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): Hardswish()
                  )
                  (2): QuantModule_Shifted(
                    1024, 256, kernel_size=(1, 1), stride=(1, 1)
                    (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
                    (input_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (output_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
                    (activation_function): StraightThrough()
                  )
                )
              )
            )
          )
        )
      )
    )
    (head): ClsHead(
      (op_list): ModuleList(
        (0): ConvLayer(
          (conv): QuantModule(
            256, 1536, kernel_size=(1, 1), stride=(1, 1)
            (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
            (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (activation_function): Hardswish()
          )
          (norm): StraightThrough()
          (act): StraightThrough()
        )
        (1): AdaptiveAvgPool2d(output_size=1)
        (2): LinearLayer(
          (linear): QuantModule(
            in_features=1536, out_features=1600, bias=False
            (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
            (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (activation_function): StraightThrough()
          )
          (norm): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)
          (act): Hardswish()
        )
        (3): LinearLayer(
          (linear): QuantModule(
            in_features=1600, out_features=1000, bias=True
            (weight_quantizer): UniformAffineQuantizer(bit=8, scale_method=mse, symmetric=False, channel_wise=True, leaf_param=False)
            (act_quantizer): UniformAffineQuantizer(bit=16, scale_method=mse, symmetric=False, channel_wise=False, leaf_param=True)
            (activation_function): StraightThrough()
          )
        )
      )
    )
  )
)
Test: [  0/782]	Time  0.810 ( 0.810)	Acc@1  90.62 ( 90.62)	Acc@5  96.88 ( 96.88)
Test: [200/782]	Time  0.373 ( 0.137)	Acc@1  87.50 ( 84.06)	Acc@5  95.31 ( 96.44)
Test: [400/782]	Time  0.030 ( 0.139)	Acc@1  92.19 ( 81.88)	Acc@5 100.00 ( 95.62)
Test: [600/782]	Time  0.032 ( 0.139)	Acc@1  81.25 ( 80.16)	Acc@5  96.88 ( 94.66)
 * Acc@1 79.302 Acc@5 94.350
Quantized accuracy before brecq: 79.30199432373047
Reconstruction for layer conv
Init alpha to be FP32
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3500
Total loss:	203.086 (rec:0.000, round:203.086)	b=20.00	count=4000
Total loss:	62.570 (rec:0.000, round:62.570)	b=19.44	count=4500
Total loss:	51.204 (rec:0.000, round:51.204)	b=18.88	count=5000
Total loss:	42.389 (rec:0.000, round:42.389)	b=18.31	count=5500
Total loss:	36.677 (rec:0.000, round:36.677)	b=17.75	count=6000
Total loss:	31.482 (rec:0.000, round:31.482)	b=17.19	count=6500
Total loss:	24.626 (rec:0.000, round:24.626)	b=16.62	count=7000
Total loss:	18.440 (rec:0.000, round:18.440)	b=16.06	count=7500
Total loss:	14.499 (rec:0.000, round:14.499)	b=15.50	count=8000
Total loss:	12.624 (rec:0.000, round:12.624)	b=14.94	count=8500
Total loss:	10.979 (rec:0.000, round:10.979)	b=14.38	count=9000
Total loss:	8.431 (rec:0.000, round:8.431)	b=13.81	count=9500
Total loss:	7.457 (rec:0.000, round:7.457)	b=13.25	count=10000
Total loss:	5.500 (rec:0.000, round:5.500)	b=12.69	count=10500
Total loss:	4.999 (rec:0.000, round:4.999)	b=12.12	count=11000
Total loss:	3.519 (rec:0.000, round:3.519)	b=11.56	count=11500
Total loss:	3.000 (rec:0.000, round:3.000)	b=11.00	count=12000
Total loss:	3.000 (rec:0.000, round:3.000)	b=10.44	count=12500
Total loss:	3.000 (rec:0.000, round:3.000)	b=9.88	count=13000
Total loss:	3.000 (rec:0.000, round:3.000)	b=9.31	count=13500
Total loss:	3.000 (rec:0.000, round:3.000)	b=8.75	count=14000
Total loss:	2.960 (rec:0.000, round:2.959)	b=8.19	count=14500
Total loss:	2.500 (rec:0.000, round:2.500)	b=7.62	count=15000
Total loss:	2.500 (rec:0.000, round:2.500)	b=7.06	count=15500
Total loss:	2.500 (rec:0.000, round:2.500)	b=6.50	count=16000
Total loss:	2.217 (rec:0.000, round:2.217)	b=5.94	count=16500
Total loss:	1.603 (rec:0.000, round:1.603)	b=5.38	count=17000
Total loss:	1.000 (rec:0.000, round:1.000)	b=4.81	count=17500
Total loss:	0.971 (rec:0.000, round:0.970)	b=4.25	count=18000
Total loss:	0.254 (rec:0.000, round:0.254)	b=3.69	count=18500
Total loss:	0.000 (rec:0.000, round:0.000)	b=3.12	count=19000
Total loss:	0.000 (rec:0.000, round:0.000)	b=2.56	count=19500
Total loss:	0.000 (rec:0.000, round:0.000)	b=2.00	count=20000
Reconstruction for block 1
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3500
Total loss:	172.434 (rec:0.000, round:172.434)	b=20.00	count=4000
Total loss:	69.493 (rec:0.000, round:69.493)	b=19.44	count=4500
Total loss:	60.626 (rec:0.000, round:60.625)	b=18.88	count=5000
Total loss:	54.939 (rec:0.000, round:54.939)	b=18.31	count=5500
Total loss:	52.354 (rec:0.000, round:52.354)	b=17.75	count=6000
Total loss:	49.796 (rec:0.000, round:49.796)	b=17.19	count=6500
Total loss:	45.129 (rec:0.000, round:45.128)	b=16.62	count=7000
Total loss:	41.878 (rec:0.000, round:41.877)	b=16.06	count=7500
Total loss:	37.874 (rec:0.000, round:37.874)	b=15.50	count=8000
Total loss:	32.145 (rec:0.001, round:32.144)	b=14.94	count=8500
Total loss:	28.500 (rec:0.001, round:28.499)	b=14.38	count=9000
Total loss:	26.246 (rec:0.001, round:26.245)	b=13.81	count=9500
Total loss:	22.093 (rec:0.001, round:22.092)	b=13.25	count=10000
Total loss:	16.701 (rec:0.001, round:16.701)	b=12.69	count=10500
Total loss:	14.912 (rec:0.001, round:14.912)	b=12.12	count=11000
Total loss:	11.552 (rec:0.001, round:11.552)	b=11.56	count=11500
Total loss:	9.246 (rec:0.001, round:9.245)	b=11.00	count=12000
Total loss:	5.707 (rec:0.001, round:5.706)	b=10.44	count=12500
Total loss:	4.997 (rec:0.001, round:4.996)	b=9.88	count=13000
Total loss:	4.501 (rec:0.001, round:4.500)	b=9.31	count=13500
Total loss:	4.127 (rec:0.001, round:4.127)	b=8.75	count=14000
Total loss:	3.494 (rec:0.001, round:3.493)	b=8.19	count=14500
Total loss:	2.325 (rec:0.001, round:2.324)	b=7.62	count=15000
Total loss:	1.993 (rec:0.001, round:1.992)	b=7.06	count=15500
Total loss:	1.307 (rec:0.001, round:1.306)	b=6.50	count=16000
Total loss:	0.658 (rec:0.001, round:0.657)	b=5.94	count=16500
Total loss:	0.002 (rec:0.002, round:0.000)	b=5.38	count=17000
Total loss:	0.001 (rec:0.001, round:0.000)	b=4.81	count=17500
Total loss:	0.001 (rec:0.001, round:0.000)	b=4.25	count=18000
Total loss:	0.001 (rec:0.001, round:0.000)	b=3.69	count=18500
Total loss:	0.001 (rec:0.001, round:0.000)	b=3.12	count=19000
Total loss:	0.002 (rec:0.002, round:0.000)	b=2.56	count=19500
Total loss:	0.002 (rec:0.002, round:0.000)	b=2.00	count=20000
Reconstruction for block 0
Init alpha to be FP32
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	428.561 (rec:0.001, round:428.561)	b=20.00	count=4000
Total loss:	163.099 (rec:0.001, round:163.097)	b=19.44	count=4500
Total loss:	133.920 (rec:0.001, round:133.919)	b=18.88	count=5000
Total loss:	119.918 (rec:0.001, round:119.917)	b=18.31	count=5500
Total loss:	111.904 (rec:0.001, round:111.903)	b=17.75	count=6000
Total loss:	105.247 (rec:0.001, round:105.246)	b=17.19	count=6500
Total loss:	97.466 (rec:0.001, round:97.465)	b=16.62	count=7000
Total loss:	89.063 (rec:0.001, round:89.062)	b=16.06	count=7500
Total loss:	79.465 (rec:0.001, round:79.463)	b=15.50	count=8000
Total loss:	70.888 (rec:0.001, round:70.887)	b=14.94	count=8500
Total loss:	64.009 (rec:0.001, round:64.008)	b=14.38	count=9000
Total loss:	57.683 (rec:0.002, round:57.681)	b=13.81	count=9500
Total loss:	52.722 (rec:0.001, round:52.721)	b=13.25	count=10000
Total loss:	47.523 (rec:0.002, round:47.522)	b=12.69	count=10500
Total loss:	41.446 (rec:0.002, round:41.445)	b=12.12	count=11000
Total loss:	35.100 (rec:0.002, round:35.098)	b=11.56	count=11500
Total loss:	25.658 (rec:0.002, round:25.657)	b=11.00	count=12000
Total loss:	20.619 (rec:0.002, round:20.617)	b=10.44	count=12500
Total loss:	18.988 (rec:0.002, round:18.986)	b=9.88	count=13000
Total loss:	16.620 (rec:0.002, round:16.618)	b=9.31	count=13500
Total loss:	13.872 (rec:0.002, round:13.869)	b=8.75	count=14000
Total loss:	11.905 (rec:0.002, round:11.903)	b=8.19	count=14500
Total loss:	9.291 (rec:0.003, round:9.288)	b=7.62	count=15000
Total loss:	5.671 (rec:0.003, round:5.667)	b=7.06	count=15500
Total loss:	4.503 (rec:0.003, round:4.500)	b=6.50	count=16000
Total loss:	2.924 (rec:0.003, round:2.921)	b=5.94	count=16500
Total loss:	1.534 (rec:0.004, round:1.531)	b=5.38	count=17000
Total loss:	0.755 (rec:0.004, round:0.751)	b=4.81	count=17500
Total loss:	0.101 (rec:0.004, round:0.098)	b=4.25	count=18000
Total loss:	0.004 (rec:0.004, round:0.000)	b=3.69	count=18500
Total loss:	0.004 (rec:0.004, round:0.000)	b=3.12	count=19000
Total loss:	0.004 (rec:0.004, round:0.000)	b=2.56	count=19500
Total loss:	0.004 (rec:0.004, round:0.000)	b=2.00	count=20000
Reconstruction for block 1
Init alpha to be FP32
Total loss:	0.014 (rec:0.014, round:0.000)	b=0.00	count=500
Total loss:	0.015 (rec:0.015, round:0.000)	b=0.00	count=1000
Total loss:	0.016 (rec:0.016, round:0.000)	b=0.00	count=1500
Total loss:	0.013 (rec:0.013, round:0.000)	b=0.00	count=2000
Total loss:	0.011 (rec:0.011, round:0.000)	b=0.00	count=2500
Total loss:	0.013 (rec:0.013, round:0.000)	b=0.00	count=3000
Total loss:	0.013 (rec:0.013, round:0.000)	b=0.00	count=3500
Total loss:	1525.242 (rec:0.014, round:1525.227)	b=20.00	count=4000
Total loss:	551.792 (rec:0.016, round:551.775)	b=19.44	count=4500
Total loss:	448.613 (rec:0.013, round:448.600)	b=18.88	count=5000
Total loss:	384.183 (rec:0.016, round:384.167)	b=18.31	count=5500
Total loss:	344.533 (rec:0.017, round:344.516)	b=17.75	count=6000
Total loss:	316.305 (rec:0.016, round:316.289)	b=17.19	count=6500
Total loss:	292.065 (rec:0.014, round:292.051)	b=16.62	count=7000
Total loss:	269.155 (rec:0.012, round:269.143)	b=16.06	count=7500
Total loss:	243.221 (rec:0.014, round:243.207)	b=15.50	count=8000
Total loss:	222.472 (rec:0.013, round:222.459)	b=14.94	count=8500
Total loss:	204.514 (rec:0.012, round:204.501)	b=14.38	count=9000
Total loss:	184.320 (rec:0.014, round:184.306)	b=13.81	count=9500
Total loss:	161.461 (rec:0.014, round:161.447)	b=13.25	count=10000
Total loss:	141.432 (rec:0.013, round:141.419)	b=12.69	count=10500
Total loss:	123.376 (rec:0.014, round:123.361)	b=12.12	count=11000
Total loss:	110.630 (rec:0.014, round:110.616)	b=11.56	count=11500
Total loss:	91.394 (rec:0.013, round:91.381)	b=11.00	count=12000
Total loss:	76.654 (rec:0.013, round:76.641)	b=10.44	count=12500
Total loss:	66.006 (rec:0.013, round:65.994)	b=9.88	count=13000
Total loss:	54.694 (rec:0.014, round:54.679)	b=9.31	count=13500
Total loss:	45.969 (rec:0.013, round:45.956)	b=8.75	count=14000
Total loss:	32.995 (rec:0.014, round:32.981)	b=8.19	count=14500
Total loss:	22.158 (rec:0.015, round:22.143)	b=7.62	count=15000
Total loss:	13.543 (rec:0.016, round:13.527)	b=7.06	count=15500
Total loss:	7.859 (rec:0.015, round:7.845)	b=6.50	count=16000
Total loss:	4.579 (rec:0.016, round:4.563)	b=5.94	count=16500
Total loss:	2.000 (rec:0.014, round:1.987)	b=5.38	count=17000
Total loss:	1.092 (rec:0.014, round:1.078)	b=4.81	count=17500
Total loss:	0.014 (rec:0.014, round:0.000)	b=4.25	count=18000
Total loss:	0.015 (rec:0.015, round:0.000)	b=3.69	count=18500
Total loss:	0.014 (rec:0.014, round:0.000)	b=3.12	count=19000
Total loss:	0.014 (rec:0.014, round:0.000)	b=2.56	count=19500
Total loss:	0.015 (rec:0.015, round:0.000)	b=2.00	count=20000
Reconstruction for block 0
Init alpha to be FP32
Total loss:	0.008 (rec:0.008, round:0.000)	b=0.00	count=500
Total loss:	0.007 (rec:0.007, round:0.000)	b=0.00	count=1000
Total loss:	0.008 (rec:0.008, round:0.000)	b=0.00	count=1500
Total loss:	0.007 (rec:0.007, round:0.000)	b=0.00	count=2000
Total loss:	0.007 (rec:0.007, round:0.000)	b=0.00	count=2500
Total loss:	0.007 (rec:0.007, round:0.000)	b=0.00	count=3000
Total loss:	0.008 (rec:0.008, round:0.000)	b=0.00	count=3500
Total loss:	1745.841 (rec:0.007, round:1745.834)	b=20.00	count=4000
Total loss:	698.603 (rec:0.008, round:698.595)	b=19.44	count=4500
Total loss:	610.088 (rec:0.007, round:610.081)	b=18.88	count=5000
Total loss:	546.286 (rec:0.007, round:546.279)	b=18.31	count=5500
Total loss:	506.283 (rec:0.007, round:506.275)	b=17.75	count=6000
Total loss:	467.802 (rec:0.008, round:467.794)	b=17.19	count=6500
Total loss:	429.945 (rec:0.007, round:429.938)	b=16.62	count=7000
Total loss:	401.337 (rec:0.008, round:401.329)	b=16.06	count=7500
Total loss:	367.399 (rec:0.008, round:367.392)	b=15.50	count=8000
Total loss:	328.251 (rec:0.008, round:328.244)	b=14.94	count=8500
Total loss:	298.102 (rec:0.008, round:298.095)	b=14.38	count=9000
Total loss:	265.755 (rec:0.008, round:265.747)	b=13.81	count=9500
Total loss:	236.825 (rec:0.008, round:236.817)	b=13.25	count=10000
Total loss:	199.209 (rec:0.009, round:199.200)	b=12.69	count=10500
Total loss:	173.771 (rec:0.008, round:173.763)	b=12.12	count=11000
Total loss:	142.512 (rec:0.009, round:142.503)	b=11.56	count=11500
Total loss:	119.057 (rec:0.009, round:119.048)	b=11.00	count=12000
Total loss:	98.038 (rec:0.009, round:98.030)	b=10.44	count=12500
Total loss:	80.348 (rec:0.010, round:80.338)	b=9.88	count=13000
Total loss:	66.460 (rec:0.009, round:66.451)	b=9.31	count=13500
Total loss:	51.500 (rec:0.008, round:51.492)	b=8.75	count=14000
Total loss:	36.789 (rec:0.010, round:36.779)	b=8.19	count=14500
Total loss:	25.501 (rec:0.009, round:25.492)	b=7.62	count=15000
Total loss:	18.825 (rec:0.009, round:18.815)	b=7.06	count=15500
Total loss:	11.821 (rec:0.009, round:11.812)	b=6.50	count=16000
Total loss:	6.963 (rec:0.009, round:6.954)	b=5.94	count=16500
Total loss:	2.395 (rec:0.009, round:2.386)	b=5.38	count=17000
Total loss:	0.050 (rec:0.009, round:0.040)	b=4.81	count=17500
Total loss:	0.009 (rec:0.009, round:0.000)	b=4.25	count=18000
Total loss:	0.009 (rec:0.009, round:0.000)	b=3.69	count=18500
Total loss:	0.009 (rec:0.009, round:0.000)	b=3.12	count=19000
Total loss:	0.009 (rec:0.009, round:0.000)	b=2.56	count=19500
Total loss:	0.010 (rec:0.010, round:0.000)	b=2.00	count=20000
Reconstruction for block 1
Init alpha to be FP32
Total loss:	0.015 (rec:0.015, round:0.000)	b=0.00	count=500
Total loss:	0.016 (rec:0.016, round:0.000)	b=0.00	count=1000
Total loss:	0.014 (rec:0.014, round:0.000)	b=0.00	count=1500
Total loss:	0.015 (rec:0.015, round:0.000)	b=0.00	count=2000
Total loss:	0.015 (rec:0.015, round:0.000)	b=0.00	count=2500
Total loss:	0.014 (rec:0.014, round:0.000)	b=0.00	count=3000
Total loss:	0.015 (rec:0.015, round:0.000)	b=0.00	count=3500
Total loss:	6981.174 (rec:0.015, round:6981.159)	b=20.00	count=4000
Total loss:	2649.815 (rec:0.014, round:2649.801)	b=19.44	count=4500
Total loss:	2254.210 (rec:0.014, round:2254.196)	b=18.88	count=5000
Total loss:	1991.605 (rec:0.016, round:1991.589)	b=18.31	count=5500
Total loss:	1784.975 (rec:0.016, round:1784.960)	b=17.75	count=6000
Total loss:	1602.508 (rec:0.014, round:1602.494)	b=17.19	count=6500
Total loss:	1444.971 (rec:0.015, round:1444.956)	b=16.62	count=7000
Total loss:	1293.631 (rec:0.014, round:1293.617)	b=16.06	count=7500
Total loss:	1172.870 (rec:0.016, round:1172.854)	b=15.50	count=8000
Total loss:	1065.919 (rec:0.015, round:1065.904)	b=14.94	count=8500
Total loss:	961.971 (rec:0.013, round:961.958)	b=14.38	count=9000
Total loss:	862.958 (rec:0.014, round:862.944)	b=13.81	count=9500
Total loss:	758.713 (rec:0.016, round:758.697)	b=13.25	count=10000
Total loss:	662.529 (rec:0.016, round:662.513)	b=12.69	count=10500
Total loss:	570.905 (rec:0.016, round:570.889)	b=12.12	count=11000
Total loss:	496.092 (rec:0.014, round:496.078)	b=11.56	count=11500
Total loss:	411.892 (rec:0.016, round:411.876)	b=11.00	count=12000
Total loss:	331.575 (rec:0.015, round:331.560)	b=10.44	count=12500
Total loss:	267.344 (rec:0.016, round:267.328)	b=9.88	count=13000
Total loss:	215.471 (rec:0.016, round:215.455)	b=9.31	count=13500
Total loss:	162.926 (rec:0.015, round:162.911)	b=8.75	count=14000
Total loss:	118.679 (rec:0.014, round:118.665)	b=8.19	count=14500
Total loss:	85.127 (rec:0.017, round:85.109)	b=7.62	count=15000
Total loss:	56.762 (rec:0.017, round:56.745)	b=7.06	count=15500
Total loss:	35.919 (rec:0.017, round:35.902)	b=6.50	count=16000
Total loss:	20.485 (rec:0.015, round:20.470)	b=5.94	count=16500
Total loss:	8.671 (rec:0.016, round:8.656)	b=5.38	count=17000
Total loss:	4.315 (rec:0.016, round:4.299)	b=4.81	count=17500
Total loss:	0.783 (rec:0.014, round:0.769)	b=4.25	count=18000
Total loss:	0.016 (rec:0.016, round:0.000)	b=3.69	count=18500
Total loss:	0.015 (rec:0.015, round:0.000)	b=3.12	count=19000
Total loss:	0.017 (rec:0.017, round:0.000)	b=2.56	count=19500
Total loss:	0.016 (rec:0.016, round:0.000)	b=2.00	count=20000
Reconstruction for block 2
Init alpha to be FP32
Total loss:	0.023 (rec:0.023, round:0.000)	b=0.00	count=500
Total loss:	0.025 (rec:0.025, round:0.000)	b=0.00	count=1000
Total loss:	0.020 (rec:0.020, round:0.000)	b=0.00	count=1500
Total loss:	0.022 (rec:0.022, round:0.000)	b=0.00	count=2000
Total loss:	0.021 (rec:0.021, round:0.000)	b=0.00	count=2500
Total loss:	0.021 (rec:0.021, round:0.000)	b=0.00	count=3000
Total loss:	0.023 (rec:0.023, round:0.000)	b=0.00	count=3500
Total loss:	7169.517 (rec:0.020, round:7169.497)	b=20.00	count=4000
Total loss:	2837.952 (rec:0.023, round:2837.929)	b=19.44	count=4500
Total loss:	2475.788 (rec:0.022, round:2475.766)	b=18.88	count=5000
Total loss:	2243.917 (rec:0.022, round:2243.896)	b=18.31	count=5500
Total loss:	2062.585 (rec:0.022, round:2062.562)	b=17.75	count=6000
Total loss:	1899.223 (rec:0.023, round:1899.200)	b=17.19	count=6500
Total loss:	1759.736 (rec:0.022, round:1759.714)	b=16.62	count=7000
Total loss:	1640.157 (rec:0.023, round:1640.134)	b=16.06	count=7500
Total loss:	1510.207 (rec:0.022, round:1510.184)	b=15.50	count=8000
Total loss:	1395.125 (rec:0.021, round:1395.104)	b=14.94	count=8500
Total loss:	1270.812 (rec:0.022, round:1270.790)	b=14.38	count=9000
Total loss:	1156.053 (rec:0.024, round:1156.029)	b=13.81	count=9500
Total loss:	1027.083 (rec:0.022, round:1027.061)	b=13.25	count=10000
Total loss:	911.501 (rec:0.025, round:911.477)	b=12.69	count=10500
Total loss:	789.788 (rec:0.022, round:789.767)	b=12.12	count=11000
Total loss:	670.684 (rec:0.022, round:670.661)	b=11.56	count=11500
Total loss:	569.936 (rec:0.023, round:569.913)	b=11.00	count=12000
Total loss:	475.338 (rec:0.023, round:475.315)	b=10.44	count=12500
Total loss:	384.074 (rec:0.027, round:384.047)	b=9.88	count=13000
Total loss:	303.499 (rec:0.025, round:303.473)	b=9.31	count=13500
Total loss:	232.397 (rec:0.025, round:232.373)	b=8.75	count=14000
Total loss:	166.865 (rec:0.021, round:166.844)	b=8.19	count=14500
Total loss:	112.109 (rec:0.024, round:112.085)	b=7.62	count=15000
Total loss:	74.398 (rec:0.024, round:74.375)	b=7.06	count=15500
Total loss:	43.949 (rec:0.024, round:43.925)	b=6.50	count=16000
Total loss:	21.258 (rec:0.024, round:21.234)	b=5.94	count=16500
Total loss:	8.884 (rec:0.024, round:8.861)	b=5.38	count=17000
Total loss:	3.148 (rec:0.022, round:3.126)	b=4.81	count=17500
Total loss:	0.742 (rec:0.023, round:0.719)	b=4.25	count=18000
Total loss:	0.023 (rec:0.023, round:0.000)	b=3.69	count=18500
Total loss:	0.025 (rec:0.025, round:0.000)	b=3.12	count=19000
Total loss:	0.023 (rec:0.023, round:0.000)	b=2.56	count=19500
Total loss:	0.024 (rec:0.024, round:0.000)	b=2.00	count=20000
Reconstruction for block 0
Init alpha to be FP32
Total loss:	0.018 (rec:0.018, round:0.000)	b=0.00	count=500
Total loss:	0.016 (rec:0.016, round:0.000)	b=0.00	count=1000
Total loss:	0.018 (rec:0.018, round:0.000)	b=0.00	count=1500
Total loss:	0.016 (rec:0.016, round:0.000)	b=0.00	count=2000
Total loss:	0.017 (rec:0.017, round:0.000)	b=0.00	count=2500
Total loss:	0.016 (rec:0.016, round:0.000)	b=0.00	count=3000
Total loss:	0.017 (rec:0.017, round:0.000)	b=0.00	count=3500
Total loss:	7357.035 (rec:0.017, round:7357.018)	b=20.00	count=4000
Total loss:	2974.335 (rec:0.018, round:2974.316)	b=19.44	count=4500
Total loss:	2569.363 (rec:0.018, round:2569.344)	b=18.88	count=5000
Total loss:	2301.739 (rec:0.019, round:2301.720)	b=18.31	count=5500
Total loss:	2099.272 (rec:0.020, round:2099.252)	b=17.75	count=6000
Total loss:	1915.485 (rec:0.019, round:1915.466)	b=17.19	count=6500
Total loss:	1748.068 (rec:0.020, round:1748.048)	b=16.62	count=7000
Total loss:	1602.362 (rec:0.020, round:1602.342)	b=16.06	count=7500
Total loss:	1474.539 (rec:0.020, round:1474.520)	b=15.50	count=8000
Total loss:	1344.248 (rec:0.019, round:1344.228)	b=14.94	count=8500
Total loss:	1214.735 (rec:0.018, round:1214.717)	b=14.38	count=9000
Total loss:	1108.305 (rec:0.020, round:1108.285)	b=13.81	count=9500
Total loss:	991.432 (rec:0.019, round:991.412)	b=13.25	count=10000
Total loss:	878.766 (rec:0.020, round:878.746)	b=12.69	count=10500
Total loss:	767.882 (rec:0.020, round:767.862)	b=12.12	count=11000
Total loss:	662.855 (rec:0.019, round:662.836)	b=11.56	count=11500
Total loss:	554.566 (rec:0.021, round:554.545)	b=11.00	count=12000
Total loss:	446.813 (rec:0.021, round:446.792)	b=10.44	count=12500
Total loss:	353.631 (rec:0.020, round:353.611)	b=9.88	count=13000
Total loss:	270.986 (rec:0.020, round:270.966)	b=9.31	count=13500
Total loss:	207.145 (rec:0.021, round:207.124)	b=8.75	count=14000
Total loss:	150.268 (rec:0.021, round:150.247)	b=8.19	count=14500
Total loss:	96.433 (rec:0.022, round:96.411)	b=7.62	count=15000
Total loss:	61.656 (rec:0.021, round:61.635)	b=7.06	count=15500
Total loss:	36.704 (rec:0.021, round:36.683)	b=6.50	count=16000
Total loss:	20.637 (rec:0.023, round:20.614)	b=5.94	count=16500
Total loss:	11.015 (rec:0.022, round:10.993)	b=5.38	count=17000
Total loss:	5.469 (rec:0.022, round:5.448)	b=4.81	count=17500
Total loss:	1.729 (rec:0.021, round:1.708)	b=4.25	count=18000
Total loss:	0.940 (rec:0.023, round:0.917)	b=3.69	count=18500
Total loss:	0.091 (rec:0.022, round:0.068)	b=3.12	count=19000
Total loss:	0.020 (rec:0.020, round:0.000)	b=2.56	count=19500
Total loss:	0.023 (rec:0.023, round:0.000)	b=2.00	count=20000
Reconstruction for block context_module
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.021 (rec:0.021, round:0.000)	b=0.00	count=500
Total loss:	0.021 (rec:0.021, round:0.000)	b=0.00	count=1000
Total loss:	0.021 (rec:0.021, round:0.000)	b=0.00	count=1500
Total loss:	0.020 (rec:0.020, round:0.000)	b=0.00	count=2000
Total loss:	0.020 (rec:0.020, round:0.000)	b=0.00	count=2500
Total loss:	0.021 (rec:0.021, round:0.000)	b=0.00	count=3000
Total loss:	0.021 (rec:0.021, round:0.000)	b=0.00	count=3500
Total loss:	45160.445 (rec:0.021, round:45160.426)	b=20.00	count=4000
Total loss:	15651.974 (rec:0.022, round:15651.951)	b=19.44	count=4500
Total loss:	13954.812 (rec:0.020, round:13954.792)	b=18.88	count=5000
Total loss:	12847.684 (rec:0.020, round:12847.663)	b=18.31	count=5500
Total loss:	11946.096 (rec:0.021, round:11946.074)	b=17.75	count=6000
Total loss:	11073.714 (rec:0.020, round:11073.693)	b=17.19	count=6500
Total loss:	10275.028 (rec:0.021, round:10275.007)	b=16.62	count=7000
Total loss:	9465.745 (rec:0.020, round:9465.726)	b=16.06	count=7500
Total loss:	8675.764 (rec:0.020, round:8675.744)	b=15.50	count=8000
Total loss:	7899.343 (rec:0.021, round:7899.322)	b=14.94	count=8500
Total loss:	7164.330 (rec:0.021, round:7164.309)	b=14.38	count=9000
Total loss:	6418.750 (rec:0.021, round:6418.729)	b=13.81	count=9500
Total loss:	5685.935 (rec:0.020, round:5685.915)	b=13.25	count=10000
Total loss:	4945.680 (rec:0.021, round:4945.659)	b=12.69	count=10500
Total loss:	4261.439 (rec:0.020, round:4261.419)	b=12.12	count=11000
Total loss:	3571.440 (rec:0.021, round:3571.419)	b=11.56	count=11500
Total loss:	2941.060 (rec:0.021, round:2941.039)	b=11.00	count=12000
Total loss:	2341.555 (rec:0.022, round:2341.533)	b=10.44	count=12500
Total loss:	1787.624 (rec:0.022, round:1787.602)	b=9.88	count=13000
Total loss:	1277.484 (rec:0.022, round:1277.462)	b=9.31	count=13500
Total loss:	864.765 (rec:0.021, round:864.745)	b=8.75	count=14000
Total loss:	558.875 (rec:0.022, round:558.853)	b=8.19	count=14500
Total loss:	368.322 (rec:0.022, round:368.300)	b=7.62	count=15000
Total loss:	242.486 (rec:0.021, round:242.465)	b=7.06	count=15500
Total loss:	152.639 (rec:0.023, round:152.617)	b=6.50	count=16000
Total loss:	88.156 (rec:0.021, round:88.134)	b=5.94	count=16500
Total loss:	48.764 (rec:0.022, round:48.742)	b=5.38	count=17000
Total loss:	20.338 (rec:0.021, round:20.317)	b=4.81	count=17500
Total loss:	6.674 (rec:0.024, round:6.650)	b=4.25	count=18000
Total loss:	1.579 (rec:0.021, round:1.558)	b=3.69	count=18500
Total loss:	0.290 (rec:0.021, round:0.269)	b=3.12	count=19000
Total loss:	0.022 (rec:0.022, round:0.000)	b=2.56	count=19500
Total loss:	0.021 (rec:0.021, round:0.000)	b=2.00	count=20000
Reconstruction for block local_module
Init alpha to be FP32
Total loss:	0.032 (rec:0.032, round:0.000)	b=0.00	count=500
Total loss:	0.031 (rec:0.031, round:0.000)	b=0.00	count=1000
Total loss:	0.032 (rec:0.032, round:0.000)	b=0.00	count=1500
Total loss:	0.033 (rec:0.033, round:0.000)	b=0.00	count=2000
Total loss:	0.031 (rec:0.031, round:0.000)	b=0.00	count=2500
Total loss:	0.032 (rec:0.032, round:0.000)	b=0.00	count=3000
Total loss:	0.031 (rec:0.031, round:0.000)	b=0.00	count=3500
Total loss:	28703.832 (rec:0.031, round:28703.801)	b=20.00	count=4000
Total loss:	10581.500 (rec:0.032, round:10581.468)	b=19.44	count=4500
Total loss:	9391.862 (rec:0.032, round:9391.830)	b=18.88	count=5000
Total loss:	8534.628 (rec:0.034, round:8534.594)	b=18.31	count=5500
Total loss:	7825.528 (rec:0.031, round:7825.498)	b=17.75	count=6000
Total loss:	7164.350 (rec:0.032, round:7164.318)	b=17.19	count=6500
Total loss:	6535.949 (rec:0.033, round:6535.916)	b=16.62	count=7000
Total loss:	5989.368 (rec:0.033, round:5989.335)	b=16.06	count=7500
Total loss:	5442.255 (rec:0.031, round:5442.224)	b=15.50	count=8000
Total loss:	4922.028 (rec:0.032, round:4921.996)	b=14.94	count=8500
Total loss:	4432.247 (rec:0.032, round:4432.214)	b=14.38	count=9000
Total loss:	3987.092 (rec:0.032, round:3987.059)	b=13.81	count=9500
Total loss:	3558.703 (rec:0.032, round:3558.671)	b=13.25	count=10000
Total loss:	3161.588 (rec:0.031, round:3161.556)	b=12.69	count=10500
Total loss:	2775.305 (rec:0.032, round:2775.273)	b=12.12	count=11000
Total loss:	2409.126 (rec:0.034, round:2409.092)	b=11.56	count=11500
Total loss:	2070.863 (rec:0.033, round:2070.830)	b=11.00	count=12000
Total loss:	1738.212 (rec:0.030, round:1738.181)	b=10.44	count=12500
Total loss:	1419.382 (rec:0.033, round:1419.349)	b=9.88	count=13000
Total loss:	1131.409 (rec:0.033, round:1131.376)	b=9.31	count=13500
Total loss:	867.459 (rec:0.032, round:867.427)	b=8.75	count=14000
Total loss:	637.634 (rec:0.033, round:637.602)	b=8.19	count=14500
Total loss:	444.655 (rec:0.033, round:444.622)	b=7.62	count=15000
Total loss:	273.676 (rec:0.033, round:273.642)	b=7.06	count=15500
Total loss:	156.886 (rec:0.034, round:156.852)	b=6.50	count=16000
Total loss:	72.717 (rec:0.032, round:72.685)	b=5.94	count=16500
Total loss:	25.892 (rec:0.034, round:25.858)	b=5.38	count=17000
Total loss:	5.018 (rec:0.033, round:4.984)	b=4.81	count=17500
Total loss:	0.792 (rec:0.032, round:0.760)	b=4.25	count=18000
Total loss:	0.034 (rec:0.034, round:0.000)	b=3.69	count=18500
Total loss:	0.032 (rec:0.032, round:0.000)	b=3.12	count=19000
Total loss:	0.033 (rec:0.033, round:0.000)	b=2.56	count=19500
Total loss:	0.034 (rec:0.034, round:0.000)	b=2.00	count=20000
Reconstruction for block context_module
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.034 (rec:0.034, round:0.000)	b=0.00	count=500
Total loss:	0.035 (rec:0.035, round:0.000)	b=0.00	count=1000
Total loss:	0.034 (rec:0.034, round:0.000)	b=0.00	count=1500
Total loss:	0.034 (rec:0.034, round:0.000)	b=0.00	count=2000
Total loss:	0.034 (rec:0.034, round:0.000)	b=0.00	count=2500
Total loss:	0.034 (rec:0.034, round:0.000)	b=0.00	count=3000
Total loss:	0.036 (rec:0.036, round:0.000)	b=0.00	count=3500
Total loss:	45340.445 (rec:0.035, round:45340.410)	b=20.00	count=4000
Total loss:	16787.047 (rec:0.035, round:16787.012)	b=19.44	count=4500
Total loss:	15038.098 (rec:0.034, round:15038.064)	b=18.88	count=5000
Total loss:	13865.741 (rec:0.035, round:13865.707)	b=18.31	count=5500
Total loss:	12920.996 (rec:0.035, round:12920.961)	b=17.75	count=6000
Total loss:	11981.106 (rec:0.035, round:11981.071)	b=17.19	count=6500
Total loss:	11095.476 (rec:0.037, round:11095.438)	b=16.62	count=7000
Total loss:	10223.032 (rec:0.034, round:10222.998)	b=16.06	count=7500
Total loss:	9402.603 (rec:0.034, round:9402.568)	b=15.50	count=8000
Total loss:	8624.954 (rec:0.036, round:8624.918)	b=14.94	count=8500
Total loss:	7831.017 (rec:0.035, round:7830.981)	b=14.38	count=9000
Total loss:	7054.357 (rec:0.035, round:7054.322)	b=13.81	count=9500
Total loss:	6266.747 (rec:0.034, round:6266.713)	b=13.25	count=10000
Total loss:	5458.275 (rec:0.036, round:5458.239)	b=12.69	count=10500
Total loss:	4752.356 (rec:0.035, round:4752.321)	b=12.12	count=11000
Total loss:	4060.383 (rec:0.036, round:4060.347)	b=11.56	count=11500
Total loss:	3353.569 (rec:0.035, round:3353.534)	b=11.00	count=12000
Total loss:	2708.331 (rec:0.034, round:2708.297)	b=10.44	count=12500
Total loss:	2129.242 (rec:0.037, round:2129.205)	b=9.88	count=13000
Total loss:	1603.202 (rec:0.037, round:1603.166)	b=9.31	count=13500
Total loss:	1119.743 (rec:0.034, round:1119.708)	b=8.75	count=14000
Total loss:	715.490 (rec:0.035, round:715.455)	b=8.19	count=14500
Total loss:	387.503 (rec:0.035, round:387.468)	b=7.62	count=15000
Total loss:	201.271 (rec:0.035, round:201.235)	b=7.06	count=15500
Total loss:	93.724 (rec:0.038, round:93.686)	b=6.50	count=16000
Total loss:	47.551 (rec:0.036, round:47.514)	b=5.94	count=16500
Total loss:	21.417 (rec:0.035, round:21.382)	b=5.38	count=17000
Total loss:	11.601 (rec:0.037, round:11.565)	b=4.81	count=17500
Total loss:	4.522 (rec:0.036, round:4.485)	b=4.25	count=18000
Total loss:	0.953 (rec:0.036, round:0.918)	b=3.69	count=18500
Total loss:	0.292 (rec:0.037, round:0.256)	b=3.12	count=19000
Total loss:	0.038 (rec:0.038, round:0.000)	b=2.56	count=19500
Total loss:	0.036 (rec:0.036, round:0.000)	b=2.00	count=20000
Reconstruction for block local_module
Init alpha to be FP32
Total loss:	0.046 (rec:0.046, round:0.000)	b=0.00	count=500
Total loss:	0.047 (rec:0.047, round:0.000)	b=0.00	count=1000
Total loss:	0.046 (rec:0.046, round:0.000)	b=0.00	count=1500
Total loss:	0.046 (rec:0.046, round:0.000)	b=0.00	count=2000
Total loss:	0.044 (rec:0.044, round:0.000)	b=0.00	count=2500
Total loss:	0.047 (rec:0.047, round:0.000)	b=0.00	count=3000
Total loss:	0.048 (rec:0.048, round:0.000)	b=0.00	count=3500
Total loss:	29741.531 (rec:0.049, round:29741.482)	b=20.00	count=4000
Total loss:	11347.814 (rec:0.046, round:11347.769)	b=19.44	count=4500
Total loss:	10197.032 (rec:0.046, round:10196.986)	b=18.88	count=5000
Total loss:	9391.701 (rec:0.045, round:9391.656)	b=18.31	count=5500
Total loss:	8709.828 (rec:0.049, round:8709.779)	b=17.75	count=6000
Total loss:	8097.802 (rec:0.048, round:8097.754)	b=17.19	count=6500
Total loss:	7538.935 (rec:0.045, round:7538.890)	b=16.62	count=7000
Total loss:	7024.037 (rec:0.048, round:7023.989)	b=16.06	count=7500
Total loss:	6492.480 (rec:0.046, round:6492.435)	b=15.50	count=8000
Total loss:	5976.896 (rec:0.047, round:5976.849)	b=14.94	count=8500
Total loss:	5486.837 (rec:0.046, round:5486.791)	b=14.38	count=9000
Total loss:	5010.205 (rec:0.046, round:5010.158)	b=13.81	count=9500
Total loss:	4545.392 (rec:0.049, round:4545.343)	b=13.25	count=10000
Total loss:	4083.408 (rec:0.043, round:4083.365)	b=12.69	count=10500
Total loss:	3642.087 (rec:0.046, round:3642.042)	b=12.12	count=11000
Total loss:	3188.138 (rec:0.049, round:3188.089)	b=11.56	count=11500
Total loss:	2772.539 (rec:0.048, round:2772.490)	b=11.00	count=12000
Total loss:	2357.466 (rec:0.046, round:2357.420)	b=10.44	count=12500
Total loss:	1940.570 (rec:0.048, round:1940.522)	b=9.88	count=13000
Total loss:	1552.482 (rec:0.047, round:1552.435)	b=9.31	count=13500
Total loss:	1196.554 (rec:0.045, round:1196.509)	b=8.75	count=14000
Total loss:	881.994 (rec:0.047, round:881.946)	b=8.19	count=14500
Total loss:	603.677 (rec:0.046, round:603.631)	b=7.62	count=15000
Total loss:	396.395 (rec:0.044, round:396.351)	b=7.06	count=15500
Total loss:	236.506 (rec:0.045, round:236.461)	b=6.50	count=16000
Total loss:	109.744 (rec:0.046, round:109.698)	b=5.94	count=16500
Total loss:	44.803 (rec:0.046, round:44.757)	b=5.38	count=17000
Total loss:	12.353 (rec:0.051, round:12.302)	b=4.81	count=17500
Total loss:	0.856 (rec:0.045, round:0.811)	b=4.25	count=18000
Total loss:	0.046 (rec:0.046, round:0.000)	b=3.69	count=18500
Total loss:	0.049 (rec:0.049, round:0.000)	b=3.12	count=19000
Total loss:	0.047 (rec:0.047, round:0.000)	b=2.56	count=19500
Total loss:	0.049 (rec:0.049, round:0.000)	b=2.00	count=20000
Reconstruction for block context_module
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.048 (rec:0.048, round:0.000)	b=0.00	count=500
Total loss:	0.049 (rec:0.049, round:0.000)	b=0.00	count=1000
Total loss:	0.047 (rec:0.047, round:0.000)	b=0.00	count=1500
Total loss:	0.051 (rec:0.051, round:0.000)	b=0.00	count=2000
Total loss:	0.048 (rec:0.048, round:0.000)	b=0.00	count=2500
Total loss:	0.050 (rec:0.050, round:0.000)	b=0.00	count=3000
Total loss:	0.045 (rec:0.045, round:0.000)	b=0.00	count=3500
Total loss:	45457.465 (rec:0.052, round:45457.414)	b=20.00	count=4000
Total loss:	16979.334 (rec:0.047, round:16979.287)	b=19.44	count=4500
Total loss:	15293.288 (rec:0.046, round:15293.242)	b=18.88	count=5000
Total loss:	14170.423 (rec:0.050, round:14170.372)	b=18.31	count=5500
Total loss:	13147.705 (rec:0.051, round:13147.653)	b=17.75	count=6000
Total loss:	12253.924 (rec:0.048, round:12253.876)	b=17.19	count=6500
Total loss:	11388.578 (rec:0.049, round:11388.528)	b=16.62	count=7000
Total loss:	10518.361 (rec:0.047, round:10518.314)	b=16.06	count=7500
Total loss:	9685.280 (rec:0.049, round:9685.231)	b=15.50	count=8000
Total loss:	8906.508 (rec:0.049, round:8906.459)	b=14.94	count=8500
Total loss:	8108.790 (rec:0.051, round:8108.739)	b=14.38	count=9000
Total loss:	7348.723 (rec:0.047, round:7348.676)	b=13.81	count=9500
Total loss:	6581.241 (rec:0.049, round:6581.192)	b=13.25	count=10000
Total loss:	5839.588 (rec:0.052, round:5839.536)	b=12.69	count=10500
Total loss:	5107.388 (rec:0.049, round:5107.339)	b=12.12	count=11000
Total loss:	4405.522 (rec:0.047, round:4405.476)	b=11.56	count=11500
Total loss:	3722.803 (rec:0.047, round:3722.755)	b=11.00	count=12000
Total loss:	3085.375 (rec:0.048, round:3085.327)	b=10.44	count=12500
Total loss:	2489.642 (rec:0.048, round:2489.594)	b=9.88	count=13000
Total loss:	1960.859 (rec:0.046, round:1960.813)	b=9.31	count=13500
Total loss:	1431.638 (rec:0.048, round:1431.589)	b=8.75	count=14000
Total loss:	975.650 (rec:0.049, round:975.601)	b=8.19	count=14500
Total loss:	579.610 (rec:0.049, round:579.561)	b=7.62	count=15000
Total loss:	270.729 (rec:0.050, round:270.678)	b=7.06	count=15500
Total loss:	121.550 (rec:0.049, round:121.501)	b=6.50	count=16000
Total loss:	50.011 (rec:0.048, round:49.963)	b=5.94	count=16500
Total loss:	22.025 (rec:0.048, round:21.977)	b=5.38	count=17000
Total loss:	5.728 (rec:0.050, round:5.678)	b=4.81	count=17500
Total loss:	1.482 (rec:0.050, round:1.432)	b=4.25	count=18000
Total loss:	0.314 (rec:0.050, round:0.264)	b=3.69	count=18500
Total loss:	0.049 (rec:0.049, round:0.000)	b=3.12	count=19000
Total loss:	0.049 (rec:0.049, round:0.000)	b=2.56	count=19500
Total loss:	0.045 (rec:0.045, round:0.000)	b=2.00	count=20000
Reconstruction for block local_module
Init alpha to be FP32
Total loss:	0.062 (rec:0.062, round:0.000)	b=0.00	count=500
Total loss:	0.058 (rec:0.058, round:0.000)	b=0.00	count=1000
Total loss:	0.056 (rec:0.056, round:0.000)	b=0.00	count=1500
Total loss:	0.064 (rec:0.064, round:0.000)	b=0.00	count=2000
Total loss:	0.059 (rec:0.059, round:0.000)	b=0.00	count=2500
Total loss:	0.055 (rec:0.055, round:0.000)	b=0.00	count=3000
Total loss:	0.057 (rec:0.057, round:0.000)	b=0.00	count=3500
Total loss:	29811.000 (rec:0.063, round:29810.938)	b=20.00	count=4000
Total loss:	11592.081 (rec:0.063, round:11592.019)	b=19.44	count=4500
Total loss:	10446.991 (rec:0.062, round:10446.929)	b=18.88	count=5000
Total loss:	9684.575 (rec:0.059, round:9684.517)	b=18.31	count=5500
Total loss:	9009.892 (rec:0.057, round:9009.834)	b=17.75	count=6000
Total loss:	8409.793 (rec:0.061, round:8409.732)	b=17.19	count=6500
Total loss:	7857.731 (rec:0.062, round:7857.669)	b=16.62	count=7000
Total loss:	7347.828 (rec:0.059, round:7347.769)	b=16.06	count=7500
Total loss:	6847.508 (rec:0.063, round:6847.445)	b=15.50	count=8000
Total loss:	6346.474 (rec:0.062, round:6346.412)	b=14.94	count=8500
Total loss:	5856.717 (rec:0.062, round:5856.656)	b=14.38	count=9000
Total loss:	5354.457 (rec:0.060, round:5354.397)	b=13.81	count=9500
Total loss:	4890.022 (rec:0.059, round:4889.963)	b=13.25	count=10000
Total loss:	4432.748 (rec:0.061, round:4432.688)	b=12.69	count=10500
Total loss:	3959.317 (rec:0.067, round:3959.250)	b=12.12	count=11000
Total loss:	3518.164 (rec:0.059, round:3518.104)	b=11.56	count=11500
Total loss:	3068.079 (rec:0.060, round:3068.019)	b=11.00	count=12000
Total loss:	2623.358 (rec:0.062, round:2623.296)	b=10.44	count=12500
Total loss:	2206.834 (rec:0.062, round:2206.772)	b=9.88	count=13000
Total loss:	1793.028 (rec:0.058, round:1792.969)	b=9.31	count=13500
Total loss:	1409.116 (rec:0.061, round:1409.055)	b=8.75	count=14000
Total loss:	1081.968 (rec:0.059, round:1081.909)	b=8.19	count=14500
Total loss:	781.046 (rec:0.060, round:780.986)	b=7.62	count=15000
Total loss:	512.983 (rec:0.063, round:512.920)	b=7.06	count=15500
Total loss:	295.262 (rec:0.063, round:295.199)	b=6.50	count=16000
Total loss:	150.285 (rec:0.063, round:150.222)	b=5.94	count=16500
Total loss:	57.940 (rec:0.061, round:57.880)	b=5.38	count=17000
Total loss:	19.404 (rec:0.060, round:19.345)	b=4.81	count=17500
Total loss:	4.992 (rec:0.065, round:4.928)	b=4.25	count=18000
Total loss:	0.264 (rec:0.061, round:0.203)	b=3.69	count=18500
Total loss:	0.062 (rec:0.062, round:0.000)	b=3.12	count=19000
Total loss:	0.060 (rec:0.060, round:0.000)	b=2.56	count=19500
Total loss:	0.059 (rec:0.059, round:0.000)	b=2.00	count=20000
Reconstruction for block 0
Init alpha to be FP32
Total loss:	0.039 (rec:0.039, round:0.000)	b=0.00	count=500
Total loss:	0.039 (rec:0.039, round:0.000)	b=0.00	count=1000
Total loss:	0.040 (rec:0.040, round:0.000)	b=0.00	count=1500
Total loss:	0.040 (rec:0.040, round:0.000)	b=0.00	count=2000
Total loss:	0.040 (rec:0.040, round:0.000)	b=0.00	count=2500
Total loss:	0.041 (rec:0.041, round:0.000)	b=0.00	count=3000
Total loss:	0.039 (rec:0.039, round:0.000)	b=0.00	count=3500
Total loss:	30211.027 (rec:0.041, round:30210.986)	b=20.00	count=4000
Total loss:	12199.163 (rec:0.040, round:12199.123)	b=19.44	count=4500
Total loss:	10988.462 (rec:0.041, round:10988.421)	b=18.88	count=5000
Total loss:	10158.164 (rec:0.042, round:10158.122)	b=18.31	count=5500
Total loss:	9485.626 (rec:0.044, round:9485.582)	b=17.75	count=6000
Total loss:	8883.331 (rec:0.040, round:8883.291)	b=17.19	count=6500
Total loss:	8354.679 (rec:0.042, round:8354.637)	b=16.62	count=7000
Total loss:	7849.991 (rec:0.041, round:7849.951)	b=16.06	count=7500
Total loss:	7337.100 (rec:0.042, round:7337.058)	b=15.50	count=8000
Total loss:	6837.032 (rec:0.042, round:6836.990)	b=14.94	count=8500
Total loss:	6356.525 (rec:0.043, round:6356.482)	b=14.38	count=9000
Total loss:	5879.267 (rec:0.045, round:5879.222)	b=13.81	count=9500
Total loss:	5376.371 (rec:0.043, round:5376.328)	b=13.25	count=10000
Total loss:	4907.844 (rec:0.042, round:4907.802)	b=12.69	count=10500
Total loss:	4412.278 (rec:0.044, round:4412.234)	b=12.12	count=11000
Total loss:	3908.820 (rec:0.042, round:3908.777)	b=11.56	count=11500
Total loss:	3424.075 (rec:0.044, round:3424.031)	b=11.00	count=12000
Total loss:	2947.990 (rec:0.043, round:2947.947)	b=10.44	count=12500
Total loss:	2489.666 (rec:0.046, round:2489.620)	b=9.88	count=13000
Total loss:	2036.374 (rec:0.045, round:2036.329)	b=9.31	count=13500
Total loss:	1598.082 (rec:0.044, round:1598.038)	b=8.75	count=14000
Total loss:	1203.643 (rec:0.046, round:1203.598)	b=8.19	count=14500
Total loss:	838.792 (rec:0.044, round:838.748)	b=7.62	count=15000
Total loss:	547.890 (rec:0.046, round:547.845)	b=7.06	count=15500
Total loss:	322.970 (rec:0.045, round:322.924)	b=6.50	count=16000
Total loss:	163.357 (rec:0.044, round:163.313)	b=5.94	count=16500
Total loss:	63.153 (rec:0.046, round:63.107)	b=5.38	count=17000
Total loss:	23.637 (rec:0.047, round:23.590)	b=4.81	count=17500
Total loss:	3.779 (rec:0.045, round:3.734)	b=4.25	count=18000
Total loss:	0.140 (rec:0.045, round:0.094)	b=3.69	count=18500
Total loss:	0.047 (rec:0.047, round:0.000)	b=3.12	count=19000
Total loss:	0.044 (rec:0.044, round:0.000)	b=2.56	count=19500
Total loss:	0.045 (rec:0.045, round:0.000)	b=2.00	count=20000
Reconstruction for block context_module
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.047 (rec:0.047, round:0.000)	b=0.00	count=500
Total loss:	0.047 (rec:0.047, round:0.000)	b=0.00	count=1000
Total loss:	0.046 (rec:0.046, round:0.000)	b=0.00	count=1500
Total loss:	0.049 (rec:0.049, round:0.000)	b=0.00	count=2000
Total loss:	0.048 (rec:0.048, round:0.000)	b=0.00	count=2500
Total loss:	0.047 (rec:0.047, round:0.000)	b=0.00	count=3000
Total loss:	0.047 (rec:0.047, round:0.000)	b=0.00	count=3500
Total loss:	167296.953 (rec:0.048, round:167296.906)	b=20.00	count=4000
Total loss:	59354.191 (rec:0.048, round:59354.145)	b=19.44	count=4500
Total loss:	53799.504 (rec:0.047, round:53799.457)	b=18.88	count=5000
Total loss:	50041.234 (rec:0.046, round:50041.188)	b=18.31	count=5500
Total loss:	46774.867 (rec:0.048, round:46774.820)	b=17.75	count=6000
Total loss:	43703.695 (rec:0.048, round:43703.648)	b=17.19	count=6500
Total loss:	40671.117 (rec:0.047, round:40671.070)	b=16.62	count=7000
Total loss:	37744.488 (rec:0.045, round:37744.441)	b=16.06	count=7500
Total loss:	34822.113 (rec:0.046, round:34822.066)	b=15.50	count=8000
Total loss:	31967.539 (rec:0.049, round:31967.490)	b=14.94	count=8500
Total loss:	29163.652 (rec:0.046, round:29163.605)	b=14.38	count=9000
Total loss:	26462.518 (rec:0.049, round:26462.469)	b=13.81	count=9500
Total loss:	23777.307 (rec:0.049, round:23777.258)	b=13.25	count=10000
Total loss:	21154.861 (rec:0.050, round:21154.812)	b=12.69	count=10500
Total loss:	18632.562 (rec:0.048, round:18632.516)	b=12.12	count=11000
Total loss:	16110.404 (rec:0.047, round:16110.357)	b=11.56	count=11500
Total loss:	13727.255 (rec:0.047, round:13727.208)	b=11.00	count=12000
Total loss:	11499.048 (rec:0.047, round:11499.001)	b=10.44	count=12500
Total loss:	9321.746 (rec:0.049, round:9321.697)	b=9.88	count=13000
Total loss:	7248.296 (rec:0.046, round:7248.250)	b=9.31	count=13500
Total loss:	5324.641 (rec:0.047, round:5324.594)	b=8.75	count=14000
Total loss:	3543.338 (rec:0.048, round:3543.290)	b=8.19	count=14500
Total loss:	2071.586 (rec:0.047, round:2071.539)	b=7.62	count=15000
Total loss:	1020.541 (rec:0.047, round:1020.494)	b=7.06	count=15500
Total loss:	442.874 (rec:0.046, round:442.828)	b=6.50	count=16000
Total loss:	178.968 (rec:0.047, round:178.920)	b=5.94	count=16500
Total loss:	54.132 (rec:0.047, round:54.085)	b=5.38	count=17000
Total loss:	14.664 (rec:0.048, round:14.615)	b=4.81	count=17500
Total loss:	4.770 (rec:0.047, round:4.723)	b=4.25	count=18000
Total loss:	0.702 (rec:0.048, round:0.654)	b=3.69	count=18500
Total loss:	0.046 (rec:0.046, round:0.000)	b=3.12	count=19000
Total loss:	0.049 (rec:0.049, round:0.000)	b=2.56	count=19500
Total loss:	0.048 (rec:0.048, round:0.000)	b=2.00	count=20000
Reconstruction for block local_module
Init alpha to be FP32
Total loss:	0.058 (rec:0.058, round:0.000)	b=0.00	count=500
Total loss:	0.060 (rec:0.060, round:0.000)	b=0.00	count=1000
Total loss:	0.057 (rec:0.057, round:0.000)	b=0.00	count=1500
Total loss:	0.061 (rec:0.061, round:0.000)	b=0.00	count=2000
Total loss:	0.061 (rec:0.061, round:0.000)	b=0.00	count=2500
Total loss:	0.061 (rec:0.061, round:0.000)	b=0.00	count=3000
Total loss:	0.057 (rec:0.057, round:0.000)	b=0.00	count=3500
Total loss:	120088.039 (rec:0.057, round:120087.984)	b=20.00	count=4000
Total loss:	44355.328 (rec:0.059, round:44355.270)	b=19.44	count=4500
Total loss:	40293.527 (rec:0.059, round:40293.469)	b=18.88	count=5000
Total loss:	37391.590 (rec:0.058, round:37391.531)	b=18.31	count=5500
Total loss:	34849.020 (rec:0.058, round:34848.961)	b=17.75	count=6000
Total loss:	32496.768 (rec:0.057, round:32496.711)	b=17.19	count=6500
Total loss:	30232.525 (rec:0.057, round:30232.469)	b=16.62	count=7000
Total loss:	28000.574 (rec:0.062, round:28000.512)	b=16.06	count=7500
Total loss:	25832.836 (rec:0.059, round:25832.777)	b=15.50	count=8000
Total loss:	23731.066 (rec:0.058, round:23731.008)	b=14.94	count=8500
Total loss:	21691.848 (rec:0.059, round:21691.789)	b=14.38	count=9000
Total loss:	19744.824 (rec:0.059, round:19744.766)	b=13.81	count=9500
Total loss:	17828.924 (rec:0.062, round:17828.861)	b=13.25	count=10000
Total loss:	16014.259 (rec:0.060, round:16014.199)	b=12.69	count=10500
Total loss:	14201.996 (rec:0.056, round:14201.939)	b=12.12	count=11000
Total loss:	12491.316 (rec:0.058, round:12491.258)	b=11.56	count=11500
Total loss:	10830.857 (rec:0.061, round:10830.797)	b=11.00	count=12000
Total loss:	9258.244 (rec:0.056, round:9258.188)	b=10.44	count=12500
Total loss:	7783.779 (rec:0.058, round:7783.720)	b=9.88	count=13000
Total loss:	6419.188 (rec:0.061, round:6419.128)	b=9.31	count=13500
Total loss:	5138.712 (rec:0.061, round:5138.651)	b=8.75	count=14000
Total loss:	3956.688 (rec:0.060, round:3956.628)	b=8.19	count=14500
Total loss:	2908.159 (rec:0.060, round:2908.099)	b=7.62	count=15000
Total loss:	2003.335 (rec:0.059, round:2003.276)	b=7.06	count=15500
Total loss:	1213.630 (rec:0.057, round:1213.573)	b=6.50	count=16000
Total loss:	631.106 (rec:0.061, round:631.045)	b=5.94	count=16500
Total loss:	249.919 (rec:0.060, round:249.859)	b=5.38	count=17000
Total loss:	67.218 (rec:0.062, round:67.157)	b=4.81	count=17500
Total loss:	7.585 (rec:0.057, round:7.528)	b=4.25	count=18000
Total loss:	0.173 (rec:0.059, round:0.114)	b=3.69	count=18500
Total loss:	0.058 (rec:0.058, round:0.000)	b=3.12	count=19000
Total loss:	0.061 (rec:0.061, round:0.000)	b=2.56	count=19500
Total loss:	0.062 (rec:0.062, round:0.000)	b=2.00	count=20000
Reconstruction for block context_module
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.062 (rec:0.062, round:0.000)	b=0.00	count=500
Total loss:	0.064 (rec:0.064, round:0.000)	b=0.00	count=1000
Total loss:	0.060 (rec:0.060, round:0.000)	b=0.00	count=1500
Total loss:	0.061 (rec:0.061, round:0.000)	b=0.00	count=2000
Total loss:	0.059 (rec:0.059, round:0.000)	b=0.00	count=2500
Total loss:	0.063 (rec:0.063, round:0.000)	b=0.00	count=3000
Total loss:	0.059 (rec:0.059, round:0.000)	b=0.00	count=3500
Total loss:	167051.156 (rec:0.064, round:167051.094)	b=20.00	count=4000
Total loss:	60452.883 (rec:0.062, round:60452.820)	b=19.44	count=4500
Total loss:	54830.598 (rec:0.062, round:54830.535)	b=18.88	count=5000
Total loss:	51014.758 (rec:0.061, round:51014.695)	b=18.31	count=5500
Total loss:	47621.082 (rec:0.064, round:47621.020)	b=17.75	count=6000
Total loss:	44461.523 (rec:0.062, round:44461.461)	b=17.19	count=6500
Total loss:	41347.133 (rec:0.061, round:41347.070)	b=16.62	count=7000
Total loss:	38281.230 (rec:0.060, round:38281.172)	b=16.06	count=7500
Total loss:	35297.609 (rec:0.063, round:35297.547)	b=15.50	count=8000
Total loss:	32317.785 (rec:0.061, round:32317.725)	b=14.94	count=8500
Total loss:	29428.328 (rec:0.060, round:29428.270)	b=14.38	count=9000
Total loss:	26604.119 (rec:0.061, round:26604.059)	b=13.81	count=9500
Total loss:	23930.801 (rec:0.061, round:23930.740)	b=13.25	count=10000
Total loss:	21357.652 (rec:0.062, round:21357.590)	b=12.69	count=10500
Total loss:	18779.947 (rec:0.057, round:18779.891)	b=12.12	count=11000
Total loss:	16297.666 (rec:0.060, round:16297.605)	b=11.56	count=11500
Total loss:	13966.435 (rec:0.062, round:13966.373)	b=11.00	count=12000
Total loss:	11770.097 (rec:0.062, round:11770.035)	b=10.44	count=12500
Total loss:	9735.259 (rec:0.060, round:9735.198)	b=9.88	count=13000
Total loss:	7839.090 (rec:0.062, round:7839.028)	b=9.31	count=13500
Total loss:	6080.389 (rec:0.065, round:6080.324)	b=8.75	count=14000
Total loss:	4406.688 (rec:0.061, round:4406.628)	b=8.19	count=14500
Total loss:	2955.443 (rec:0.063, round:2955.380)	b=7.62	count=15000
Total loss:	1761.145 (rec:0.059, round:1761.085)	b=7.06	count=15500
Total loss:	907.727 (rec:0.060, round:907.667)	b=6.50	count=16000
Total loss:	415.877 (rec:0.059, round:415.818)	b=5.94	count=16500
Total loss:	151.602 (rec:0.064, round:151.538)	b=5.38	count=17000
Total loss:	37.385 (rec:0.061, round:37.324)	b=4.81	count=17500
Total loss:	6.853 (rec:0.065, round:6.789)	b=4.25	count=18000
Total loss:	1.279 (rec:0.063, round:1.215)	b=3.69	count=18500
Total loss:	0.062 (rec:0.059, round:0.003)	b=3.12	count=19000
Total loss:	0.067 (rec:0.067, round:0.000)	b=2.56	count=19500
Total loss:	0.064 (rec:0.064, round:0.000)	b=2.00	count=20000
Reconstruction for block local_module
Init alpha to be FP32
Total loss:	0.073 (rec:0.073, round:0.000)	b=0.00	count=500
Total loss:	0.072 (rec:0.072, round:0.000)	b=0.00	count=1000
Total loss:	0.071 (rec:0.071, round:0.000)	b=0.00	count=1500
Total loss:	0.079 (rec:0.079, round:0.000)	b=0.00	count=2000
Total loss:	0.070 (rec:0.070, round:0.000)	b=0.00	count=2500
Total loss:	0.076 (rec:0.076, round:0.000)	b=0.00	count=3000
Total loss:	0.072 (rec:0.072, round:0.000)	b=0.00	count=3500
Total loss:	120530.727 (rec:0.070, round:120530.656)	b=20.00	count=4000
Total loss:	45506.723 (rec:0.072, round:45506.652)	b=19.44	count=4500
Total loss:	41470.848 (rec:0.074, round:41470.773)	b=18.88	count=5000
Total loss:	38653.043 (rec:0.073, round:38652.969)	b=18.31	count=5500
Total loss:	36188.832 (rec:0.072, round:36188.762)	b=17.75	count=6000
Total loss:	33873.074 (rec:0.074, round:33873.000)	b=17.19	count=6500
Total loss:	31672.092 (rec:0.072, round:31672.020)	b=16.62	count=7000
Total loss:	29502.936 (rec:0.071, round:29502.865)	b=16.06	count=7500
Total loss:	27335.254 (rec:0.071, round:27335.184)	b=15.50	count=8000
Total loss:	25244.750 (rec:0.073, round:25244.678)	b=14.94	count=8500
Total loss:	23178.164 (rec:0.071, round:23178.094)	b=14.38	count=9000
Total loss:	21207.719 (rec:0.074, round:21207.645)	b=13.81	count=9500
Total loss:	19301.670 (rec:0.073, round:19301.598)	b=13.25	count=10000
Total loss:	17412.518 (rec:0.068, round:17412.449)	b=12.69	count=10500
Total loss:	15582.886 (rec:0.070, round:15582.815)	b=12.12	count=11000
Total loss:	13825.610 (rec:0.073, round:13825.538)	b=11.56	count=11500
Total loss:	12133.342 (rec:0.071, round:12133.271)	b=11.00	count=12000
Total loss:	10455.708 (rec:0.073, round:10455.635)	b=10.44	count=12500
Total loss:	8854.638 (rec:0.071, round:8854.566)	b=9.88	count=13000
Total loss:	7339.812 (rec:0.075, round:7339.737)	b=9.31	count=13500
Total loss:	5946.921 (rec:0.074, round:5946.848)	b=8.75	count=14000
Total loss:	4632.207 (rec:0.072, round:4632.134)	b=8.19	count=14500
Total loss:	3392.771 (rec:0.072, round:3392.700)	b=7.62	count=15000
Total loss:	2334.226 (rec:0.072, round:2334.154)	b=7.06	count=15500
Total loss:	1468.544 (rec:0.075, round:1468.469)	b=6.50	count=16000
Total loss:	811.629 (rec:0.071, round:811.558)	b=5.94	count=16500
Total loss:	357.290 (rec:0.074, round:357.216)	b=5.38	count=17000
Total loss:	98.548 (rec:0.076, round:98.472)	b=4.81	count=17500
Total loss:	11.107 (rec:0.078, round:11.029)	b=4.25	count=18000
Total loss:	0.085 (rec:0.076, round:0.010)	b=3.69	count=18500
Total loss:	0.072 (rec:0.072, round:0.000)	b=3.12	count=19000
Total loss:	0.073 (rec:0.073, round:0.000)	b=2.56	count=19500
Total loss:	0.069 (rec:0.069, round:0.000)	b=2.00	count=20000
Reconstruction for block context_module
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.082 (rec:0.082, round:0.000)	b=0.00	count=500
Total loss:	0.077 (rec:0.077, round:0.000)	b=0.00	count=1000
Total loss:	0.073 (rec:0.073, round:0.000)	b=0.00	count=1500
Total loss:	0.076 (rec:0.076, round:0.000)	b=0.00	count=2000
Total loss:	0.073 (rec:0.073, round:0.000)	b=0.00	count=2500
Total loss:	0.072 (rec:0.072, round:0.000)	b=0.00	count=3000
Total loss:	0.073 (rec:0.073, round:0.000)	b=0.00	count=3500
Total loss:	166593.422 (rec:0.073, round:166593.344)	b=20.00	count=4000
Total loss:	60658.934 (rec:0.076, round:60658.855)	b=19.44	count=4500
Total loss:	55062.816 (rec:0.072, round:55062.742)	b=18.88	count=5000
Total loss:	51147.770 (rec:0.075, round:51147.695)	b=18.31	count=5500
Total loss:	47691.309 (rec:0.077, round:47691.230)	b=17.75	count=6000
Total loss:	44429.434 (rec:0.073, round:44429.359)	b=17.19	count=6500
Total loss:	41212.504 (rec:0.078, round:41212.426)	b=16.62	count=7000
Total loss:	38131.582 (rec:0.074, round:38131.508)	b=16.06	count=7500
Total loss:	35072.344 (rec:0.075, round:35072.270)	b=15.50	count=8000
Total loss:	32140.115 (rec:0.071, round:32140.043)	b=14.94	count=8500
Total loss:	29250.047 (rec:0.074, round:29249.973)	b=14.38	count=9000
Total loss:	26474.936 (rec:0.076, round:26474.859)	b=13.81	count=9500
Total loss:	23719.805 (rec:0.074, round:23719.730)	b=13.25	count=10000
Total loss:	21105.346 (rec:0.079, round:21105.266)	b=12.69	count=10500
Total loss:	18613.992 (rec:0.077, round:18613.914)	b=12.12	count=11000
Total loss:	16290.530 (rec:0.076, round:16290.454)	b=11.56	count=11500
Total loss:	14006.348 (rec:0.075, round:14006.273)	b=11.00	count=12000
Total loss:	11766.925 (rec:0.077, round:11766.848)	b=10.44	count=12500
Total loss:	9764.863 (rec:0.074, round:9764.789)	b=9.88	count=13000
Total loss:	7901.050 (rec:0.074, round:7900.976)	b=9.31	count=13500
Total loss:	6189.862 (rec:0.075, round:6189.787)	b=8.75	count=14000
Total loss:	4647.988 (rec:0.074, round:4647.914)	b=8.19	count=14500
Total loss:	3272.095 (rec:0.075, round:3272.020)	b=7.62	count=15000
Total loss:	2092.242 (rec:0.076, round:2092.167)	b=7.06	count=15500
Total loss:	1184.318 (rec:0.076, round:1184.242)	b=6.50	count=16000
Total loss:	568.990 (rec:0.075, round:568.916)	b=5.94	count=16500
Total loss:	209.844 (rec:0.073, round:209.772)	b=5.38	count=17000
Total loss:	52.471 (rec:0.074, round:52.397)	b=4.81	count=17500
Total loss:	7.758 (rec:0.076, round:7.682)	b=4.25	count=18000
Total loss:	0.882 (rec:0.075, round:0.806)	b=3.69	count=18500
Total loss:	0.142 (rec:0.074, round:0.069)	b=3.12	count=19000
Total loss:	0.074 (rec:0.074, round:0.000)	b=2.56	count=19500
Total loss:	0.074 (rec:0.074, round:0.000)	b=2.00	count=20000
Reconstruction for block local_module
Init alpha to be FP32
Total loss:	0.085 (rec:0.085, round:0.000)	b=0.00	count=500
Total loss:	0.087 (rec:0.087, round:0.000)	b=0.00	count=1000
Total loss:	0.082 (rec:0.082, round:0.000)	b=0.00	count=1500
Total loss:	0.091 (rec:0.091, round:0.000)	b=0.00	count=2000
Total loss:	0.086 (rec:0.086, round:0.000)	b=0.00	count=2500
Total loss:	0.086 (rec:0.086, round:0.000)	b=0.00	count=3000
Total loss:	0.085 (rec:0.085, round:0.000)	b=0.00	count=3500
Total loss:	120553.086 (rec:0.082, round:120553.000)	b=20.00	count=4000
Total loss:	45704.324 (rec:0.087, round:45704.238)	b=19.44	count=4500
Total loss:	41692.906 (rec:0.085, round:41692.820)	b=18.88	count=5000
Total loss:	38839.980 (rec:0.086, round:38839.895)	b=18.31	count=5500
Total loss:	36274.453 (rec:0.081, round:36274.371)	b=17.75	count=6000
Total loss:	33879.641 (rec:0.087, round:33879.555)	b=17.19	count=6500
Total loss:	31589.762 (rec:0.086, round:31589.676)	b=16.62	count=7000
Total loss:	29338.346 (rec:0.089, round:29338.258)	b=16.06	count=7500
Total loss:	27116.818 (rec:0.084, round:27116.734)	b=15.50	count=8000
Total loss:	24950.928 (rec:0.085, round:24950.844)	b=14.94	count=8500
Total loss:	22861.982 (rec:0.085, round:22861.898)	b=14.38	count=9000
Total loss:	20855.395 (rec:0.086, round:20855.309)	b=13.81	count=9500
Total loss:	18886.113 (rec:0.088, round:18886.025)	b=13.25	count=10000
Total loss:	16990.064 (rec:0.088, round:16989.977)	b=12.69	count=10500
Total loss:	15105.414 (rec:0.087, round:15105.327)	b=12.12	count=11000
Total loss:	13276.716 (rec:0.087, round:13276.629)	b=11.56	count=11500
Total loss:	11534.966 (rec:0.089, round:11534.876)	b=11.00	count=12000
Total loss:	9921.882 (rec:0.085, round:9921.797)	b=10.44	count=12500
Total loss:	8354.055 (rec:0.085, round:8353.969)	b=9.88	count=13000
Total loss:	6905.509 (rec:0.093, round:6905.416)	b=9.31	count=13500
Total loss:	5542.966 (rec:0.085, round:5542.881)	b=8.75	count=14000
Total loss:	4325.186 (rec:0.087, round:4325.099)	b=8.19	count=14500
Total loss:	3178.925 (rec:0.089, round:3178.836)	b=7.62	count=15000
Total loss:	2180.649 (rec:0.087, round:2180.562)	b=7.06	count=15500
Total loss:	1335.458 (rec:0.086, round:1335.372)	b=6.50	count=16000
Total loss:	712.741 (rec:0.089, round:712.653)	b=5.94	count=16500
Total loss:	306.077 (rec:0.087, round:305.990)	b=5.38	count=17000
Total loss:	87.988 (rec:0.086, round:87.902)	b=4.81	count=17500
Total loss:	10.872 (rec:0.086, round:10.785)	b=4.25	count=18000
Total loss:	0.607 (rec:0.084, round:0.523)	b=3.69	count=18500
Total loss:	0.090 (rec:0.090, round:0.000)	b=3.12	count=19000
Total loss:	0.086 (rec:0.086, round:0.000)	b=2.56	count=19500
Total loss:	0.089 (rec:0.089, round:0.000)	b=2.00	count=20000
Reconstruction for block context_module
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Init alpha to be FP32
Total loss:	0.093 (rec:0.093, round:0.000)	b=0.00	count=500
Total loss:	0.093 (rec:0.093, round:0.000)	b=0.00	count=1000
Total loss:	0.086 (rec:0.086, round:0.000)	b=0.00	count=1500
Total loss:	0.089 (rec:0.089, round:0.000)	b=0.00	count=2000
Total loss:	0.088 (rec:0.088, round:0.000)	b=0.00	count=2500
Total loss:	0.084 (rec:0.084, round:0.000)	b=0.00	count=3000
Total loss:	0.088 (rec:0.088, round:0.000)	b=0.00	count=3500
Total loss:	166750.516 (rec:0.086, round:166750.422)	b=20.00	count=4000
Total loss:	60719.438 (rec:0.088, round:60719.348)	b=19.44	count=4500
Total loss:	55217.398 (rec:0.087, round:55217.312)	b=18.88	count=5000
Total loss:	51318.398 (rec:0.088, round:51318.312)	b=18.31	count=5500
Total loss:	47919.527 (rec:0.093, round:47919.434)	b=17.75	count=6000
Total loss:	44615.754 (rec:0.088, round:44615.668)	b=17.19	count=6500
Total loss:	41475.008 (rec:0.089, round:41474.918)	b=16.62	count=7000
Total loss:	38419.316 (rec:0.089, round:38419.227)	b=16.06	count=7500
Total loss:	35418.020 (rec:0.087, round:35417.934)	b=15.50	count=8000
Total loss:	32500.520 (rec:0.088, round:32500.432)	b=14.94	count=8500
Total loss:	29643.502 (rec:0.088, round:29643.414)	b=14.38	count=9000
Total loss:	26859.672 (rec:0.088, round:26859.584)	b=13.81	count=9500
Total loss:	24125.400 (rec:0.087, round:24125.312)	b=13.25	count=10000
Total loss:	21533.275 (rec:0.088, round:21533.188)	b=12.69	count=10500
Total loss:	18994.361 (rec:0.090, round:18994.271)	b=12.12	count=11000
Total loss:	16560.422 (rec:0.090, round:16560.332)	b=11.56	count=11500
Total loss:	14284.362 (rec:0.091, round:14284.271)	b=11.00	count=12000
Total loss:	12152.777 (rec:0.087, round:12152.690)	b=10.44	count=12500
Total loss:	10096.722 (rec:0.089, round:10096.633)	b=9.88	count=13000
Total loss:	8124.933 (rec:0.088, round:8124.844)	b=9.31	count=13500
Total loss:	6315.443 (rec:0.086, round:6315.356)	b=8.75	count=14000
Total loss:	4691.665 (rec:0.090, round:4691.574)	b=8.19	count=14500
Total loss:	3212.112 (rec:0.085, round:3212.027)	b=7.62	count=15000
Total loss:	1960.424 (rec:0.086, round:1960.338)	b=7.06	count=15500
Total loss:	1067.177 (rec:0.093, round:1067.084)	b=6.50	count=16000
Total loss:	445.011 (rec:0.088, round:444.923)	b=5.94	count=16500
Total loss:	152.699 (rec:0.089, round:152.610)	b=5.38	count=17000
Total loss:	41.576 (rec:0.091, round:41.486)	b=4.81	count=17500
Total loss:	6.377 (rec:0.086, round:6.291)	b=4.25	count=18000
Total loss:	0.737 (rec:0.087, round:0.650)	b=3.69	count=18500
Total loss:	0.089 (rec:0.089, round:0.000)	b=3.12	count=19000
Total loss:	0.088 (rec:0.088, round:0.000)	b=2.56	count=19500
Total loss:	0.089 (rec:0.089, round:0.000)	b=2.00	count=20000
Reconstruction for block local_module
Init alpha to be FP32
Total loss:	0.111 (rec:0.111, round:0.000)	b=0.00	count=500
Total loss:	0.101 (rec:0.101, round:0.000)	b=0.00	count=1000
Total loss:	0.100 (rec:0.100, round:0.000)	b=0.00	count=1500
Total loss:	0.102 (rec:0.102, round:0.000)	b=0.00	count=2000
Total loss:	0.101 (rec:0.101, round:0.000)	b=0.00	count=2500
Total loss:	0.103 (rec:0.103, round:0.000)	b=0.00	count=3000
Total loss:	0.098 (rec:0.098, round:0.000)	b=0.00	count=3500
Total loss:	120622.023 (rec:0.105, round:120621.922)	b=20.00	count=4000
Total loss:	46028.488 (rec:0.106, round:46028.383)	b=19.44	count=4500
Total loss:	41939.730 (rec:0.105, round:41939.625)	b=18.88	count=5000
Total loss:	39059.273 (rec:0.108, round:39059.164)	b=18.31	count=5500
Total loss:	36543.164 (rec:0.102, round:36543.062)	b=17.75	count=6000
Total loss:	34103.070 (rec:0.103, round:34102.969)	b=17.19	count=6500
Total loss:	31782.906 (rec:0.102, round:31782.805)	b=16.62	count=7000
Total loss:	29517.900 (rec:0.101, round:29517.799)	b=16.06	count=7500
Total loss:	27258.469 (rec:0.102, round:27258.367)	b=15.50	count=8000
Total loss:	25096.355 (rec:0.102, round:25096.254)	b=14.94	count=8500
Total loss:	22995.414 (rec:0.102, round:22995.312)	b=14.38	count=9000
Total loss:	20944.465 (rec:0.105, round:20944.359)	b=13.81	count=9500
Total loss:	18932.158 (rec:0.104, round:18932.055)	b=13.25	count=10000
Total loss:	16950.100 (rec:0.106, round:16949.992)	b=12.69	count=10500
Total loss:	15058.504 (rec:0.104, round:15058.400)	b=12.12	count=11000
Total loss:	13267.872 (rec:0.104, round:13267.769)	b=11.56	count=11500
Total loss:	11590.233 (rec:0.113, round:11590.120)	b=11.00	count=12000
Total loss:	9920.144 (rec:0.104, round:9920.039)	b=10.44	count=12500
Total loss:	8379.356 (rec:0.104, round:8379.252)	b=9.88	count=13000
Total loss:	6907.516 (rec:0.104, round:6907.412)	b=9.31	count=13500
Total loss:	5557.028 (rec:0.107, round:5556.921)	b=8.75	count=14000
Total loss:	4345.932 (rec:0.104, round:4345.828)	b=8.19	count=14500
Total loss:	3200.987 (rec:0.102, round:3200.885)	b=7.62	count=15000
Total loss:	2190.411 (rec:0.112, round:2190.299)	b=7.06	count=15500
Total loss:	1346.764 (rec:0.107, round:1346.657)	b=6.50	count=16000
Total loss:	704.920 (rec:0.105, round:704.815)	b=5.94	count=16500
Total loss:	302.776 (rec:0.113, round:302.663)	b=5.38	count=17000
Total loss:	86.091 (rec:0.105, round:85.986)	b=4.81	count=17500
Total loss:	13.017 (rec:0.101, round:12.916)	b=4.25	count=18000
Total loss:	0.769 (rec:0.104, round:0.665)	b=3.69	count=18500
Total loss:	0.103 (rec:0.103, round:0.000)	b=3.12	count=19000
Total loss:	0.113 (rec:0.113, round:0.000)	b=2.56	count=19500
Total loss:	0.112 (rec:0.112, round:0.000)	b=2.00	count=20000
Reconstruction for layer conv
Init alpha to be FP32
Total loss:	0.041 (rec:0.041, round:0.000)	b=0.00	count=500
Total loss:	0.041 (rec:0.041, round:0.000)	b=0.00	count=1000
Total loss:	0.041 (rec:0.041, round:0.000)	b=0.00	count=1500
Total loss:	0.043 (rec:0.043, round:0.000)	b=0.00	count=2000
Total loss:	0.044 (rec:0.044, round:0.000)	b=0.00	count=2500
Total loss:	0.043 (rec:0.043, round:0.000)	b=0.00	count=3000
Total loss:	0.043 (rec:0.043, round:0.000)	b=0.00	count=3500
Total loss:	181661.953 (rec:0.041, round:181661.906)	b=20.00	count=4000
Total loss:	70436.984 (rec:0.045, round:70436.938)	b=19.44	count=4500
Total loss:	64170.645 (rec:0.045, round:64170.602)	b=18.88	count=5000
Total loss:	59972.934 (rec:0.047, round:59972.887)	b=18.31	count=5500
Total loss:	56344.875 (rec:0.047, round:56344.828)	b=17.75	count=6000
Total loss:	53102.957 (rec:0.047, round:53102.910)	b=17.19	count=6500
Total loss:	50085.562 (rec:0.045, round:50085.516)	b=16.62	count=7000
Total loss:	47069.695 (rec:0.044, round:47069.652)	b=16.06	count=7500
Total loss:	44137.750 (rec:0.046, round:44137.703)	b=15.50	count=8000
Total loss:	41271.141 (rec:0.047, round:41271.094)	b=14.94	count=8500
Total loss:	38394.234 (rec:0.043, round:38394.191)	b=14.38	count=9000
Total loss:	35537.137 (rec:0.047, round:35537.090)	b=13.81	count=9500
Total loss:	32741.219 (rec:0.048, round:32741.172)	b=13.25	count=10000
Total loss:	29924.691 (rec:0.047, round:29924.645)	b=12.69	count=10500
Total loss:	27108.348 (rec:0.045, round:27108.303)	b=12.12	count=11000
Total loss:	24297.410 (rec:0.047, round:24297.363)	b=11.56	count=11500
Total loss:	21461.994 (rec:0.049, round:21461.945)	b=11.00	count=12000
Total loss:	18674.699 (rec:0.046, round:18674.652)	b=10.44	count=12500
Total loss:	15969.745 (rec:0.049, round:15969.696)	b=9.88	count=13000
Total loss:	13346.377 (rec:0.049, round:13346.328)	b=9.31	count=13500
Total loss:	10881.720 (rec:0.050, round:10881.670)	b=8.75	count=14000
Total loss:	8480.201 (rec:0.049, round:8480.152)	b=8.19	count=14500
Total loss:	6287.354 (rec:0.054, round:6287.301)	b=7.62	count=15000
Total loss:	4326.063 (rec:0.053, round:4326.010)	b=7.06	count=15500
Total loss:	2681.935 (rec:0.051, round:2681.885)	b=6.50	count=16000
Total loss:	1464.025 (rec:0.053, round:1463.971)	b=5.94	count=16500
Total loss:	652.115 (rec:0.055, round:652.060)	b=5.38	count=17000
Total loss:	200.900 (rec:0.050, round:200.850)	b=4.81	count=17500
Total loss:	31.469 (rec:0.054, round:31.414)	b=4.25	count=18000
Total loss:	1.554 (rec:0.053, round:1.502)	b=3.69	count=18500
Total loss:	0.109 (rec:0.054, round:0.054)	b=3.12	count=19000
Total loss:	0.054 (rec:0.054, round:0.000)	b=2.56	count=19500
Total loss:	0.053 (rec:0.053, round:0.000)	b=2.00	count=20000
Reconstruction for layer linear
Init alpha to be FP32
Total loss:	0.004 (rec:0.004, round:0.000)	b=0.00	count=500
Total loss:	0.003 (rec:0.003, round:0.000)	b=0.00	count=1000
Total loss:	0.003 (rec:0.003, round:0.000)	b=0.00	count=1500
Total loss:	0.003 (rec:0.003, round:0.000)	b=0.00	count=2000
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=2500
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=3000
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=3500
Total loss:	1152978.750 (rec:0.002, round:1152978.750)	b=20.00	count=4000
Total loss:	391091.938 (rec:0.005, round:391091.938)	b=19.44	count=4500
Total loss:	353005.312 (rec:0.004, round:353005.312)	b=18.88	count=5000
Total loss:	326224.594 (rec:0.005, round:326224.594)	b=18.31	count=5500
Total loss:	302650.312 (rec:0.005, round:302650.312)	b=17.75	count=6000
Total loss:	280531.469 (rec:0.005, round:280531.469)	b=17.19	count=6500
Total loss:	259545.641 (rec:0.004, round:259545.641)	b=16.62	count=7000
Total loss:	239005.906 (rec:0.004, round:239005.906)	b=16.06	count=7500
Total loss:	219120.656 (rec:0.006, round:219120.656)	b=15.50	count=8000
Total loss:	199582.688 (rec:0.005, round:199582.688)	b=14.94	count=8500
Total loss:	180298.219 (rec:0.004, round:180298.219)	b=14.38	count=9000
Total loss:	161530.047 (rec:0.004, round:161530.047)	b=13.81	count=9500
Total loss:	143441.109 (rec:0.004, round:143441.109)	b=13.25	count=10000
Total loss:	126227.383 (rec:0.005, round:126227.375)	b=12.69	count=10500
Total loss:	109715.258 (rec:0.005, round:109715.250)	b=12.12	count=11000
Total loss:	94237.680 (rec:0.005, round:94237.672)	b=11.56	count=11500
Total loss:	79707.133 (rec:0.006, round:79707.125)	b=11.00	count=12000
Total loss:	66177.398 (rec:0.005, round:66177.391)	b=10.44	count=12500
Total loss:	53709.207 (rec:0.005, round:53709.203)	b=9.88	count=13000
Total loss:	42249.738 (rec:0.006, round:42249.730)	b=9.31	count=13500
Total loss:	32215.545 (rec:0.006, round:32215.539)	b=8.75	count=14000
Total loss:	23427.877 (rec:0.006, round:23427.871)	b=8.19	count=14500
Total loss:	16089.088 (rec:0.006, round:16089.082)	b=7.62	count=15000
Total loss:	10258.337 (rec:0.007, round:10258.330)	b=7.06	count=15500
Total loss:	5960.068 (rec:0.006, round:5960.062)	b=6.50	count=16000
Total loss:	3007.953 (rec:0.007, round:3007.945)	b=5.94	count=16500
Total loss:	1198.170 (rec:0.008, round:1198.162)	b=5.38	count=17000
Total loss:	294.606 (rec:0.006, round:294.599)	b=4.81	count=17500
Total loss:	29.343 (rec:0.010, round:29.334)	b=4.25	count=18000
Total loss:	1.085 (rec:0.008, round:1.077)	b=3.69	count=18500
Total loss:	0.166 (rec:0.008, round:0.159)	b=3.12	count=19000
Total loss:	0.006 (rec:0.006, round:0.000)	b=2.56	count=19500
Total loss:	0.005 (rec:0.005, round:0.000)	b=2.00	count=20000
Reconstruction for layer linear
Init alpha to be FP32
Total loss:	0.081 (rec:0.081, round:0.000)	b=0.00	count=500
Total loss:	0.077 (rec:0.077, round:0.000)	b=0.00	count=1000
Total loss:	0.063 (rec:0.063, round:0.000)	b=0.00	count=1500
Total loss:	0.064 (rec:0.064, round:0.000)	b=0.00	count=2000
Total loss:	0.050 (rec:0.050, round:0.000)	b=0.00	count=2500
Total loss:	0.046 (rec:0.046, round:0.000)	b=0.00	count=3000
Total loss:	0.043 (rec:0.043, round:0.000)	b=0.00	count=3500
Total loss:	738132.812 (rec:0.038, round:738132.750)	b=20.00	count=4000
Total loss:	298767.375 (rec:0.057, round:298767.312)	b=19.44	count=4500
Total loss:	272618.375 (rec:0.054, round:272618.312)	b=18.88	count=5000
Total loss:	254256.375 (rec:0.055, round:254256.312)	b=18.31	count=5500
Total loss:	238085.547 (rec:0.051, round:238085.500)	b=17.75	count=6000
Total loss:	223171.594 (rec:0.054, round:223171.547)	b=17.19	count=6500
Total loss:	208660.922 (rec:0.052, round:208660.875)	b=16.62	count=7000
Total loss:	194702.438 (rec:0.050, round:194702.391)	b=16.06	count=7500
Total loss:	181083.047 (rec:0.051, round:181083.000)	b=15.50	count=8000
Total loss:	167606.547 (rec:0.050, round:167606.500)	b=14.94	count=8500
Total loss:	154202.906 (rec:0.050, round:154202.859)	b=14.38	count=9000
Total loss:	140983.188 (rec:0.049, round:140983.141)	b=13.81	count=9500
Total loss:	128071.203 (rec:0.054, round:128071.148)	b=13.25	count=10000
Total loss:	115392.062 (rec:0.055, round:115392.008)	b=12.69	count=10500
Total loss:	103051.836 (rec:0.055, round:103051.781)	b=12.12	count=11000
Total loss:	91115.133 (rec:0.056, round:91115.078)	b=11.56	count=11500
Total loss:	79516.734 (rec:0.060, round:79516.672)	b=11.00	count=12000
Total loss:	68392.023 (rec:0.060, round:68391.961)	b=10.44	count=12500
Total loss:	57720.770 (rec:0.066, round:57720.703)	b=9.88	count=13000
Total loss:	47568.105 (rec:0.066, round:47568.039)	b=9.31	count=13500
Total loss:	37925.184 (rec:0.067, round:37925.117)	b=8.75	count=14000
Total loss:	28849.668 (rec:0.071, round:28849.598)	b=8.19	count=14500
Total loss:	20543.033 (rec:0.075, round:20542.959)	b=7.62	count=15000
Total loss:	13132.537 (rec:0.082, round:13132.455)	b=7.06	count=15500
Total loss:	7080.680 (rec:0.080, round:7080.600)	b=6.50	count=16000
Total loss:	3139.732 (rec:0.085, round:3139.648)	b=5.94	count=16500
Total loss:	1179.031 (rec:0.084, round:1178.948)	b=5.38	count=17000
Total loss:	356.379 (rec:0.084, round:356.295)	b=4.81	count=17500
Total loss:	69.107 (rec:0.085, round:69.022)	b=4.25	count=18000
Total loss:	6.723 (rec:0.081, round:6.642)	b=3.69	count=18500
Total loss:	0.264 (rec:0.086, round:0.177)	b=3.12	count=19000
Total loss:	0.080 (rec:0.080, round:0.000)	b=2.56	count=19500
Total loss:	0.083 (rec:0.083, round:0.000)	b=2.00	count=20000
Test: [  0/782]	Time  1.008 ( 1.008)	Acc@1  90.62 ( 90.62)	Acc@5  96.88 ( 96.88)
Test: [200/782]	Time  0.389 ( 0.142)	Acc@1  87.50 ( 84.15)	Acc@5  95.31 ( 96.48)
Test: [400/782]	Time  0.334 ( 0.141)	Acc@1  92.19 ( 81.92)	Acc@5 100.00 ( 95.60)
Test: [600/782]	Time  0.396 ( 0.138)	Acc@1  81.25 ( 80.21)	Acc@5  95.31 ( 94.64)
 * Acc@1 79.374 Acc@5 94.358
Weight quantization accuracy: 79.3740005493164
Reconstruction for layer conv
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=5000
Reconstruction for block 1
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=5000
Reconstruction for block 0
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=5000
Reconstruction for block 1
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=5000
Reconstruction for block 0
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=5000
Reconstruction for block 1
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=5000
Reconstruction for block 2
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=5000
Reconstruction for block 0
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=500
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=1000
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=1500
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=2000
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=2500
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=3000
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=3500
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=4000
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=4500
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=5000
Reconstruction for block context_module
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=5000
Reconstruction for block local_module
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=5000
Reconstruction for block context_module
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=5000
Reconstruction for block local_module
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=5000
Reconstruction for block context_module
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=5000
Reconstruction for block local_module
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=5000
Reconstruction for block 0
Total loss:	0.003 (rec:0.003, round:0.000)	b=0.00	count=500
Total loss:	0.003 (rec:0.003, round:0.000)	b=0.00	count=1000
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=1500
Total loss:	0.003 (rec:0.003, round:0.000)	b=0.00	count=2000
Total loss:	0.003 (rec:0.003, round:0.000)	b=0.00	count=2500
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=3000
Total loss:	0.003 (rec:0.003, round:0.000)	b=0.00	count=3500
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=4000
Total loss:	0.003 (rec:0.003, round:0.000)	b=0.00	count=4500
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=5000
Reconstruction for block context_module
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=500
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=1000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4500
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=5000
Reconstruction for block local_module
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=5000
Reconstruction for block context_module
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3500
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=4000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=5000
Reconstruction for block local_module
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=5000
Reconstruction for block context_module
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=2500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=3500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=4500
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=5000
Reconstruction for block local_module
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=5000
Reconstruction for block context_module
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.000 (rec:0.000, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=5000
Reconstruction for block local_module
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=500
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=1000
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=1500
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=2000
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=2500
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=3000
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=3500
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=4000
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=4500
Total loss:	0.002 (rec:0.002, round:0.000)	b=0.00	count=5000
Reconstruction for layer conv
Total loss:	0.003 (rec:0.003, round:0.000)	b=0.00	count=500
Total loss:	0.003 (rec:0.003, round:0.000)	b=0.00	count=1000
Total loss:	0.003 (rec:0.003, round:0.000)	b=0.00	count=1500
Total loss:	0.003 (rec:0.003, round:0.000)	b=0.00	count=2000
Total loss:	0.003 (rec:0.003, round:0.000)	b=0.00	count=2500
Total loss:	0.003 (rec:0.003, round:0.000)	b=0.00	count=3000
Total loss:	0.004 (rec:0.004, round:0.000)	b=0.00	count=3500
Total loss:	0.003 (rec:0.003, round:0.000)	b=0.00	count=4000
Total loss:	0.003 (rec:0.003, round:0.000)	b=0.00	count=4500
Total loss:	0.003 (rec:0.003, round:0.000)	b=0.00	count=5000
Reconstruction for layer linear
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=1500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=2500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=3500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4000
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=4500
Total loss:	0.001 (rec:0.001, round:0.000)	b=0.00	count=5000
Reconstruction for layer linear
Total loss:	0.026 (rec:0.026, round:0.000)	b=0.00	count=500
Total loss:	0.026 (rec:0.026, round:0.000)	b=0.00	count=1000
Total loss:	0.023 (rec:0.023, round:0.000)	b=0.00	count=1500
Total loss:	0.024 (rec:0.024, round:0.000)	b=0.00	count=2000
Total loss:	0.024 (rec:0.024, round:0.000)	b=0.00	count=2500
Total loss:	0.028 (rec:0.028, round:0.000)	b=0.00	count=3000
Total loss:	0.026 (rec:0.026, round:0.000)	b=0.00	count=3500
Total loss:	0.030 (rec:0.030, round:0.000)	b=0.00	count=4000
Total loss:	0.027 (rec:0.027, round:0.000)	b=0.00	count=4500
Total loss:	0.023 (rec:0.023, round:0.000)	b=0.00	count=5000
Test: [  0/782]	Time  1.102 ( 1.102)	Acc@1  90.62 ( 90.62)	Acc@5  96.88 ( 96.88)
Test: [200/782]	Time  0.358 ( 0.146)	Acc@1  87.50 ( 84.13)	Acc@5  95.31 ( 96.46)
Test: [400/782]	Time  0.332 ( 0.143)	Acc@1  92.19 ( 81.90)	Acc@5 100.00 ( 95.59)
Test: [600/782]	Time  0.161 ( 0.138)	Acc@1  81.25 ( 80.21)	Acc@5  95.31 ( 94.64)
 * Acc@1 79.370 Acc@5 94.348
Full quantization (W8A16) accuracy: 79.3699951171875
